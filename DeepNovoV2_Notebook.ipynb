{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "DeepNovoV2_Notebook.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nadJTaCa1qZm",
        "colab_type": "text"
      },
      "source": [
        "# DeepNoVo V2 - \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUVW9WZWc9RF",
        "colab_type": "text"
      },
      "source": [
        "**Read Me** :  \n",
        "This Notebook is an deconstruction and adaptation of the DeepNovoV2 DeepLearning program of [Tran et al.](https://arxiv.org/abs/1904.08514), by S. Chardin and S. Diebolt for the DeepLearning UE at ESPCI.\n",
        "It was adapted and trainned for the use of the SMBP at ESPCI.\n",
        "\n",
        "How to execute this program ? \n",
        "This notebook is very compute extensive and was design to run on a GPU equiped machine. Build initialy using Google Colab, it run in acceptable time with a GPU runtime provide for free by Google.\n",
        "\n",
        "- You have to select the option in the next cell:\n",
        "> - option = 'train' : Trainning Mode  \n",
        "> - option = 'denovo' : Prediction of denovo spectrum mode\n",
        "> - option = 'valid' : Validation of the training done previously mode\n",
        "> - option = 'test' : Compare the predicted sequences (denovo) with a reference\n",
        "\n",
        "Select the mode you want, by setting the option variable to the string presented above.  \n",
        "Then set the appropriate path for the mode you selected. You can use the table of content to direct yourself to the path needed for the mode you selected.  \n",
        "\n",
        "Then run all the cells to launch the program (Ctrl+F9).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL9vJb3fSwB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "option =  ['train','denovo','valid','test']  #'train' 'denovo' 'valid' 'test'\n",
        "data_path = '/content/drive/My Drive/DeepNovo'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6b8eJ6y12-P",
        "colab_type": "code",
        "outputId": "d093d32b-0770-4300-f7a9-4ed603ef42f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Import google drive with the data to run the program\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynbyzPNE1qZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "import csv\n",
        "import re\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from enum import Enum\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "\n",
        "#set drive path as python directory\n",
        "os.chdir(data_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdmjbK-1qZs",
        "colab_type": "text"
      },
      "source": [
        "# Table of Contents\n",
        "1. [Global Variables](#globalvariables)\n",
        "2. [Feature Class](#featureclass)\n",
        "3. [Train Function](#train_func)\n",
        "4. [Neural Networks Models](#model)\n",
        "5. [Inference Model Wrapper](#inference) \n",
        "5. [Build Model](#build_func)\n",
        "6. [Data Reader Function](#reader)\n",
        "7. [Accecories Functions](#accesories)\n",
        "> a)[Collate Function](#collate)  \n",
        "b)[Extract & Move Data](#extract)  \n",
        "c) [Focal Loss Function](#loss)\n",
        "\n",
        "8. [Validation Function](#valid_func)\n",
        "9. [Cython Intregration](#cython)\n",
        "10. [Training Launch sequence](#train)\n",
        "11. [Validation of the trainning](#valid)\n",
        "12. [Denovo Part](#denovo)\n",
        "> a)[Denovo File Path](#denovo_path)  \n",
        "b)[Denovo Data Reader](#denovo_datareader)  \n",
        "c)[Results Writer Function](#denovo_writer)  \n",
        "d)[Knapsack Implementation](#knapsack)   \n",
        "f)[ION CNN Denovo](#ioncnn)  \n",
        "g)[Denovo Launch Sequence](#denovo_launch)\n",
        "13. [Test of the prediction](#test)\n",
        ">a)[Testing File path selection](#test_path)  \n",
        "b)[Worker Test function](#test_worker)  \n",
        "c)[Read Feature Accuracy](#test_accuracy)  \n",
        "d)[Score cutoff function](#test_cutoff)  \n",
        "e)[Testing Launch Sequence](#test_launch)\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "041AcISl1qZs",
        "colab_type": "text"
      },
      "source": [
        "## Configuration Parameters / Global Variables <a name=\"globalvariables\"></a>\n",
        "All the parameters needed and can be set-up here  \n",
        "**Caution**: the batch_size parameter needs to be also set-up in the [Train Function](#train_func) due to an issue unresolved (ligne 14)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAIipyT3SGTS",
        "colab_type": "text"
      },
      "source": [
        "### Global Variables and Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FO4G1qx1qZt",
        "colab_type": "code",
        "outputId": "76b35069-08e4-4a16-8fc1-bebad76371d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "use_lstm = True #specify for taining and prediction the use of the LSTM model or not\n",
        "#changing LSTM and the number of layer require a new training\n",
        "#Can be retrain in addition of another training if not the same parameters\n",
        "\n",
        "train_dir = 'train' #path to save the training parameters\n",
        "\n",
        "#name of the training parameters saving\n",
        "forward_model_save_name = 'forward_deepnovo.pth'\n",
        "backward_model_save_name = 'backward_deepnovo.pth'\n",
        "init_net_save_name = 'init_net.pth'\n",
        "\n",
        "#\n",
        "activation_func = F.relu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)\n",
        "\n",
        "class Direction(Enum):\n",
        "    forward = 1\n",
        "    backward = 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaG-BvhVL4jA",
        "colab_type": "code",
        "outputId": "3ef70220-e17d-414d-fe65-13a9579dbb13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# ==============================================================================\n",
        "# GLOBAL VARIABLES for VOCABULARY\n",
        "# ==============================================================================\n",
        "\n",
        "# Special vocabulary symbols - we always put them at the start.\n",
        "_PAD = \"_PAD\"\n",
        "_GO = \"_GO\"\n",
        "_EOS = \"_EOS\"\n",
        "_START_VOCAB = [_PAD, _GO, _EOS]\n",
        "\n",
        "PAD_ID = 0\n",
        "GO_ID = 1\n",
        "EOS_ID = 2\n",
        "assert PAD_ID == 0\n",
        "vocab_reverse = ['A',\n",
        "                 'R',\n",
        "                 'N',\n",
        "                 'N(Deamidated)',\n",
        "                 'D',\n",
        "                 'C',\n",
        "                 'C(Carboxymethyl)',\n",
        "                 #~ 'C(Carbamidomethylation)',\n",
        "                 'E',\n",
        "                 'Q',\n",
        "                 'Q(Deamidated)',\n",
        "                 'G',\n",
        "                 'H',\n",
        "                 'I',\n",
        "                 'L',\n",
        "                 'K',\n",
        "                 'M',\n",
        "                 'M(Oxidation)',\n",
        "                 'F',\n",
        "                 'P',\n",
        "                 'S',\n",
        "                 'T',\n",
        "                 'W',\n",
        "                 'Y',\n",
        "                 'V',\n",
        "                ]\n",
        "\n",
        "vocab_reverse = _START_VOCAB + vocab_reverse\n",
        "print(\"vocab_reverse \", vocab_reverse)\n",
        "\n",
        "vocab = dict([(x, y) for (y, x) in enumerate(vocab_reverse)])\n",
        "print(\"vocab \", vocab)\n",
        "\n",
        "vocab_size = len(vocab_reverse)\n",
        "print(\"vocab_size \", vocab_size)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# GLOBAL VARIABLES for THEORETICAL MASS\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "mass_H = 1.0078\n",
        "mass_H2O = 18.0106\n",
        "mass_NH3 = 17.0265\n",
        "mass_N_terminus = 1.0078\n",
        "mass_C_terminus = 17.0027\n",
        "mass_CO = 27.9949\n",
        "\n",
        "mass_AA = {'_PAD': 0.0,\n",
        "           '_GO': mass_N_terminus-mass_H,\n",
        "           '_EOS': mass_C_terminus+mass_H,\n",
        "           'A': 71.03711, # 0\n",
        "           'R': 156.10111, # 1\n",
        "           'N': 114.04293, # 2\n",
        "           'N(Deamidated)': 115.02695,\n",
        "           'D': 115.02694, # 3\n",
        "           #~ 'C(Carbamidomethylation)': 103.00919, # 4\n",
        "           #~ 'C(Carbamidomethylation)': 160.03065, # C(+57.02)\n",
        "           'C': 103.00919,\n",
        "           'C(Carboxymethyl)': 161.01919,\n",
        "           #~ 'C(Carbamidomethylation)': 161.01919, # C(+58.01) # orbi\n",
        "           'E': 129.04259, # 5\n",
        "           'Q': 128.05858, # 6\n",
        "           'Q(Deamidated)': 129.0426,\n",
        "           'G': 57.02146, # 7\n",
        "           'H': 137.05891, # 8\n",
        "           'I': 113.08406, # 9\n",
        "           'L': 113.08406, # 10\n",
        "           'K': 128.09496, # 11\n",
        "           'M': 131.04049, # 12\n",
        "           'M(Oxidation)': 147.0354,\n",
        "           'F': 147.06841, # 13\n",
        "           'P': 97.05276, # 14\n",
        "           'S': 87.03203, # 15\n",
        "           'T': 101.04768, # 16\n",
        "           'W': 186.07931, # 17\n",
        "           'Y': 163.06333, # 18\n",
        "           'V': 99.06841, # 19\n",
        "          }\n",
        "\n",
        "mass_ID = [mass_AA[vocab_reverse[x]] for x in range(vocab_size)]\n",
        "mass_ID_np = np.array(mass_ID, dtype=np.float32)\n",
        "\n",
        "mass_AA_min = mass_AA[\"G\"] # 57.02146\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# GLOBAL VARIABLES for PRECISION, RESOLUTION, temp-Limits of MASS & LEN\n",
        "# ==============================================================================\n",
        "\n",
        "WINDOW_SIZE = 10 # 10 bins\n",
        "print(\"WINDOW_SIZE \", WINDOW_SIZE)\n",
        "\n",
        "MZ_MAX = 3000.0\n",
        "\n",
        "MAX_NUM_PEAK = 500\n",
        "\n",
        "KNAPSACK_AA_RESOLUTION = 10000 # 0.0001 Da\n",
        "mass_AA_min_round = int(round(mass_AA_min * KNAPSACK_AA_RESOLUTION)) # 57.02146\n",
        "KNAPSACK_MASS_PRECISION_TOLERANCE = 100 # 0.01 Da\n",
        "num_position = 0\n",
        "\n",
        "PRECURSOR_MASS_PRECISION_TOLERANCE = 0.01\n",
        "\n",
        "# ONLY for accuracy evaluation\n",
        "\n",
        "AA_MATCH_PRECISION = 0.1\n",
        "\n",
        "# skip (x > MZ_MAX,MAX_LEN)\n",
        "MAX_LEN = 30 #50 if args.search_denovo else 30\n",
        "print(\"MAX_LEN \", MAX_LEN)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# HYPER-PARAMETERS of the NEURAL NETWORKS\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "num_ion = 12\n",
        "print(\"num_ion \", num_ion)\n",
        "\n",
        "weight_decay = 0.0  # no weight decay lead to better result.\n",
        "print(\"weight_decay \", weight_decay)\n",
        "\n",
        "#~ encoding_cnn_size = 4 * (RESOLUTION//10) # 4 # proportion to RESOLUTION\n",
        "#~ encoding_cnn_filter = 4\n",
        "#~ print(\"encoding_cnn_size \", encoding_cnn_size)\n",
        "#~ print(\"encoding_cnn_filter \", encoding_cnn_filter)\n",
        "\n",
        "embedding_size = 512\n",
        "print(\"embedding_size \", embedding_size)\n",
        "\n",
        "num_lstm_layers = 2 #2\n",
        "num_units = 64\n",
        "lstm_hidden_units = 512\n",
        "print(\"num_lstm_layers \", num_lstm_layers)\n",
        "print(\"num_units \", num_units)\n",
        "\n",
        "dropout_rate = 0.25 #0.25\n",
        "\n",
        "batch_size = 10 #24\n",
        "num_workers = 2\n",
        "print(\"batch_size \", batch_size)\n",
        "\n",
        "num_epoch = 20 #20\n",
        "\n",
        "init_lr = 1e-3\n",
        "\n",
        "steps_per_validation = 10 #300  # 100 # 2 # 4 # 200\n",
        "print(\"steps_per_validation \", steps_per_validation)\n",
        "\n",
        "#Denovo Beamsearch\n",
        "beam_size_param = 5 #2 #5\n",
        "print(\"Beam size = \",beam_size_param)\n",
        "\n",
        "# feature file column format\n",
        "col_feature_id = \"spec_group_id\"\n",
        "col_precursor_mz = \"m/z\"\n",
        "col_precursor_charge = \"z\"\n",
        "col_rt_mean = \"rt_mean\"\n",
        "col_raw_sequence = \"seq\"\n",
        "col_scan_list = \"scans\"\n",
        "col_feature_area = \"feature area\"\n",
        "\n",
        "# predicted file column format\n",
        "pcol_feature_id = 0\n",
        "pcol_feature_area = 1\n",
        "pcol_sequence = 2\n",
        "pcol_score = 3\n",
        "pcol_position_score = 4\n",
        "pcol_precursor_mz = 5\n",
        "pcol_precursor_charge = 6\n",
        "pcol_protein_id = 7\n",
        "pcol_scan_list_middle = 8\n",
        "pcol_scan_list_original = 9\n",
        "pcol_score_max = 10\n",
        "\n",
        "#Parameters for the Cython Module\n",
        "distance_scale_factor = 100.\n",
        "sinusoid_base = 30000.\n",
        "spectrum_reso = 10\n",
        "n_position = int(MZ_MAX) * spectrum_reso"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_reverse  ['_PAD', '_GO', '_EOS', 'A', 'R', 'N', 'N(Deamidated)', 'D', 'C', 'C(Carboxymethyl)', 'E', 'Q', 'Q(Deamidated)', 'G', 'H', 'I', 'L', 'K', 'M', 'M(Oxidation)', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
            "vocab  {'_PAD': 0, '_GO': 1, '_EOS': 2, 'A': 3, 'R': 4, 'N': 5, 'N(Deamidated)': 6, 'D': 7, 'C': 8, 'C(Carboxymethyl)': 9, 'E': 10, 'Q': 11, 'Q(Deamidated)': 12, 'G': 13, 'H': 14, 'I': 15, 'L': 16, 'K': 17, 'M': 18, 'M(Oxidation)': 19, 'F': 20, 'P': 21, 'S': 22, 'T': 23, 'W': 24, 'Y': 25, 'V': 26}\n",
            "vocab_size  27\n",
            "WINDOW_SIZE  10\n",
            "MAX_LEN  30\n",
            "num_ion  12\n",
            "weight_decay  0.0\n",
            "embedding_size  512\n",
            "num_lstm_layers  2\n",
            "num_units  64\n",
            "batch_size  10\n",
            "steps_per_validation  10\n",
            "Beam size =  5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwbZDNSCMY2q",
        "colab_type": "text"
      },
      "source": [
        "### Dataset paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtDSIFPgMWYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# DATASETS Path\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "knapsack_file = \"knapsack_big.npy\"\n",
        "#topk_output = 1\n",
        "\n",
        "input_spectrum_file_train = \"Test_files/spectrum_smbp.mgf\"\n",
        "input_feature_file_train = \"Test_files/features_smbp.csv.train\"\n",
        "input_spectrum_file_valid = \"Test_files/spectrum_smbp.mgf\"\n",
        "input_feature_file_valid = \"Test_files/features_smbp.csv.valid\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_g2StBQ1qZx",
        "colab_type": "text"
      },
      "source": [
        "## Feature Class, Perplexity, ajustable learning rate <a name=\"featureclass\"></a>\n",
        "Also, Save model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSXd-kiI1qZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#------------------------------------------- Define Feature Class ------------------------------\n",
        "\n",
        "@dataclass\n",
        "class Feature:\n",
        "    spec_id: str\n",
        "    mz: str\n",
        "    z: str\n",
        "    rt_mean: str\n",
        "    seq: str\n",
        "    scan: str\n",
        "\n",
        "    def to_list(self):\n",
        "        \"\"\"Convert the dataset to list\"\"\"\n",
        "        return [self.spec_id, self.mz, self.z, self.rt_mean, self.seq, self.scan, \"0.0:1.0\", \"1.0\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTYYFEHe1qZ1",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(log_loss):\n",
        "    \"\"\" Compute the perplexity from the loss of the model\"\"\"\n",
        "    return math.exp(log_loss) if log_loss < 300 else float('inf')\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 3 epochs\"\"\"\n",
        "    lr = init_lr * (0.1 ** ((epoch + 1) // 3))\n",
        "    print(f\"epoch: {epoch}\\tlr: {lr}\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def save_model(forward_deepnovo, backward_deepnovo, init_net):\n",
        "    \"\"\"\n",
        "    Save the model paramters for forward, backward and initalisation net, at the\n",
        "    location sets in the Global variables\n",
        "    \"\"\"\n",
        "    torch.save(forward_deepnovo.state_dict(), os.path.join(train_dir,\n",
        "                                                           forward_model_save_name))\n",
        "    torch.save(backward_deepnovo.state_dict(), os.path.join(train_dir,\n",
        "                                                            backward_model_save_name))\n",
        "    if use_lstm:\n",
        "        torch.save(init_net.state_dict(), os.path.join(train_dir,\n",
        "                                                   init_net_save_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iff0TWen1qZ4",
        "colab_type": "text"
      },
      "source": [
        "## Train Function <a name=\"train_func\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0KT4fku1qZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_perp_tab = []\n",
        "valid_perp_tab = []\n",
        "\n",
        "def train():\n",
        "    \"\"\"\n",
        "    Function govern the training of the model:\n",
        "    Parameters = input_feature_file_train\n",
        "                  input_spectrum_file_train\n",
        "    \"\"\"\n",
        "    # Training Dataset creation\n",
        "    train_set = DeepNovoTrainDataset(input_feature_file_train,\n",
        "                                     input_spectrum_file_train)\n",
        "    num_train_features = len(train_set)\n",
        "    batch_size = 24 #specified here because error with only global variable\n",
        "    steps_per_epoch = int(num_train_features / batch_size)\n",
        "    print(steps_per_epoch,'steps per epoch')\n",
        "    train_data_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    num_workers=num_workers,\n",
        "                                                    collate_fn=collate_func)\n",
        "    #Validation Dataset creation\n",
        "    valid_set = DeepNovoTrainDataset(input_feature_file_valid,\n",
        "                                     input_spectrum_file_valid)\n",
        "    valid_data_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=False,\n",
        "                                                    num_workers=num_workers,\n",
        "                                                    collate_fn=collate_func)\n",
        "    \n",
        "    forward_deepnovo, backward_deepnovo, init_net = build_model()\n",
        "    dense_params = list(forward_deepnovo.parameters()) + list(backward_deepnovo.parameters())\n",
        "    dense_optimizer = optim.Adam(dense_params,\n",
        "                                 lr=init_lr,\n",
        "                                 weight_decay=weight_decay)\n",
        "    dense_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(dense_optimizer, 'min', factor=0.5, verbose=True,\n",
        "                                                                 threshold=1e-4, cooldown=10, min_lr=1e-5)\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "    # train loop\n",
        "    best_epoch = None\n",
        "    best_step = None\n",
        "    start_time = time.time()\n",
        "    for epoch in tqdm(range(num_epoch)):\n",
        "        #adjust learning rate every 3 epoch\n",
        "        adjust_learning_rate(dense_optimizer, epoch)\n",
        "        for i, data in enumerate(train_data_loader):\n",
        "            dense_optimizer.zero_grad()\n",
        "            peak_location, \\\n",
        "            peak_intensity, \\\n",
        "            spectrum_representation, \\\n",
        "            batch_forward_id_target, \\\n",
        "            batch_backward_id_target, \\\n",
        "            batch_forward_ion_index, \\\n",
        "            batch_backward_ion_index, \\\n",
        "            batch_forward_id_input, \\\n",
        "            batch_backward_id_input = extract_and_move_data(data)\n",
        "            batch_size = batch_backward_id_target.size(0)\n",
        "\n",
        "            if use_lstm:\n",
        "                initial_state_tuple = init_net(spectrum_representation)\n",
        "                forward_logit, _ = forward_deepnovo(batch_forward_ion_index, peak_location, peak_intensity,\n",
        "                                                    batch_forward_id_input, initial_state_tuple)\n",
        "                backward_logit, _ = backward_deepnovo(batch_backward_ion_index, peak_location, peak_intensity,\n",
        "                                                      batch_backward_id_input, initial_state_tuple)\n",
        "            else:\n",
        "                forward_logit = forward_deepnovo(batch_forward_ion_index, peak_location, peak_intensity)\n",
        "                backward_logit = backward_deepnovo(batch_backward_ion_index, peak_location, peak_intensity)\n",
        "\n",
        "            forward_loss, _ = focal_loss(forward_logit, batch_forward_id_target, ignore_index=0, gamma=2.)\n",
        "            backward_loss, _ = focal_loss(backward_logit, batch_backward_id_target, ignore_index=0, gamma=2.)\n",
        "            total_loss = (forward_loss + backward_loss) / 2.\n",
        "            # compute gradient\n",
        "            total_loss.backward()\n",
        "            dense_optimizer.step()\n",
        "            #depending of the steps_per_validation selected, print the time it takes,\n",
        "            #for the model to train, and display the loss off training and of validation\n",
        "            if (i + 1) % steps_per_validation == 0:\n",
        "                duration = time.time() - start_time\n",
        "                step_time = duration / steps_per_validation\n",
        "                loss_cpu = total_loss.item()\n",
        "                # evaluation mode\n",
        "                forward_deepnovo.eval()\n",
        "                backward_deepnovo.eval()\n",
        "                validation_loss = validation(forward_deepnovo, backward_deepnovo, init_net, valid_data_loader)\n",
        "                dense_scheduler.step(validation_loss)\n",
        "\n",
        "                #Training and validation loss are saved to display graph of evolution.\n",
        "                training_perp_tab.append(perplexity(loss_cpu))\n",
        "                valid_perp_tab.append(perplexity(validation_loss))\n",
        "\n",
        "                print(f\"epoch {epoch} step {i}/{steps_per_epoch}, \"\n",
        "                            f\"train perplexity: {perplexity(loss_cpu)}\\t\"\n",
        "                            f\"validation perplexity: {perplexity(validation_loss)}\\tstep time: {step_time}\")\n",
        "\n",
        "                if validation_loss < best_valid_loss:\n",
        "                    best_valid_loss = validation_loss\n",
        "                    print('best valid loss achieved at epoch',epoch, 'step', i)\n",
        "                    best_epoch = epoch\n",
        "                    best_step = i\n",
        "                    # save model if achieve a new best valid loss. Careful, if the\n",
        "                    #step_per_validation is to low compared to the size of the data\n",
        "                    #the model may not be saved, because not evaluated during training\n",
        "                    save_model(forward_deepnovo, backward_deepnovo, init_net)\n",
        "\n",
        "                # back to train model\n",
        "                forward_deepnovo.train()\n",
        "                backward_deepnovo.train()\n",
        "\n",
        "                start_time = time.time()\n",
        "            # observed that most of gpu memory is unoccupied cache, so clear cache after each batch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    print('best model at epoch',best_epoch,'step',best_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXszJ8nr1qZ8",
        "colab_type": "text"
      },
      "source": [
        "## Model of the NN <a name=\"model\"></a>\n",
        "All the Neural Network models needed and used for the treatment of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll4inevn1qZ9",
        "colab_type": "text"
      },
      "source": [
        "   ### DeepNovoPointNet with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drj8yUBK1qZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepNovoPointNetWithLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNovoPointNetWithLSTM, self).__init__()\n",
        "        self.t_net = TNet(with_lstm=True)\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                      embedding_dim=embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,\n",
        "                            lstm_hidden_units,\n",
        "                            num_layers=num_lstm_layers,\n",
        "                            batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.output_layer = nn.Linear(num_units + lstm_hidden_units,\n",
        "                                      vocab_size)\n",
        "\n",
        "    def forward(self, location_index, peaks_location, peaks_intensity, aa_input=None, state_tuple=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param location_index: [batch, T, 26, 8] long\n",
        "        :param peaks_location: [batch, N] N stands for MAX_NUM_PEAK, long\n",
        "        :param peaks_intensity: [batch, N], float32\n",
        "        :param aa_input:[batch, T]\n",
        "        :param state_tuple: (h0, c0), where each is [num_lstm_layer, batch_size, num_units] tensor\n",
        "        :return:\n",
        "            logits: [batch, T, 26]\n",
        "        \"\"\"\n",
        "        assert aa_input is not None\n",
        "        N = peaks_location.size(1)\n",
        "        assert N == peaks_intensity.size(1)\n",
        "        batch_size, T, vocab_size, num_ion = location_index.size()\n",
        "\n",
        "        peaks_location = peaks_location.view(batch_size, 1, N, 1)\n",
        "        peaks_intensity = peaks_intensity.view(batch_size, 1, N, 1)\n",
        "        peaks_location = peaks_location.expand(-1, T, -1, -1)  # [batch, T, N, 1]\n",
        "        peaks_location_mask = (peaks_location > 1e-5).float()\n",
        "        peaks_intensity = peaks_intensity.expand(-1, T, -1, -1)  # [batch, T, N, 1]\n",
        "\n",
        "        location_index = location_index.view(batch_size, T, 1, vocab_size * num_ion)\n",
        "        location_index_mask = (location_index > 1e-5).float()\n",
        "\n",
        "        location_exp_minus_abs_diff = torch.exp(\n",
        "            -torch.abs(\n",
        "                (peaks_location - location_index) * distance_scale_factor\n",
        "            )\n",
        "        )\n",
        "        # [batch, T, N, 26*8]\n",
        "\n",
        "        location_exp_minus_abs_diff = location_exp_minus_abs_diff * peaks_location_mask * location_index_mask\n",
        "\n",
        "        input_feature = torch.cat((location_exp_minus_abs_diff, peaks_intensity), dim=3)\n",
        "        input_feature = input_feature.view(batch_size * T, N, vocab_size * num_ion + 1)\n",
        "        input_feature = input_feature.transpose(1, 2)\n",
        "\n",
        "        ion_feature = self.t_net(input_feature).view(batch_size, T, num_units)  # attention on peaks\n",
        "\n",
        "        # embedding\n",
        "        aa_embedded = self.embedding(aa_input)\n",
        "        lstm_input = aa_embedded  # [batch, T, embedding_size]\n",
        "        #dropout layer\n",
        "        #lstm_input = self.dropout(lstm_input)\n",
        "        #dropout doesn't appear to be efficient in this configuration\n",
        "        output_feature, new_state_tuple = self.lstm(lstm_input, state_tuple)\n",
        "        output_feature = torch.cat((ion_feature, activation_func(output_feature)), dim=2)\n",
        "        output_feature = self.dropout(output_feature)\n",
        "        logit = self.output_layer(output_feature)\n",
        "        return logit, new_state_tuple\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F466FSfQ1qaB",
        "colab_type": "text"
      },
      "source": [
        "### T-Net Module\n",
        "Same model presented in the PointNet Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe9imurR1qaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TNet(nn.Module):\n",
        "    \"\"\"\n",
        "    the T-net structure in the Point Net paper\n",
        "    \"\"\"\n",
        "    def __init__(self, with_lstm=False):\n",
        "        super(TNet, self).__init__()\n",
        "        self.with_lstm = with_lstm\n",
        "        self.conv1 = nn.Conv1d(vocab_size * num_ion + 1, num_units, 1)\n",
        "        self.conv2 = nn.Conv1d(num_units, 2*num_units, 1)\n",
        "        self.conv3 = nn.Conv1d(2*num_units, 4*num_units, 1)\n",
        "        self.fc1 = nn.Linear(4*num_units, 2*num_units)\n",
        "        self.fc2 = nn.Linear(2*num_units, num_units)\n",
        "        if not with_lstm:\n",
        "            self.output_layer = nn.Linear(num_units, vocab_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.input_batch_norm = nn.BatchNorm1d(vocab_size * num_ion + 1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(num_units)\n",
        "        self.bn2 = nn.BatchNorm1d(2*num_units)\n",
        "        self.bn3 = nn.BatchNorm1d(4*num_units)\n",
        "        self.bn4 = nn.BatchNorm1d(2*num_units)\n",
        "        self.bn5 = nn.BatchNorm1d(num_units)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "\n",
        "        :param x: [batch * T, 26*8+1, N]\n",
        "        :return:\n",
        "            logit: [batch * T, 26]\n",
        "        \"\"\"\n",
        "        x = self.input_batch_norm(x)\n",
        "        x = activation_func(self.bn1(self.conv1(x)))\n",
        "        x = activation_func(self.bn2(self.conv2(x)))\n",
        "        x = activation_func(self.bn3(self.conv3(x)))\n",
        "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
        "        assert x.size(1) == 4*num_units\n",
        "\n",
        "        x = activation_func(self.bn4(self.fc1(x)))\n",
        "        x = activation_func(self.bn5(self.fc2(x)))\n",
        "        if not self.with_lstm:\n",
        "            x = self.output_layer(x)  # [batch * T, 26]\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AloXQe0k1qaE",
        "colab_type": "text"
      },
      "source": [
        "### Initializing NN module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NewPxsP11qaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InitNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InitNet, self).__init__()\n",
        "        self.init_state_layer = nn.Linear(embedding_size, 2 * lstm_hidden_units)\n",
        "\n",
        "    def forward(self, spectrum_representation):\n",
        "        \"\"\"\n",
        "\n",
        "        :param spectrum_representation: [N, embedding_size]\n",
        "        :return:\n",
        "            [num_lstm_layers, batch_size, lstm_units], [num_lstm_layers, batch_size, lstm_units],\n",
        "        \"\"\"\n",
        "        x = torch.tanh(self.init_state_layer(spectrum_representation))\n",
        "        h_0, c_0 = torch.split(x, lstm_hidden_units, dim=1)\n",
        "        h_0 = torch.unsqueeze(h_0, dim=0)\n",
        "        h_0 = h_0.repeat(num_lstm_layers, 1, 1)\n",
        "        c_0 = torch.unsqueeze(c_0, dim=0)\n",
        "        c_0 = c_0.repeat(num_lstm_layers, 1, 1)\n",
        "        return h_0, c_0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoXZ-4Tk1qaH",
        "colab_type": "text"
      },
      "source": [
        "### DeepNovoPointNet model, without the LSTM NN module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDDlKn-u1qaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepNovoPointNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNovoPointNet, self).__init__()\n",
        "        self.t_net = TNet(with_lstm=False)\n",
        "        self.distance_scale_factor = distance_scale_factor\n",
        "\n",
        "    def forward(self, location_index, peaks_location, peaks_intensity):\n",
        "        \"\"\"\n",
        "\n",
        "        :param location_index: [batch, T, 26, 8] long\n",
        "        :param peaks_location: [batch, N] N stands for MAX_NUM_PEAK, long\n",
        "        :param peaks_intensity: [batch, N], float32\n",
        "        :return:\n",
        "            logits: [batch, T, 26]\n",
        "        \"\"\"\n",
        "\n",
        "        N = peaks_location.size(1)\n",
        "        assert N == peaks_intensity.size(1)\n",
        "        batch_size, T, vocab_size, num_ion = location_index.size()\n",
        "\n",
        "        peaks_location = peaks_location.view(batch_size, 1, N, 1)\n",
        "        peaks_intensity = peaks_intensity.view(batch_size, 1, N, 1)\n",
        "        peaks_location = peaks_location.expand(-1, T, -1, -1)  # [batch, T, N, 1]\n",
        "        peaks_location_mask = (peaks_location > 1e-5).float()\n",
        "        peaks_intensity = peaks_intensity.expand(-1, T, -1, -1)  # [batch, T, N, 1]\n",
        "\n",
        "        location_index = location_index.view(batch_size, T, 1, vocab_size*num_ion)\n",
        "        location_index_mask = (location_index > 1e-5).float()\n",
        "\n",
        "        location_exp_minus_abs_diff = torch.exp(\n",
        "            -torch.abs(\n",
        "                (peaks_location - location_index) * self.distance_scale_factor\n",
        "            )\n",
        "        )\n",
        "        # [batch, T, N, 26*8]\n",
        "\n",
        "        location_exp_minus_abs_diff = location_exp_minus_abs_diff * peaks_location_mask * location_index_mask\n",
        "\n",
        "        input_feature = torch.cat((location_exp_minus_abs_diff, peaks_intensity), dim=3)\n",
        "        input_feature = input_feature.view(batch_size*T, N, vocab_size*num_ion + 1)\n",
        "        input_feature = input_feature.transpose(1, 2)\n",
        "\n",
        "        result = self.t_net(input_feature).view(batch_size, T, vocab_size)\n",
        "        return result\n",
        "\n",
        "\n",
        "if use_lstm: #Global Variable\n",
        "    DeepNovoModel = DeepNovoPointNetWithLSTM\n",
        "else:\n",
        "    DeepNovoModel = DeepNovoPointNet\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZupAkjDF1qaL",
        "colab_type": "text"
      },
      "source": [
        "## Inference Model Wrapper <a name=\"inference\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7zEsNBW1qaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferenceModelWrapper(object):\n",
        "    \"\"\"\n",
        "    a wrapper class so that the beam search part of code is the same for both with lstm and without lstm model.\n",
        "    \"\"\"\n",
        "    def __init__(self, forward_model: DeepNovoModel, backward_model: DeepNovoModel, init_net: InitNet=None):\n",
        "        self.forward_model = forward_model\n",
        "        self.backward_model = backward_model\n",
        "        # make sure all models are in eval mode\n",
        "        self.forward_model.eval()\n",
        "        self.backward_model.eval()\n",
        "        if use_lstm:\n",
        "            assert init_net is not None\n",
        "            self.init_net = init_net\n",
        "            self.init_net.eval()\n",
        "\n",
        "    def step(self, candidate_location, peaks_location, peaks_intensity, aa_input, state_tuple, direction):\n",
        "        \"\"\"\n",
        "        :param state_tuple: tuple of ([num_layer, batch_size, num_unit], [num_layer, batch_size, num_unit])\n",
        "        :param aa_input: [batch, 1]\n",
        "        :param candidate_location: [batch, 1, 26, 8]\n",
        "        :param peaks_location: [batch, N]\n",
        "        :param peaks_intensity: [batch, N]\n",
        "        :param direction: enum class, whether forward or backward\n",
        "        :return: (log_prob, new_hidden_state)\n",
        "        log_prob: the pred log prob of shape [batch, 26]\n",
        "        \"\"\"\n",
        "        if direction == Direction.forward:\n",
        "            model = self.forward_model\n",
        "        else:\n",
        "            model = self.backward_model\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if use_lstm:\n",
        "                logit, new_state_tuple = model(candidate_location, peaks_location, peaks_intensity, aa_input,\n",
        "                                               state_tuple)\n",
        "            else:\n",
        "                logit = model(candidate_location, peaks_location, peaks_intensity)\n",
        "                new_state_tuple = None\n",
        "            logit = torch.squeeze(logit, dim=1)\n",
        "            log_prob = F.log_softmax(logit)\n",
        "        return log_prob, new_state_tuple\n",
        "\n",
        "    def initial_hidden_state(self, spectrum_representation):\n",
        "        \"\"\"\n",
        "\n",
        "        :param: spectrum_representation, [batch, embedding_size]\n",
        "        :return:\n",
        "            [num_lstm_layers, batch_size, lstm_units], [num_lstm_layers, batch_size, lstm_units],\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            h_0, c_0 = self.init_net(spectrum_representation)\n",
        "            return h_0.to(device), c_0.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co7njpUO6tZW",
        "colab_type": "text"
      },
      "source": [
        "## Build Model  <a name=\"build_func\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axr-ZTZY1qaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(training=True):\n",
        "    \"\"\"\n",
        "    :Parameters:\n",
        "        training to put the model in training of prediction mode\n",
        "    :return:\n",
        "        forward_deepnovo, backward_deepnovo, init_net : in device, and with the \n",
        "        pretrained parameters if present.\n",
        "    \"\"\"\n",
        "    forward_deepnovo = DeepNovoModel()\n",
        "    backward_deepnovo = DeepNovoModel()\n",
        "    if use_lstm:\n",
        "        init_net = InitNet() #initialisation of the LSTM\n",
        "    else:\n",
        "        init_net = None\n",
        "\n",
        "    # load pretrained params if exist\n",
        "    if os.path.exists(os.path.join(train_dir, forward_model_save_name)):\n",
        "        assert os.path.exists(os.path.join(train_dir, backward_model_save_name))\n",
        "        print(\"load pretrained model\")\n",
        "        forward_deepnovo.load_state_dict(torch.load(os.path.join(train_dir, forward_model_save_name),\n",
        "                                                    map_location=device))\n",
        "        backward_deepnovo.load_state_dict(torch.load(os.path.join(train_dir, backward_model_save_name),\n",
        "                                                     map_location=device))\n",
        "        if use_lstm:\n",
        "            init_net.load_state_dict(torch.load(os.path.join(train_dir, init_net_save_name),\n",
        "                                                map_location=device))\n",
        "    else:\n",
        "        assert training, f\"building model for testing, but could not found weight under directory \" \\\n",
        "                         f\"{train_dir}\"\n",
        "        print(\"initialize a set of new parameters\")\n",
        "\n",
        "    if use_lstm:\n",
        "        # share embedding matrix\n",
        "        backward_deepnovo.embedding.weight = forward_deepnovo.embedding.weight\n",
        "\n",
        "    backward_deepnovo = backward_deepnovo.to(device)\n",
        "    forward_deepnovo = forward_deepnovo.to(device)\n",
        "    if use_lstm:\n",
        "        init_net = init_net.to(device)\n",
        "    return forward_deepnovo, backward_deepnovo, init_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43SlkAYR1qaY",
        "colab_type": "text"
      },
      "source": [
        "## Data reader function <a name=\"reader\"></a>\n",
        "Allow the transformation of original spectrum to dataset used by Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW6_c6VdvTOB",
        "colab_type": "text"
      },
      "source": [
        "### Parse raw sequence\n",
        "Initially contained the exeptions of the a.a modifications, to converte them for a better readability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq-DeA_O1qaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------Data_reader------------------------------------------\n",
        "\n",
        "\n",
        "def parse_raw_sequence(raw_sequence: str):\n",
        "    return True, re.findall(r'[A-Z](?:\\(.+?\\))?', raw_sequence)\n",
        "    \n",
        "    ''' #old parser. Used specific hard-coded exeptions not adapted for MASCOT\n",
        "    # Decomment if use data from the paper of Tran et al.\n",
        "    raw_sequence_len = len(raw_sequence)\n",
        "    peptide = []\n",
        "    index = 0\n",
        "    while index < raw_sequence_len:\n",
        "        if raw_sequence[index] == \"(\":\n",
        "            if peptide[-1] == \"C\" and raw_sequence[index:index + 8] == \"(+57.02)\":\n",
        "                peptide[-1] = \"C(Carbamidomethylation)\"\n",
        "                index += 8\n",
        "            elif peptide[-1] == 'M' and raw_sequence[index:index + 8] == \"(+15.99)\":\n",
        "                peptide[-1] = 'M(Oxidation)'\n",
        "                index += 8\n",
        "            elif peptide[-1] == 'N' and raw_sequence[index:index + 6] == \"(+.98)\":\n",
        "                peptide[-1] = 'N(Deamidation)'\n",
        "                index += 6\n",
        "            elif peptide[-1] == 'Q' and raw_sequence[index:index + 6] == \"(+.98)\":\n",
        "                peptide[-1] = 'Q(Deamidation)'\n",
        "                index += 6\n",
        "            else:  # unknown modification\n",
        "                print('unknown modification in seq', raw_sequence)\n",
        "                return False, peptide\n",
        "        else:\n",
        "            peptide.append(raw_sequence[index])\n",
        "            index += 1\n",
        "\n",
        "    return True, peptide\n",
        "    '''\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wUhLyNTvpaY",
        "colab_type": "text"
      },
      "source": [
        "### Class Denovo Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7EgNwEqvQyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------Class Definition------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class DDAFeature:\n",
        "    feature_id: str\n",
        "    mz: float\n",
        "    z: float\n",
        "    rt_mean: float\n",
        "    peptide: list\n",
        "    scan: str\n",
        "    mass: float\n",
        "    feature_area: str\n",
        "\n",
        "@dataclass\n",
        "class DenovoData:\n",
        "    peak_location: np.ndarray\n",
        "    peak_intensity: np.ndarray\n",
        "    spectrum_representation: np.ndarray\n",
        "    original_dda_feature: DDAFeature\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainData:\n",
        "    peak_location: np.ndarray\n",
        "    peak_intensity: np.ndarray\n",
        "    spectrum_representation: np.ndarray\n",
        "    forward_id_target: list\n",
        "    backward_id_target: list\n",
        "    forward_ion_location_index_list: list\n",
        "    backward_ion_location_index_list: list\n",
        "    forward_id_input: list\n",
        "    backward_id_input: list\n",
        "\n",
        "\n",
        "class DeepNovoTrainDataset(Dataset):\n",
        "    def __init__(self, feature_filename, spectrum_filename, transform=None):\n",
        "        \"\"\"\n",
        "        read all feature information and store in memory,\n",
        "        :param feature_filename:\n",
        "        :param spectrum_filename:\n",
        "        \"\"\"\n",
        "        print(f\"input spectrum file: {spectrum_filename}\")\n",
        "        print(f\"input feature file: {feature_filename}\")\n",
        "        self.spectrum_filename = spectrum_filename\n",
        "        self.input_spectrum_handle = None\n",
        "        self.feature_list = []\n",
        "        self.spectrum_location_dict = {}\n",
        "        self.transform = transform\n",
        "        # read spectrum location file\n",
        "        spectrum_location_file = spectrum_filename + '.location.pytorch.pkl'\n",
        "        if os.path.exists(spectrum_location_file):\n",
        "            print(\"read cached spectrum locations\")\n",
        "            with open(spectrum_location_file, 'rb') as fr:\n",
        "                self.spectrum_location_dict = pickle.load(fr)\n",
        "        else:\n",
        "            print(\"build spectrum location from scratch\")\n",
        "            spectrum_location_dict = {}\n",
        "            line = True\n",
        "            with open(spectrum_filename, 'r') as f:\n",
        "                while line:\n",
        "                    current_location = f.tell()\n",
        "                    line = f.readline()\n",
        "                    if \"BEGIN IONS\" in line:\n",
        "                        spectrum_location = current_location\n",
        "                    elif \"SCANS=\" in line:\n",
        "                        scan = re.split('[=\\r\\n]', line)[1]\n",
        "                        spectrum_location_dict[scan] = spectrum_location\n",
        "            self.spectrum_location_dict = spectrum_location_dict\n",
        "            with open(spectrum_location_file, 'wb') as fw:\n",
        "                pickle.dump(self.spectrum_location_dict, fw)\n",
        "\n",
        "        # read feature file\n",
        "        skipped_by_mass = 0\n",
        "        skipped_by_ptm = 0\n",
        "        skipped_by_length = 0\n",
        "        with open(feature_filename, 'r') as fr:\n",
        "            reader = csv.reader(fr, delimiter=',')\n",
        "            header = next(reader)\n",
        "            feature_id_index = header.index(col_feature_id)\n",
        "            mz_index = header.index(col_precursor_mz)\n",
        "            z_index = header.index(col_precursor_charge)\n",
        "            rt_mean_index = header.index(col_rt_mean)\n",
        "            seq_index = header.index(col_raw_sequence)\n",
        "            scan_index = header.index(col_scan_list)\n",
        "            feature_area_index = header.index(col_feature_area)\n",
        "            for line in reader:\n",
        "                mass = (float(line[mz_index]) - mass_H) * float(line[z_index])\n",
        "                ok, peptide = parse_raw_sequence(line[seq_index])\n",
        "                if not ok:\n",
        "                    skipped_by_ptm += 1\n",
        "                    print(f\"{line[seq_index]} skipped by ptm\")\n",
        "                    continue\n",
        "                if mass > MZ_MAX:\n",
        "                    skipped_by_mass += 1\n",
        "                    continue\n",
        "                if len(peptide) >= MAX_LEN:\n",
        "                    skipped_by_length += 1\n",
        "                    continue\n",
        "                new_feature = DDAFeature(feature_id=line[feature_id_index],\n",
        "                                         mz=float(line[mz_index]),\n",
        "                                         z=float(line[z_index]),\n",
        "                                         rt_mean=float(line[rt_mean_index]),\n",
        "                                         peptide=peptide,\n",
        "                                         scan=line[scan_index],\n",
        "                                         mass=mass,\n",
        "                                         feature_area=line[feature_area_index])\n",
        "                self.feature_list.append(new_feature)\n",
        "        print(f\"read {len(self.feature_list)} features, {skipped_by_mass} skipped by mass, \"\n",
        "                    f\"{skipped_by_ptm} skipped by unknown modification, {skipped_by_length} skipped by length\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature_list)\n",
        "\n",
        "    def close(self):\n",
        "        self.input_spectrum_handle.close()\n",
        "\n",
        "    def _parse_spectrum_ion(self):\n",
        "        mz_list = []\n",
        "        intensity_list = []\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        while not \"END IONS\" in line:\n",
        "            mz, intensity = re.split(' |\\r|\\n', line)[:2]\n",
        "            mz_float = float(mz)\n",
        "            intensity_float = float(intensity)\n",
        "            # skip an ion if its mass > MZ_MAX\n",
        "            if mz_float > MZ_MAX:\n",
        "                line = self.input_spectrum_handle.readline()\n",
        "                continue\n",
        "            mz_list.append(mz_float)\n",
        "            intensity_list.append(intensity_float)\n",
        "            line = self.input_spectrum_handle.readline()\n",
        "        return mz_list, intensity_list\n",
        "\n",
        "    def _get_feature(self, feature: DDAFeature) -> TrainData:\n",
        "        spectrum_location = self.spectrum_location_dict[feature.scan]\n",
        "        self.input_spectrum_handle.seek(spectrum_location)\n",
        "        # parse header lines\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"BEGIN IONS\" in line, \"Error: wrong input BEGIN IONS\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"TITLE=\" in line, \"Error: wrong input TITLE=\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"PEPMASS=\" in line, \"Error: wrong input PEPMASS=\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"CHARGE=\" in line, \"Error: wrong input CHARGE=\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"SCANS=\" in line, \"Error: wrong input SCANS=\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"RTINSECONDS=\" in line, \"Error: wrong input RTINSECONDS=\"\n",
        "        mz_list, intensity_list = self._parse_spectrum_ion()\n",
        "        peak_location, peak_intensity, spectrum_representation = process_peaks(mz_list, intensity_list, feature.mass)\n",
        "\n",
        "        assert np.max(peak_intensity) < 1.0 + 1e-5\n",
        "        # no id needed for the denovo (Denovo part)\n",
        "        peptide_id_list = [vocab[x] for x in feature.peptide]\n",
        "        forward_id_input = [GO_ID] + peptide_id_list\n",
        "        forward_id_target = peptide_id_list + [EOS_ID]\n",
        "        forward_ion_location_index_list = []\n",
        "        prefix_mass = 0.\n",
        "        for i, id in enumerate(forward_id_input):\n",
        "            prefix_mass += mass_ID[id]\n",
        "            ion_location = get_ion_index(feature.mass, prefix_mass, 0)\n",
        "            forward_ion_location_index_list.append(ion_location)\n",
        "\n",
        "        backward_id_input = [EOS_ID] + peptide_id_list[::-1]\n",
        "        backward_id_target = peptide_id_list[::-1] + [GO_ID]\n",
        "        backward_ion_location_index_list = []\n",
        "        suffix_mass = 0\n",
        "        for i, id in enumerate(backward_id_input):\n",
        "            suffix_mass += mass_ID[id]\n",
        "            ion_location = get_ion_index(feature.mass, suffix_mass, 1)\n",
        "            backward_ion_location_index_list.append(ion_location)\n",
        "\n",
        "        return TrainData(peak_location=peak_location,\n",
        "                         peak_intensity=peak_intensity,\n",
        "                         spectrum_representation=spectrum_representation,\n",
        "                         forward_id_target=forward_id_target,\n",
        "                         backward_id_target=backward_id_target,\n",
        "                         forward_ion_location_index_list=forward_ion_location_index_list,\n",
        "                         backward_ion_location_index_list=backward_ion_location_index_list,\n",
        "                         forward_id_input=forward_id_input,\n",
        "                         backward_id_input=backward_id_input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.input_spectrum_handle is None:\n",
        "            self.input_spectrum_handle = open(self.spectrum_filename, 'r')\n",
        "        feature = self.feature_list[idx]\n",
        "        return self._get_feature(feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZM4nJeK1qab",
        "colab_type": "text"
      },
      "source": [
        "## Collate Function <a name=\"collate\"></a>\n",
        "Allows for the creation of a map-style for the dataset TrainData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9CVExmx1qac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_func(train_data_list):\n",
        "    \"\"\"\n",
        "\n",
        "    :param train_data_list: list of TrainData\n",
        "    :return:\n",
        "        peak_location: [batch, N]\n",
        "        peak_intensity: [batch, N]\n",
        "        forward_target_id: [batch, T]\n",
        "        backward_target_id: [batch, T]\n",
        "        forward_ion_index_list: [batch, T, 26, 8]\n",
        "        backward_ion_index_list: [batch, T, 26, 8]\n",
        "    \"\"\"\n",
        "    # sort data by seq length (decreasing order)\n",
        "    train_data_list.sort(key=lambda x: len(x.forward_id_target), reverse=True)\n",
        "    batch_max_seq_len = len(train_data_list[0].forward_id_target)\n",
        "    ion_index_shape = train_data_list[0].forward_ion_location_index_list[0].shape\n",
        "    assert ion_index_shape == (vocab_size, num_ion)\n",
        "\n",
        "    peak_location = [x.peak_location for x in train_data_list]\n",
        "    peak_location = np.stack(peak_location) # [batch_size, N]\n",
        "    peak_location = torch.from_numpy(peak_location)\n",
        "\n",
        "    peak_intensity = [x.peak_intensity for x in train_data_list]\n",
        "    peak_intensity = np.stack(peak_intensity) # [batch_size, N]\n",
        "    peak_intensity = torch.from_numpy(peak_intensity)\n",
        "\n",
        "    spectrum_representation = [x.spectrum_representation for x in train_data_list]\n",
        "    spectrum_representation = np.stack(spectrum_representation)  # [batch_size, embed_size]\n",
        "    spectrum_representation = torch.from_numpy(spectrum_representation)\n",
        "\n",
        "    batch_forward_ion_index = []\n",
        "    batch_forward_id_target = []\n",
        "    batch_forward_id_input = []\n",
        "    for data in train_data_list:\n",
        "        ion_index = np.zeros((batch_max_seq_len, ion_index_shape[0], ion_index_shape[1]),\n",
        "                               np.float32)\n",
        "        forward_ion_index = np.stack(data.forward_ion_location_index_list)\n",
        "        ion_index[:forward_ion_index.shape[0], :, :] = forward_ion_index\n",
        "        batch_forward_ion_index.append(ion_index)\n",
        "\n",
        "        f_target = np.zeros((batch_max_seq_len,), np.int64)\n",
        "        forward_target = np.array(data.forward_id_target, np.int64)\n",
        "        f_target[:forward_target.shape[0]] = forward_target\n",
        "        batch_forward_id_target.append(f_target)\n",
        "\n",
        "        f_input = np.zeros((batch_max_seq_len,), np.int64)\n",
        "        forward_input = np.array(data.forward_id_input, np.int64)\n",
        "        f_input[:forward_input.shape[0]] = forward_input\n",
        "        batch_forward_id_input.append(f_input)\n",
        "\n",
        "\n",
        "\n",
        "    batch_forward_id_target = torch.from_numpy(np.stack(batch_forward_id_target))  # [batch_size, T]\n",
        "    batch_forward_ion_index = torch.from_numpy(np.stack(batch_forward_ion_index))  # [batch, T, 26, 8]\n",
        "    batch_forward_id_input = torch.from_numpy(np.stack(batch_forward_id_input))\n",
        "\n",
        "    batch_backward_ion_index = []\n",
        "    batch_backward_id_target = []\n",
        "    batch_backward_id_input = []\n",
        "    for data in train_data_list:\n",
        "        ion_index = np.zeros((batch_max_seq_len, ion_index_shape[0], ion_index_shape[1]),\n",
        "                             np.float32)\n",
        "        backward_ion_index = np.stack(data.backward_ion_location_index_list)\n",
        "        ion_index[:backward_ion_index.shape[0], :, :] = backward_ion_index\n",
        "        batch_backward_ion_index.append(ion_index)\n",
        "\n",
        "        b_target = np.zeros((batch_max_seq_len,), np.int64)\n",
        "        backward_target = np.array(data.backward_id_target, np.int64)\n",
        "        b_target[:backward_target.shape[0]] = backward_target\n",
        "        batch_backward_id_target.append(b_target)\n",
        "\n",
        "        b_input = np.zeros((batch_max_seq_len,), np.int64)\n",
        "        backward_input = np.array(data.backward_id_input, np.int64)\n",
        "        b_input[:backward_input.shape[0]] = backward_input\n",
        "        batch_backward_id_input.append(b_input)\n",
        "\n",
        "    batch_backward_id_target = torch.from_numpy(np.stack(batch_backward_id_target))  # [batch_size, T]\n",
        "    batch_backward_ion_index = torch.from_numpy(np.stack(batch_backward_ion_index))  # [batch, T, 26, 8]\n",
        "    batch_backward_id_input = torch.from_numpy(np.stack(batch_backward_id_input))\n",
        "\n",
        "    return (peak_location,\n",
        "            peak_intensity,\n",
        "            spectrum_representation,\n",
        "            batch_forward_id_target,\n",
        "            batch_backward_id_target,\n",
        "            batch_forward_ion_index,\n",
        "            batch_backward_ion_index,\n",
        "            batch_forward_id_input,\n",
        "            batch_backward_id_input\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHe2X1lY1qbV",
        "colab_type": "text"
      },
      "source": [
        "## Extract and move data <a name=\"extract\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krkvoHis1qbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_and_move_data(data):\n",
        "    \"\"\"\n",
        "    This function extract the data from the dataclass pytorch and put it in the device\n",
        "    used (CPU/RAM or GPU). This speed-up the process.\n",
        "    :param data: result from dataloader\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    peak_location, \\\n",
        "    peak_intensity, \\\n",
        "    spectrum_representation,\\\n",
        "    batch_forward_id_target, \\\n",
        "    batch_backward_id_target, \\\n",
        "    batch_forward_ion_index, \\\n",
        "    batch_backward_ion_index, \\\n",
        "    batch_forward_id_input, \\\n",
        "    batch_backward_id_input = data\n",
        "\n",
        "    # move to device\n",
        "    peak_location = peak_location.to(device)\n",
        "    peak_intensity = peak_intensity.to(device)\n",
        "    spectrum_representation = spectrum_representation.to(device)\n",
        "    batch_forward_id_target = batch_forward_id_target.to(device)\n",
        "    batch_backward_id_target = batch_backward_id_target.to(device)\n",
        "    batch_forward_ion_index = batch_forward_ion_index.to(device)\n",
        "    batch_backward_ion_index = batch_backward_ion_index.to(device)\n",
        "    batch_forward_id_input = batch_forward_id_input.to(device)\n",
        "    batch_backward_id_input = batch_backward_id_input.to(device)\n",
        "    return (peak_location,\n",
        "            peak_intensity,\n",
        "            spectrum_representation,\n",
        "            batch_forward_id_target,\n",
        "            batch_backward_id_target,\n",
        "            batch_forward_ion_index,\n",
        "            batch_backward_ion_index,\n",
        "            batch_forward_id_input,\n",
        "            batch_backward_id_input\n",
        "            )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oITCBrXS1qbX",
        "colab_type": "text"
      },
      "source": [
        "## Focal loss function <a name=\"loss\"></a>\n",
        "Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training.\n",
        "[Focal Loss for Dense Object Detection](https://arxiv.org/pdf/1708.02002.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3HFdpJP1qbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(logits, labels, ignore_index=-100, gamma=2.):\n",
        "    \"\"\"\n",
        "    :param logits: float tensor of shape [batch, T, 26]\n",
        "    :param labels: long tensor of shape [batch, T]\n",
        "    :param ignore_index: ignore the loss of those tokens\n",
        "    :param gamma:\n",
        "    :return: average loss, num_valid_token\n",
        "    \"\"\"\n",
        "    valid_token_mask = (labels != ignore_index).float()  # [batch, T]\n",
        "    num_valid_token = torch.sum(valid_token_mask)\n",
        "    batch_size, T, num_classes = logits.size()\n",
        "    sigmoid_p = torch.sigmoid(logits) #sigmoid used instead of soft_max\n",
        "    target_tensor = to_one_hot(labels, n_dims=num_classes).float().to(device)\n",
        "    zeros = torch.zeros_like(sigmoid_p)\n",
        "    pos_p_sub = torch.where(target_tensor >= sigmoid_p, target_tensor - sigmoid_p, zeros)  # [batch, T, 26]\n",
        "    neg_p_sub = torch.where(target_tensor > zeros, zeros, sigmoid_p)  # [batch, T, 26]\n",
        "\n",
        "    per_token_loss = - (pos_p_sub ** gamma) * torch.log(torch.clamp(sigmoid_p, 1e-8, 1.0)) - \\\n",
        "                     (neg_p_sub ** gamma) * torch.log(torch.clamp(1.0 - sigmoid_p, 1e-8, 1.0))\n",
        "    per_entry_loss = torch.sum(per_token_loss, dim=2)  # [batch, T]\n",
        "    per_entry_loss = per_entry_loss * valid_token_mask  # masking out loss from pad tokens\n",
        "\n",
        "    per_entry_average_loss = torch.sum(per_entry_loss) / (num_valid_token + 1e-6)\n",
        "    return per_entry_average_loss, num_valid_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSTTJNqZ1qbZ",
        "colab_type": "text"
      },
      "source": [
        "### to_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyit6H_n1qbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(y, n_dims=None):\n",
        "    \"\"\" Take integer y with n dims and convert it to 1-hot representation with n+1 dims. \n",
        "    c.f. Report 2.5\n",
        "    \"\"\"\n",
        "    y_tensor = y.data\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
        "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
        "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS1EXktjNPzz",
        "colab_type": "text"
      },
      "source": [
        "## Validation Function <a name=\"valid_func\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkpv8RC4NOxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(forward_deepnovo, backward_deepnovo, init_net, valid_loader) -> float:\n",
        "    \"\"\" Compute and return the loss of the model trained on Validation dataset (valid_loader) \"\"\"\n",
        "    with torch.no_grad():\n",
        "        valid_loss = 0\n",
        "        num_valid_samples = 0\n",
        "        for data in valid_loader:\n",
        "            peak_location, \\\n",
        "            peak_intensity, \\\n",
        "            spectrum_representation, \\\n",
        "            batch_forward_id_target, \\\n",
        "            batch_backward_id_target, \\\n",
        "            batch_forward_ion_index, \\\n",
        "            batch_backward_ion_index, \\\n",
        "            batch_forward_id_input, \\\n",
        "            batch_backward_id_input = extract_and_move_data(data)\n",
        "            batch_size = batch_backward_id_target.size(0)\n",
        "            #evaluate the validation data through the model\n",
        "            if use_lstm:\n",
        "                initial_state_tuple = init_net(spectrum_representation)\n",
        "                forward_logit, _ = forward_deepnovo(batch_forward_ion_index, peak_location, peak_intensity,\n",
        "                                                    batch_forward_id_input, initial_state_tuple)\n",
        "                backward_logit, _ = backward_deepnovo(batch_backward_ion_index, peak_location, peak_intensity,\n",
        "                                                      batch_backward_id_input, initial_state_tuple)\n",
        "            else:\n",
        "                forward_logit = forward_deepnovo(batch_forward_ion_index, peak_location, peak_intensity)\n",
        "                backward_logit = backward_deepnovo(batch_backward_ion_index, peak_location, peak_intensity)\n",
        "            #compute the loss\n",
        "            forward_loss, f_num = focal_loss(forward_logit, batch_forward_id_target, ignore_index=0, gamma=2.)\n",
        "            backward_loss, b_num = focal_loss(backward_logit, batch_backward_id_target, ignore_index=0, gamma=2.)\n",
        "            valid_loss += forward_loss.item() * f_num.item() + backward_loss.item() * b_num.item()\n",
        "            num_valid_samples += f_num.item() + b_num.item()\n",
        "    #average the loss on all validation samples        \n",
        "    average_valid_loss = valid_loss / (num_valid_samples + 1e-6)\n",
        "    return float(average_valid_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt5_Be2GwAM6",
        "colab_type": "text"
      },
      "source": [
        "## Cython Integration <a name=\"cython\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZio9RBF1qbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#---------------------------------cython------------------------------------\n",
        "'''\n",
        "Initialy build in cython and built in C in order to boost the speed of the entire\n",
        "algorithm. But for notebook and Colab compatibility purposes, it was tranlated \n",
        "and adapted as a python function. It takes a little bit of time to build, but it\n",
        "is working.\n",
        "'''\n",
        "\n",
        "def get_sinusoid_encoding_table(n_position, embed_size, padding_idx=0):\n",
        "    \"\"\" Sinusoid position encoding table\n",
        "    n_position: maximum integer that the embedding op could receive\n",
        "    embed_size: embed size\n",
        "    return\n",
        "      a embedding matrix of shape [n_position, embed_size]\n",
        "    \"\"\"\n",
        "\n",
        "    def cal_angle(position, hid_idx):\n",
        "        return position / np.power(sinusoid_base, 2 * (hid_idx // 2) / embed_size)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(embed_size)]\n",
        "\n",
        "    sinusoid_matrix = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position + 1)], dtype=np.float32)\n",
        "\n",
        "    sinusoid_matrix[:, 0::2] = np.sin(sinusoid_matrix[:, 0::2])  # dim 2i\n",
        "    sinusoid_matrix[:, 1::2] = np.cos(sinusoid_matrix[:, 1::2])  # dim 2i+1\n",
        "\n",
        "    sinusoid_matrix[padding_idx] = 0.\n",
        "    return sinusoid_matrix\n",
        "\n",
        "sinusoid_matrix = get_sinusoid_encoding_table(n_position, embedding_size,\n",
        "                                              padding_idx=PAD_ID)\n",
        "\n",
        "\n",
        "def get_ion_index(peptide_mass, prefix_mass, direction):\n",
        "  \"\"\"\n",
        "  :param peptide_mass: neutral mass of a peptide\n",
        "  :param prefix_mass:\n",
        "  :param direction: 0 for forward, 1 for backward\n",
        "  :return: an int32 ndarray of shape [26, 8], each element represent a index of the spectrum embbeding matrix. for out\n",
        "  of bound position, the index is 0\n",
        "  \"\"\"\n",
        "  if direction == 0:\n",
        "    candidate_b_mass = prefix_mass + mass_ID_np\n",
        "    candidate_y_mass = peptide_mass - candidate_b_mass\n",
        "  elif direction == 1:\n",
        "    candidate_y_mass = prefix_mass + mass_ID_np\n",
        "    candidate_b_mass = peptide_mass - candidate_y_mass\n",
        "  candidate_a_mass = candidate_b_mass - mass_CO\n",
        "\n",
        "  # b-ions\n",
        "  candidate_b_H2O = candidate_b_mass - mass_H2O\n",
        "  candidate_b_NH3 = candidate_b_mass - mass_NH3\n",
        "  candidate_b_plus2_charge1 = ((candidate_b_mass + 2 * mass_H) / 2\n",
        "                               - mass_H)\n",
        "\n",
        "  # a-ions\n",
        "  candidate_a_H2O = candidate_a_mass - mass_H2O\n",
        "  candidate_a_NH3 = candidate_a_mass - mass_NH3\n",
        "  candidate_a_plus2_charge1 = ((candidate_a_mass + 2 * mass_H) / 2\n",
        "                               - mass_H)\n",
        "\n",
        "  # y-ions\n",
        "  candidate_y_H2O = candidate_y_mass - mass_H2O\n",
        "  candidate_y_NH3 = candidate_y_mass - mass_NH3\n",
        "  candidate_y_plus2_charge1 = ((candidate_y_mass + 2 * mass_H) / 2\n",
        "                               - mass_H)\n",
        "\n",
        "  # ion_8\n",
        "  b_ions = [candidate_b_mass,\n",
        "            candidate_b_H2O,\n",
        "            candidate_b_NH3,\n",
        "            candidate_b_plus2_charge1]\n",
        "  y_ions = [candidate_y_mass,\n",
        "            candidate_y_H2O,\n",
        "            candidate_y_NH3,\n",
        "            candidate_y_plus2_charge1]\n",
        "  a_ions = [candidate_a_mass,\n",
        "            candidate_a_H2O,\n",
        "            candidate_a_NH3,\n",
        "            candidate_a_plus2_charge1]\n",
        "  ion_mass_list = b_ions + y_ions + a_ions\n",
        "  ion_mass = np.array(ion_mass_list, dtype=np.float32)  # 8 by 26\n",
        "\n",
        "  # ion locations\n",
        "  in_bound_mask = np.logical_and(\n",
        "      ion_mass > 0,\n",
        "      ion_mass <= MZ_MAX).astype(np.float32)\n",
        "  ion_location = ion_mass * in_bound_mask  # 8 by 26, out of bound index would have value 0\n",
        "  return ion_location.transpose()  # 26 by 8\n",
        "\n",
        "\n",
        "def pad_to_length(data: list, length, pad_token=0.):\n",
        "  \"\"\"\n",
        "  pad data to length if len(data) is smaller than length\n",
        "  :param data:\n",
        "  :param length:\n",
        "  :param pad_token:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  for i in range(length - len(data)):\n",
        "    data.append(pad_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WMY5gGL1qbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_peaks(spectrum_mz_list, spectrum_intensity_list, peptide_mass):\n",
        "  \"\"\"\n",
        "\n",
        "  :param spectrum_mz_list:\n",
        "  :param spectrum_intensity_list:\n",
        "  :param peptide_mass: peptide neutral mass\n",
        "  :return:\n",
        "    peak_location: int64, [N]\n",
        "    peak_intensity: float32, [N]\n",
        "    spectrum_representation: float32 [embedding_size]\n",
        "  \"\"\"\n",
        "  charge = 1.0\n",
        "  spectrum_intensity_max = np.max(spectrum_intensity_list)\n",
        "  # charge 1 peptide location\n",
        "  spectrum_mz_list.append(peptide_mass + charge*mass_H)\n",
        "  spectrum_intensity_list.append(spectrum_intensity_max)\n",
        "\n",
        "  # N-terminal, b-ion, peptide_mass_C\n",
        "  # append N-terminal\n",
        "  mass_N = mass_N_terminus - mass_H\n",
        "  spectrum_mz_list.append(mass_N + charge*mass_H)\n",
        "  spectrum_intensity_list.append(spectrum_intensity_max)\n",
        "  # append peptide_mass_C\n",
        "  mass_C = mass_C_terminus + mass_H\n",
        "  peptide_mass_C = peptide_mass - mass_C\n",
        "  spectrum_mz_list.append(peptide_mass_C + charge*mass_H)\n",
        "  spectrum_intensity_list.append(spectrum_intensity_max)\n",
        "\n",
        "  # C-terminal, y-ion, peptide_mass_N\n",
        "  # append C-terminal\n",
        "  mass_C = mass_C_terminus + mass_H\n",
        "  spectrum_mz_list.append(mass_C + charge*mass_H)\n",
        "  spectrum_intensity_list.append(spectrum_intensity_max)\n",
        "\n",
        "\n",
        "  pad_to_length(spectrum_mz_list, MAX_NUM_PEAK)\n",
        "  pad_to_length(spectrum_intensity_list, MAX_NUM_PEAK)\n",
        "\n",
        "  spectrum_mz = np.array(spectrum_mz_list, dtype=np.float32)\n",
        "  spectrum_mz_location = np.ceil(spectrum_mz * spectrum_reso).astype(np.int32)\n",
        "\n",
        "  neutral_mass = spectrum_mz - charge*mass_H\n",
        "  in_bound_mask = np.logical_and(neutral_mass > 0., neutral_mass < MZ_MAX)\n",
        "  neutral_mass[~in_bound_mask] = 0.\n",
        "  # intensity\n",
        "  spectrum_intensity = np.array(spectrum_intensity_list, dtype=np.float32)\n",
        "  norm_intensity = spectrum_intensity / spectrum_intensity_max\n",
        "\n",
        "  spectrum_representation = np.zeros(embedding_size, dtype=np.float32)\n",
        "  for i, loc in enumerate(spectrum_mz_location):\n",
        "    if loc < 0.5 or loc > n_position:\n",
        "      continue\n",
        "    else:\n",
        "      spectrum_representation += sinusoid_matrix[loc] * norm_intensity[i]\n",
        "\n",
        "  top_N_indices = np.argpartition(norm_intensity, -MAX_NUM_PEAK)[-MAX_NUM_PEAK:]\n",
        "  intensity = norm_intensity[top_N_indices]\n",
        "  mass_location = neutral_mass[top_N_indices]\n",
        "\n",
        "  return mass_location, intensity, spectrum_representation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0a02qmu1qbh",
        "colab_type": "text"
      },
      "source": [
        "## Launch the training process <a name=\"train\"></a>\n",
        "This cell lauch the training with all the parameters input before hand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55D1rlwA1qbh",
        "colab_type": "code",
        "outputId": "bf9b8840-9fa2-4dc3-f97e-f0146bf3531d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if 'train' in option:\n",
        "  train()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(training_perp_tab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input spectrum file: Test_files/spectrum_smbp.mgf\n",
            "input feature file: Test_files/features_smbp.csv.train\n",
            "read cached spectrum locations\n",
            "225 steps per epoch\n",
            "input spectrum file: Test_files/spectrum_smbp.mgf\n",
            "input feature file: Test_files/features_smbp.csv.valid\n",
            "read cached spectrum locations\n",
            "load pretrained model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\tlr: 0.001\n",
            "epoch 0 step 9/225, train perplexity: 1.12171429493715\tvalidation perplexity: 1.3551681860440326\tstep time: 0.5048170804977417\n",
            "best valid loss achieved at epoch 0 step 9\n",
            "epoch 0 step 19/225, train perplexity: 1.100287263587582\tvalidation perplexity: 1.36149977064866\tstep time: 0.5170042037963867\n",
            "epoch 0 step 29/225, train perplexity: 1.080862740973174\tvalidation perplexity: 1.355353719592859\tstep time: 0.4936106204986572\n",
            "epoch 0 step 39/225, train perplexity: 1.1595187904177537\tvalidation perplexity: 1.3398646686368727\tstep time: 0.500812315940857\n",
            "best valid loss achieved at epoch 0 step 39\n",
            "epoch 0 step 49/225, train perplexity: 1.0719883376890245\tvalidation perplexity: 1.353454618560996\tstep time: 0.5502294778823853\n",
            "epoch 0 step 59/225, train perplexity: 1.0955457733432845\tvalidation perplexity: 1.353050568920357\tstep time: 0.4890955686569214\n",
            "epoch 0 step 69/225, train perplexity: 1.0987700000440952\tvalidation perplexity: 1.3557972533854756\tstep time: 0.5130147218704224\n",
            "epoch 0 step 79/225, train perplexity: 1.138293867765827\tvalidation perplexity: 1.3600448037670296\tstep time: 0.5362741947174072\n",
            "epoch 0 step 89/225, train perplexity: 1.1587664490680967\tvalidation perplexity: 1.357949832639048\tstep time: 0.5024526357650757\n",
            "epoch 0 step 99/225, train perplexity: 1.1310359888304797\tvalidation perplexity: 1.3831092352130132\tstep time: 0.48826425075531005\n",
            "epoch 0 step 109/225, train perplexity: 1.0816015229059013\tvalidation perplexity: 1.3910552539865049\tstep time: 0.4632055044174194\n",
            "epoch 0 step 119/225, train perplexity: 1.0864176373722831\tvalidation perplexity: 1.3300293123368863\tstep time: 0.46642067432403567\n",
            "best valid loss achieved at epoch 0 step 119\n",
            "epoch 0 step 129/225, train perplexity: 1.1279652320308655\tvalidation perplexity: 1.3491536230213323\tstep time: 0.5098166465759277\n",
            "epoch 0 step 139/225, train perplexity: 1.0574669376268753\tvalidation perplexity: 1.3521874206701194\tstep time: 0.5407689094543457\n",
            "epoch 0 step 149/225, train perplexity: 1.0985761859843324\tvalidation perplexity: 1.3530323930576025\tstep time: 0.49382948875427246\n",
            "epoch 0 step 159/225, train perplexity: 1.0900995648138816\tvalidation perplexity: 1.3730413291315136\tstep time: 0.4901067495346069\n",
            "epoch 0 step 169/225, train perplexity: 1.151921400985128\tvalidation perplexity: 1.3966161266011794\tstep time: 0.5066668748855591\n",
            "epoch 0 step 179/225, train perplexity: 1.1014142151507897\tvalidation perplexity: 1.3697862502287186\tstep time: 0.5077869653701782\n",
            "epoch 0 step 189/225, train perplexity: 1.079505655296176\tvalidation perplexity: 1.4069453641140053\tstep time: 0.510870361328125\n",
            "epoch 0 step 199/225, train perplexity: 1.0498550792014931\tvalidation perplexity: 1.3879290066864682\tstep time: 0.4931591033935547\n",
            "epoch 0 step 209/225, train perplexity: 1.0557484296465514\tvalidation perplexity: 1.382896534203917\tstep time: 0.47557318210601807\n",
            "epoch 0 step 219/225, train perplexity: 1.073394936398499\tvalidation perplexity: 1.3991152160313671\tstep time: 0.4966862916946411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  5%|▌         | 1/20 [02:48<53:12, 168.05s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1\tlr: 0.001\n",
            "Epoch    23: reducing learning rate of group 0 to 5.0000e-04.\n",
            "epoch 1 step 9/225, train perplexity: 1.0410094894866742\tvalidation perplexity: 1.4048856963639822\tstep time: 0.8203651428222656\n",
            "epoch 1 step 19/225, train perplexity: 1.0599629137512379\tvalidation perplexity: 1.3954242967988015\tstep time: 0.507679557800293\n",
            "epoch 1 step 29/225, train perplexity: 1.042858802037829\tvalidation perplexity: 1.383926612593134\tstep time: 0.49432346820831297\n",
            "epoch 1 step 39/225, train perplexity: 1.0751076362816798\tvalidation perplexity: 1.3866604440434582\tstep time: 0.5090654850006103\n",
            "epoch 1 step 49/225, train perplexity: 1.0295513488793762\tvalidation perplexity: 1.3982204106527618\tstep time: 0.46465330123901366\n",
            "epoch 1 step 59/225, train perplexity: 1.0400415836364498\tvalidation perplexity: 1.387029056295043\tstep time: 0.5106380224227905\n",
            "epoch 1 step 69/225, train perplexity: 1.0471882890975446\tvalidation perplexity: 1.408106674935594\tstep time: 0.508194875717163\n",
            "epoch 1 step 79/225, train perplexity: 1.0614662382999742\tvalidation perplexity: 1.3960923246279464\tstep time: 0.4834390878677368\n",
            "epoch 1 step 89/225, train perplexity: 1.0416989192009358\tvalidation perplexity: 1.386128704792936\tstep time: 0.4847005605697632\n",
            "epoch 1 step 99/225, train perplexity: 1.0446120066168314\tvalidation perplexity: 1.3946455049028428\tstep time: 0.49807722568511964\n",
            "epoch 1 step 109/225, train perplexity: 1.0412570944418322\tvalidation perplexity: 1.3961194722044454\tstep time: 0.5490345239639283\n",
            "epoch 1 step 119/225, train perplexity: 1.0254169201154586\tvalidation perplexity: 1.3899339762760372\tstep time: 0.4587927579879761\n",
            "epoch 1 step 129/225, train perplexity: 1.064652093304658\tvalidation perplexity: 1.3933629540568553\tstep time: 0.480010461807251\n",
            "epoch 1 step 139/225, train perplexity: 1.0466874555661732\tvalidation perplexity: 1.3739717517183878\tstep time: 0.48923940658569337\n",
            "epoch 1 step 149/225, train perplexity: 1.027136143604195\tvalidation perplexity: 1.3984341270941103\tstep time: 0.49408531188964844\n",
            "epoch 1 step 159/225, train perplexity: 1.1416414112644495\tvalidation perplexity: 1.4078640669358067\tstep time: 0.49810740947723386\n",
            "epoch 1 step 169/225, train perplexity: 1.0570005874092556\tvalidation perplexity: 1.3970466082828836\tstep time: 0.5180619955062866\n",
            "epoch 1 step 179/225, train perplexity: 1.0271951595631004\tvalidation perplexity: 1.3984300917490644\tstep time: 0.4814086198806763\n",
            "epoch 1 step 189/225, train perplexity: 1.039466792507481\tvalidation perplexity: 1.4150664691802197\tstep time: 0.5235189914703369\n",
            "epoch 1 step 199/225, train perplexity: 1.0300797655201381\tvalidation perplexity: 1.403838425847487\tstep time: 0.48949580192565917\n",
            "epoch 1 step 209/225, train perplexity: 1.1065631557179432\tvalidation perplexity: 1.414819637912062\tstep time: 0.4691173076629639\n",
            "Epoch    44: reducing learning rate of group 0 to 2.5000e-04.\n",
            "epoch 1 step 219/225, train perplexity: 1.043039732165586\tvalidation perplexity: 1.4237411021211626\tstep time: 0.5382940769195557\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 10%|█         | 2/20 [05:34<50:17, 167.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 2\tlr: 0.0001\n",
            "epoch 2 step 9/225, train perplexity: 1.0328559594815248\tvalidation perplexity: 1.4076732796772984\tstep time: 0.7857954978942872\n",
            "epoch 2 step 19/225, train perplexity: 1.025896721961368\tvalidation perplexity: 1.4064128770209727\tstep time: 0.5013789892196655\n",
            "epoch 2 step 29/225, train perplexity: 1.0318161145335687\tvalidation perplexity: 1.4163486048042062\tstep time: 0.5329119920730591\n",
            "epoch 2 step 39/225, train perplexity: 1.0324622698074062\tvalidation perplexity: 1.4233205397053943\tstep time: 0.49419260025024414\n",
            "epoch 2 step 49/225, train perplexity: 1.0309935203466774\tvalidation perplexity: 1.419250118171423\tstep time: 0.5207384347915649\n",
            "epoch 2 step 59/225, train perplexity: 1.025562070364562\tvalidation perplexity: 1.4188329769024135\tstep time: 0.5160293340682983\n",
            "epoch 2 step 69/225, train perplexity: 1.028425282686497\tvalidation perplexity: 1.4224887915605207\tstep time: 0.5139033555984497\n",
            "epoch 2 step 79/225, train perplexity: 1.0470153840849397\tvalidation perplexity: 1.4242523486075815\tstep time: 0.5037606000900269\n",
            "epoch 2 step 89/225, train perplexity: 1.0373523383262147\tvalidation perplexity: 1.4246017654112264\tstep time: 0.5043683290481568\n",
            "epoch 2 step 99/225, train perplexity: 1.0244234707272826\tvalidation perplexity: 1.4249707094786381\tstep time: 0.49325075149536135\n",
            "epoch 2 step 109/225, train perplexity: 1.0305228900940557\tvalidation perplexity: 1.4266506113965196\tstep time: 0.5153359651565552\n",
            "epoch 2 step 119/225, train perplexity: 1.0493231829798437\tvalidation perplexity: 1.4229536038856345\tstep time: 0.4924540758132935\n",
            "epoch 2 step 129/225, train perplexity: 1.0736003056797563\tvalidation perplexity: 1.4186533905561383\tstep time: 0.49185452461242674\n",
            "epoch 2 step 139/225, train perplexity: 1.037964019631722\tvalidation perplexity: 1.4208457397734098\tstep time: 0.5015476942062378\n",
            "epoch 2 step 149/225, train perplexity: 1.0316361243982415\tvalidation perplexity: 1.426163828616077\tstep time: 0.5164780139923095\n",
            "epoch 2 step 159/225, train perplexity: 1.0347510317086142\tvalidation perplexity: 1.4294494915850373\tstep time: 0.5047978162765503\n",
            "epoch 2 step 169/225, train perplexity: 1.0181859896011893\tvalidation perplexity: 1.4243972734112667\tstep time: 0.5292638063430786\n",
            "epoch 2 step 179/225, train perplexity: 1.0423196246925233\tvalidation perplexity: 1.4223483624774085\tstep time: 0.47521564960479734\n",
            "epoch 2 step 189/225, train perplexity: 1.0235954983208755\tvalidation perplexity: 1.423274426128319\tstep time: 0.5022330522537232\n",
            "epoch 2 step 199/225, train perplexity: 1.0351853023505038\tvalidation perplexity: 1.42582303759203\tstep time: 0.49782187938690187\n",
            "Epoch    65: reducing learning rate of group 0 to 5.0000e-05.\n",
            "epoch 2 step 209/225, train perplexity: 1.0357541414168567\tvalidation perplexity: 1.4190735601461033\tstep time: 0.5158045768737793\n",
            "epoch 2 step 219/225, train perplexity: 1.036634272848478\tvalidation perplexity: 1.4194092769470659\tstep time: 0.49049882888793944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 15%|█▌        | 3/20 [08:22<47:28, 167.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 3\tlr: 0.0001\n",
            "epoch 3 step 9/225, train perplexity: 1.0143825036580398\tvalidation perplexity: 1.4230603295649702\tstep time: 0.7789069652557373\n",
            "epoch 3 step 19/225, train perplexity: 1.0208255231081325\tvalidation perplexity: 1.4233845006719932\tstep time: 0.5094246149063111\n",
            "epoch 3 step 29/225, train perplexity: 1.0269153125014323\tvalidation perplexity: 1.4224860183688195\tstep time: 0.5145751476287842\n",
            "epoch 3 step 39/225, train perplexity: 1.0164222748996126\tvalidation perplexity: 1.4235480794660236\tstep time: 0.49779937267303465\n",
            "epoch 3 step 49/225, train perplexity: 1.0279523687305576\tvalidation perplexity: 1.427059663097409\tstep time: 0.46431376934051516\n",
            "epoch 3 step 59/225, train perplexity: 1.0238313293636965\tvalidation perplexity: 1.4307184977664391\tstep time: 0.4851961851119995\n",
            "epoch 3 step 69/225, train perplexity: 1.0181499203415205\tvalidation perplexity: 1.434410730993146\tstep time: 0.49321761131286623\n",
            "epoch 3 step 79/225, train perplexity: 1.0293898863894155\tvalidation perplexity: 1.4332365134609983\tstep time: 0.49341597557067873\n",
            "epoch 3 step 89/225, train perplexity: 1.0607268576606448\tvalidation perplexity: 1.431459692864774\tstep time: 0.504219365119934\n",
            "epoch 3 step 99/225, train perplexity: 1.0353297178954501\tvalidation perplexity: 1.4256234378430275\tstep time: 0.4738388299942017\n",
            "epoch 3 step 109/225, train perplexity: 1.0290605126497179\tvalidation perplexity: 1.4261471849096978\tstep time: 0.5293487310409546\n",
            "epoch 3 step 119/225, train perplexity: 1.0141937799205625\tvalidation perplexity: 1.4260220196737001\tstep time: 0.49649221897125245\n",
            "epoch 3 step 129/225, train perplexity: 1.019945606273865\tvalidation perplexity: 1.4232958132086484\tstep time: 0.5039927244186402\n",
            "epoch 3 step 139/225, train perplexity: 1.0318949042086751\tvalidation perplexity: 1.4261726951384885\tstep time: 0.4907132387161255\n",
            "epoch 3 step 149/225, train perplexity: 1.031484331376801\tvalidation perplexity: 1.4226326733315848\tstep time: 0.5032348394393921\n",
            "epoch 3 step 159/225, train perplexity: 1.0220653840544565\tvalidation perplexity: 1.4198017395603046\tstep time: 0.5070379734039306\n",
            "epoch 3 step 169/225, train perplexity: 1.0375879644462513\tvalidation perplexity: 1.421226848432167\tstep time: 0.5233848810195922\n",
            "epoch 3 step 179/225, train perplexity: 1.045736826183087\tvalidation perplexity: 1.4227876423950678\tstep time: 0.5019372224807739\n",
            "epoch 3 step 189/225, train perplexity: 1.0264771673283648\tvalidation perplexity: 1.42038728185058\tstep time: 0.5016988515853882\n",
            "Epoch    86: reducing learning rate of group 0 to 5.0000e-05.\n",
            "epoch 3 step 199/225, train perplexity: 1.0506930276785578\tvalidation perplexity: 1.4256813455053161\tstep time: 0.47308423519134524\n",
            "epoch 3 step 209/225, train perplexity: 1.0248691959849117\tvalidation perplexity: 1.425499398980269\tstep time: 0.5261153936386108\n",
            "epoch 3 step 219/225, train perplexity: 1.031024707709666\tvalidation perplexity: 1.4307196014280634\tstep time: 0.5095494508743286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 20%|██        | 4/20 [11:08<44:36, 167.27s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 4\tlr: 0.0001\n",
            "epoch 4 step 9/225, train perplexity: 1.0281270319706517\tvalidation perplexity: 1.4304285566451787\tstep time: 0.8019822597503662\n",
            "epoch 4 step 19/225, train perplexity: 1.0254776709886084\tvalidation perplexity: 1.43068615875978\tstep time: 0.5138920068740844\n",
            "epoch 4 step 29/225, train perplexity: 1.027016775127986\tvalidation perplexity: 1.439118882990315\tstep time: 0.5145364999771118\n",
            "epoch 4 step 39/225, train perplexity: 1.0364210481945333\tvalidation perplexity: 1.4415471092680336\tstep time: 0.48632447719573973\n",
            "epoch 4 step 49/225, train perplexity: 1.025638764368682\tvalidation perplexity: 1.4388821519932173\tstep time: 0.5196708679199219\n",
            "epoch 4 step 59/225, train perplexity: 1.0271434309713485\tvalidation perplexity: 1.4397289998568765\tstep time: 0.5012337446212769\n",
            "epoch 4 step 69/225, train perplexity: 1.0193116322387272\tvalidation perplexity: 1.4393389260137401\tstep time: 0.5160807132720947\n",
            "epoch 4 step 79/225, train perplexity: 1.018939040030289\tvalidation perplexity: 1.4407919130802505\tstep time: 0.5009461164474487\n",
            "epoch 4 step 89/225, train perplexity: 1.0280491963708183\tvalidation perplexity: 1.4432262410360133\tstep time: 0.4831101894378662\n",
            "epoch 4 step 99/225, train perplexity: 1.038918257283389\tvalidation perplexity: 1.4477715935939686\tstep time: 0.5072404384613037\n",
            "epoch 4 step 109/225, train perplexity: 1.0244240202710035\tvalidation perplexity: 1.447597250266194\tstep time: 0.47849349975585936\n",
            "epoch 4 step 119/225, train perplexity: 1.0385784294480065\tvalidation perplexity: 1.4528008497548912\tstep time: 0.5057437181472778\n",
            "epoch 4 step 129/225, train perplexity: 1.0856547451560306\tvalidation perplexity: 1.444431370800905\tstep time: 0.5044126033782959\n",
            "epoch 4 step 139/225, train perplexity: 1.0118836322245235\tvalidation perplexity: 1.4388744074396114\tstep time: 0.4886197566986084\n",
            "epoch 4 step 149/225, train perplexity: 1.0200670979111577\tvalidation perplexity: 1.4388159410303247\tstep time: 0.5258875608444213\n",
            "epoch 4 step 159/225, train perplexity: 1.0143644351554055\tvalidation perplexity: 1.4476635212439606\tstep time: 0.5126601934432984\n",
            "epoch 4 step 169/225, train perplexity: 1.018424024890597\tvalidation perplexity: 1.4485001220356741\tstep time: 0.4838233947753906\n",
            "epoch 4 step 179/225, train perplexity: 1.0328893271733177\tvalidation perplexity: 1.4408332410819147\tstep time: 0.46996755599975587\n",
            "Epoch   107: reducing learning rate of group 0 to 5.0000e-05.\n",
            "epoch 4 step 189/225, train perplexity: 1.0214228094350173\tvalidation perplexity: 1.4289978303999142\tstep time: 0.45566983222961427\n",
            "epoch 4 step 199/225, train perplexity: 1.0291216116529667\tvalidation perplexity: 1.4284909962800827\tstep time: 0.5180683374404907\n",
            "epoch 4 step 209/225, train perplexity: 1.0324402235001344\tvalidation perplexity: 1.4293447068829408\tstep time: 0.5498777389526367\n",
            "epoch 4 step 219/225, train perplexity: 1.0186540291162944\tvalidation perplexity: 1.4316981169738614\tstep time: 0.5078149795532226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 25%|██▌       | 5/20 [13:56<41:49, 167.32s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 5\tlr: 1.0000000000000003e-05\n",
            "epoch 5 step 9/225, train perplexity: 1.0202431977444268\tvalidation perplexity: 1.4327699619533674\tstep time: 0.823484230041504\n",
            "epoch 5 step 19/225, train perplexity: 1.0343071332229414\tvalidation perplexity: 1.4343285961386523\tstep time: 0.5054529905319214\n",
            "epoch 5 step 29/225, train perplexity: 1.0128944156385724\tvalidation perplexity: 1.4334304323891696\tstep time: 0.5021158933639527\n",
            "epoch 5 step 39/225, train perplexity: 1.0209830958367743\tvalidation perplexity: 1.4321411272236138\tstep time: 0.5281268119812011\n",
            "epoch 5 step 49/225, train perplexity: 1.0188496897868748\tvalidation perplexity: 1.4322021334240156\tstep time: 0.49933321475982667\n",
            "epoch 5 step 59/225, train perplexity: 1.0338968523657364\tvalidation perplexity: 1.4302094946094137\tstep time: 0.49778194427490235\n",
            "epoch 5 step 69/225, train perplexity: 1.017051595093042\tvalidation perplexity: 1.431061173312561\tstep time: 0.4926232099533081\n",
            "epoch 5 step 79/225, train perplexity: 1.0196375665625816\tvalidation perplexity: 1.4307163340176554\tstep time: 0.45690460205078126\n",
            "epoch 5 step 89/225, train perplexity: 1.0209459633474607\tvalidation perplexity: 1.4297956341840743\tstep time: 0.5097045183181763\n",
            "epoch 5 step 99/225, train perplexity: 1.027828659071307\tvalidation perplexity: 1.4323327559546737\tstep time: 0.5013465642929077\n",
            "epoch 5 step 109/225, train perplexity: 1.031065636793766\tvalidation perplexity: 1.4317532908045714\tstep time: 0.5207131385803223\n",
            "epoch 5 step 119/225, train perplexity: 1.037435071526792\tvalidation perplexity: 1.4324791477741647\tstep time: 0.5098215341567993\n",
            "epoch 5 step 129/225, train perplexity: 1.0159731740822922\tvalidation perplexity: 1.4314793891727862\tstep time: 0.5077242374420166\n",
            "epoch 5 step 139/225, train perplexity: 1.0379050229359894\tvalidation perplexity: 1.4329117174520085\tstep time: 0.48050875663757325\n",
            "epoch 5 step 149/225, train perplexity: 1.0256115854527892\tvalidation perplexity: 1.4332704865002182\tstep time: 0.49540302753448484\n",
            "epoch 5 step 159/225, train perplexity: 1.0237461628401594\tvalidation perplexity: 1.4350387435414962\tstep time: 0.49237229824066164\n",
            "epoch 5 step 169/225, train perplexity: 1.0259231230406063\tvalidation perplexity: 1.43656326440578\tstep time: 0.4903200626373291\n",
            "epoch 5 step 179/225, train perplexity: 1.0173828594353618\tvalidation perplexity: 1.4351513023332207\tstep time: 0.4895617008209229\n",
            "epoch 5 step 189/225, train perplexity: 1.0203146135356764\tvalidation perplexity: 1.4362714495543905\tstep time: 0.5016517400741577\n",
            "epoch 5 step 199/225, train perplexity: 1.035029748244075\tvalidation perplexity: 1.4363683274776406\tstep time: 0.511033010482788\n",
            "epoch 5 step 209/225, train perplexity: 1.0194181311659083\tvalidation perplexity: 1.4347085389171481\tstep time: 0.46353461742401125\n",
            "epoch 5 step 219/225, train perplexity: 1.0156884479028887\tvalidation perplexity: 1.4338751572648403\tstep time: 0.5005855321884155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 30%|███       | 6/20 [16:42<38:59, 167.10s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 6\tlr: 1.0000000000000003e-05\n",
            "epoch 6 step 9/225, train perplexity: 1.0207062530142348\tvalidation perplexity: 1.4364509395549359\tstep time: 0.8159302949905396\n",
            "epoch 6 step 19/225, train perplexity: 1.0219159528903752\tvalidation perplexity: 1.438297320701887\tstep time: 0.47282588481903076\n",
            "epoch 6 step 29/225, train perplexity: 1.012749644420196\tvalidation perplexity: 1.438946881128826\tstep time: 0.46428885459899905\n",
            "epoch 6 step 39/225, train perplexity: 1.0167694112911714\tvalidation perplexity: 1.4391096283043536\tstep time: 0.47384617328643797\n",
            "epoch 6 step 49/225, train perplexity: 1.0305909884714264\tvalidation perplexity: 1.439110898336485\tstep time: 0.5153362512588501\n",
            "epoch 6 step 59/225, train perplexity: 1.0350893064324493\tvalidation perplexity: 1.4364324605417365\tstep time: 0.5421351194381714\n",
            "epoch 6 step 69/225, train perplexity: 1.0212600213554681\tvalidation perplexity: 1.4359973268721198\tstep time: 0.5151298522949219\n",
            "epoch 6 step 79/225, train perplexity: 1.0250249566715748\tvalidation perplexity: 1.4371724741929608\tstep time: 0.49207916259765627\n",
            "epoch 6 step 89/225, train perplexity: 1.0173597783140225\tvalidation perplexity: 1.4390113528259338\tstep time: 0.4976418256759644\n",
            "epoch 6 step 99/225, train perplexity: 1.024802626718585\tvalidation perplexity: 1.438718221727118\tstep time: 0.49661669731140134\n",
            "epoch 6 step 109/225, train perplexity: 1.0281194561168245\tvalidation perplexity: 1.4377552207092272\tstep time: 0.4987321376800537\n",
            "epoch 6 step 119/225, train perplexity: 1.0170697663998338\tvalidation perplexity: 1.4389891486757809\tstep time: 0.5032341241836548\n",
            "epoch 6 step 129/225, train perplexity: 1.026326183089994\tvalidation perplexity: 1.438179844956472\tstep time: 0.5061874151229858\n",
            "epoch 6 step 139/225, train perplexity: 1.0377509003234149\tvalidation perplexity: 1.441087186158493\tstep time: 0.5096317529678345\n",
            "epoch 6 step 149/225, train perplexity: 1.0319166620897668\tvalidation perplexity: 1.4397908299883386\tstep time: 0.513484263420105\n",
            "epoch 6 step 159/225, train perplexity: 1.036234564713648\tvalidation perplexity: 1.4392507112180573\tstep time: 0.4784940242767334\n",
            "epoch 6 step 169/225, train perplexity: 1.02486260052295\tvalidation perplexity: 1.4374721184988102\tstep time: 0.5012102365493775\n",
            "epoch 6 step 179/225, train perplexity: 1.023505556737943\tvalidation perplexity: 1.436837383595575\tstep time: 0.47540898323059083\n",
            "epoch 6 step 189/225, train perplexity: 1.0206267587665454\tvalidation perplexity: 1.4395185266330124\tstep time: 0.5162694454193115\n",
            "epoch 6 step 199/225, train perplexity: 1.0344597963794586\tvalidation perplexity: 1.4400426038291256\tstep time: 0.47886316776275634\n",
            "epoch 6 step 209/225, train perplexity: 1.0230574079818175\tvalidation perplexity: 1.440820643440484\tstep time: 0.49091739654541017\n",
            "epoch 6 step 219/225, train perplexity: 1.0102367534176453\tvalidation perplexity: 1.4414854264681711\tstep time: 0.5003139495849609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 35%|███▌      | 7/20 [19:29<36:12, 167.08s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 7\tlr: 1.0000000000000003e-05\n",
            "epoch 7 step 9/225, train perplexity: 1.029080510553138\tvalidation perplexity: 1.4397574346904396\tstep time: 0.8396673202514648\n",
            "epoch 7 step 19/225, train perplexity: 1.0466848508940454\tvalidation perplexity: 1.4400825123946008\tstep time: 0.5129099369049073\n",
            "epoch 7 step 29/225, train perplexity: 1.0190216029759998\tvalidation perplexity: 1.4411735776730286\tstep time: 0.47341873645782473\n",
            "epoch 7 step 39/225, train perplexity: 1.0195729380532903\tvalidation perplexity: 1.4394384245684082\tstep time: 0.4914422035217285\n",
            "epoch 7 step 49/225, train perplexity: 1.0251608074538185\tvalidation perplexity: 1.4408481519876246\tstep time: 0.5117789268493652\n",
            "epoch 7 step 59/225, train perplexity: 1.0140426815290673\tvalidation perplexity: 1.4395234786656612\tstep time: 0.4904134273529053\n",
            "epoch 7 step 69/225, train perplexity: 1.0239263574865343\tvalidation perplexity: 1.4417261283537044\tstep time: 0.5010897159576416\n",
            "epoch 7 step 79/225, train perplexity: 1.0297050669485113\tvalidation perplexity: 1.4422097592545375\tstep time: 0.5132694482803345\n",
            "epoch 7 step 89/225, train perplexity: 1.0229381819806898\tvalidation perplexity: 1.4417976120988547\tstep time: 0.5012353420257568\n",
            "epoch 7 step 99/225, train perplexity: 1.0194610737543237\tvalidation perplexity: 1.4430779089703465\tstep time: 0.5178418636322022\n",
            "epoch 7 step 109/225, train perplexity: 1.0229201192245772\tvalidation perplexity: 1.442306246825381\tstep time: 0.4931231260299683\n",
            "epoch 7 step 119/225, train perplexity: 1.0293249599245435\tvalidation perplexity: 1.4428237226097447\tstep time: 0.4932332277297974\n",
            "epoch 7 step 129/225, train perplexity: 1.0155063947087497\tvalidation perplexity: 1.4418681339695267\tstep time: 0.48581953048706056\n",
            "epoch 7 step 139/225, train perplexity: 1.0223979957855422\tvalidation perplexity: 1.4407528176560584\tstep time: 0.4815320253372192\n",
            "epoch 7 step 149/225, train perplexity: 1.0200202653185517\tvalidation perplexity: 1.4423083684156877\tstep time: 0.5094342947006225\n",
            "epoch 7 step 159/225, train perplexity: 1.0205016972323644\tvalidation perplexity: 1.4415424904383258\tstep time: 0.48428804874420167\n",
            "epoch 7 step 169/225, train perplexity: 1.0235750007212268\tvalidation perplexity: 1.4403054486788855\tstep time: 0.4797399044036865\n",
            "epoch 7 step 179/225, train perplexity: 1.0260131241817347\tvalidation perplexity: 1.4425757635016336\tstep time: 0.5056448698043823\n",
            "epoch 7 step 189/225, train perplexity: 1.0196646840287\tvalidation perplexity: 1.4436629661884344\tstep time: 0.5402399063110351\n",
            "epoch 7 step 199/225, train perplexity: 1.027696346384118\tvalidation perplexity: 1.4454455556670225\tstep time: 0.46305389404296876\n",
            "epoch 7 step 209/225, train perplexity: 1.0167204348242733\tvalidation perplexity: 1.4430784391281528\tstep time: 0.49100399017333984\n",
            "epoch 7 step 219/225, train perplexity: 1.0243231171559724\tvalidation perplexity: 1.4439759268274013\tstep time: 0.47787766456604003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 40%|████      | 8/20 [22:16<33:22, 166.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 8\tlr: 1.0000000000000002e-06\n",
            "epoch 8 step 9/225, train perplexity: 1.0204148157765511\tvalidation perplexity: 1.4428802586439315\tstep time: 0.8352303743362427\n",
            "epoch 8 step 19/225, train perplexity: 1.0198114666482991\tvalidation perplexity: 1.4440361792568324\tstep time: 0.49557678699493407\n",
            "epoch 8 step 29/225, train perplexity: 1.022267840745835\tvalidation perplexity: 1.4416742421185071\tstep time: 0.5151121854782105\n",
            "epoch 8 step 39/225, train perplexity: 1.038682913345345\tvalidation perplexity: 1.4439068748453505\tstep time: 0.5031499862670898\n",
            "epoch 8 step 49/225, train perplexity: 1.0225156046851134\tvalidation perplexity: 1.4418124795283151\tstep time: 0.4877768516540527\n",
            "epoch 8 step 59/225, train perplexity: 1.0206911707985653\tvalidation perplexity: 1.4422683529523463\tstep time: 0.5532869100570679\n",
            "epoch 8 step 69/225, train perplexity: 1.0305445192297085\tvalidation perplexity: 1.4408956463670883\tstep time: 0.5074589729309082\n",
            "epoch 8 step 79/225, train perplexity: 1.0269719686603018\tvalidation perplexity: 1.4411891064325923\tstep time: 0.4820920705795288\n",
            "epoch 8 step 89/225, train perplexity: 1.0194912856090823\tvalidation perplexity: 1.4419276517844752\tstep time: 0.5038820028305053\n",
            "epoch 8 step 99/225, train perplexity: 1.016699495410659\tvalidation perplexity: 1.441237808026339\tstep time: 0.472973895072937\n",
            "epoch 8 step 109/225, train perplexity: 1.0163705928180884\tvalidation perplexity: 1.4417793154657712\tstep time: 0.5355698585510253\n",
            "epoch 8 step 119/225, train perplexity: 1.015859101088042\tvalidation perplexity: 1.441181381282872\tstep time: 0.47824575901031496\n",
            "epoch 8 step 129/225, train perplexity: 1.0183651353903\tvalidation perplexity: 1.4418490488671865\tstep time: 0.4934405326843262\n",
            "epoch 8 step 139/225, train perplexity: 1.0146049730806956\tvalidation perplexity: 1.4416821063857346\tstep time: 0.49077274799346926\n",
            "epoch 8 step 149/225, train perplexity: 1.0252173840010697\tvalidation perplexity: 1.4433752644604858\tstep time: 0.5209352493286132\n",
            "epoch 8 step 159/225, train perplexity: 1.0233056530064655\tvalidation perplexity: 1.4446324235191794\tstep time: 0.47970075607299806\n",
            "epoch 8 step 169/225, train perplexity: 1.0107709123866175\tvalidation perplexity: 1.443025702987233\tstep time: 0.48374555110931394\n",
            "epoch 8 step 179/225, train perplexity: 1.0288777893367245\tvalidation perplexity: 1.4425078001962126\tstep time: 0.4881597995758057\n",
            "epoch 8 step 189/225, train perplexity: 1.0313820719811926\tvalidation perplexity: 1.4450622076102035\tstep time: 0.47858610153198244\n",
            "epoch 8 step 199/225, train perplexity: 1.0384684665080353\tvalidation perplexity: 1.4425732981160433\tstep time: 0.5068025350570678\n",
            "epoch 8 step 209/225, train perplexity: 1.0237593241816991\tvalidation perplexity: 1.440332526437266\tstep time: 0.5026673316955567\n",
            "epoch 8 step 219/225, train perplexity: 1.0216785134791442\tvalidation perplexity: 1.4433940298725776\tstep time: 0.5090636968612671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 45%|████▌     | 9/20 [25:02<30:35, 166.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 9\tlr: 1.0000000000000002e-06\n",
            "epoch 9 step 9/225, train perplexity: 1.024579148404738\tvalidation perplexity: 1.4430382367422092\tstep time: 0.8435780048370362\n",
            "epoch 9 step 19/225, train perplexity: 1.0343488052775671\tvalidation perplexity: 1.4434505013601264\tstep time: 0.5152217149734497\n",
            "epoch 9 step 29/225, train perplexity: 1.0307253518046102\tvalidation perplexity: 1.4436531982905885\tstep time: 0.48435943126678466\n",
            "epoch 9 step 39/225, train perplexity: 1.0091981081698622\tvalidation perplexity: 1.444296523092907\tstep time: 0.5529915809631347\n",
            "epoch 9 step 49/225, train perplexity: 1.009816321443774\tvalidation perplexity: 1.4453119051215388\tstep time: 0.5130903005599976\n",
            "epoch 9 step 59/225, train perplexity: 1.0169128108912513\tvalidation perplexity: 1.441979976881853\tstep time: 0.4721663475036621\n",
            "epoch 9 step 69/225, train perplexity: 1.0072307676807135\tvalidation perplexity: 1.441637665055411\tstep time: 0.5211693286895752\n",
            "epoch 9 step 79/225, train perplexity: 1.014009604429097\tvalidation perplexity: 1.4431719745001697\tstep time: 0.4989962100982666\n",
            "epoch 9 step 89/225, train perplexity: 1.0313303227196524\tvalidation perplexity: 1.4424303635580589\tstep time: 0.48917360305786134\n",
            "epoch 9 step 99/225, train perplexity: 1.0394404185008121\tvalidation perplexity: 1.440945328477198\tstep time: 0.45988614559173585\n",
            "epoch 9 step 109/225, train perplexity: 1.0354740841534185\tvalidation perplexity: 1.44115457567639\tstep time: 0.4797712564468384\n",
            "epoch 9 step 119/225, train perplexity: 1.0375696507388763\tvalidation perplexity: 1.442526670186672\tstep time: 0.48913466930389404\n",
            "epoch 9 step 129/225, train perplexity: 1.0517716202312806\tvalidation perplexity: 1.4453773712384193\tstep time: 0.5119204044342041\n",
            "epoch 9 step 139/225, train perplexity: 1.0281884451737398\tvalidation perplexity: 1.4429986694434198\tstep time: 0.5090242147445678\n",
            "epoch 9 step 149/225, train perplexity: 1.0176951730830435\tvalidation perplexity: 1.440365489682805\tstep time: 0.5076028108596802\n",
            "epoch 9 step 159/225, train perplexity: 1.0402364638236128\tvalidation perplexity: 1.4411875022226976\tstep time: 0.49411702156066895\n",
            "epoch 9 step 169/225, train perplexity: 1.0128446504679507\tvalidation perplexity: 1.4420750172406904\tstep time: 0.504419207572937\n",
            "epoch 9 step 179/225, train perplexity: 1.0331529667588135\tvalidation perplexity: 1.4423457947764329\tstep time: 0.4913088321685791\n",
            "epoch 9 step 189/225, train perplexity: 1.0171986040617258\tvalidation perplexity: 1.4440203559264664\tstep time: 0.5123618364334106\n",
            "epoch 9 step 199/225, train perplexity: 1.0220805988987796\tvalidation perplexity: 1.4443907501247502\tstep time: 0.5139473438262939\n",
            "epoch 9 step 209/225, train perplexity: 1.0182403206360728\tvalidation perplexity: 1.4429639045002658\tstep time: 0.46967995166778564\n",
            "epoch 9 step 219/225, train perplexity: 1.0300343860640937\tvalidation perplexity: 1.4418048494787536\tstep time: 0.49550130367279055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 50%|█████     | 10/20 [27:49<27:48, 166.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 10\tlr: 1.0000000000000002e-06\n",
            "epoch 10 step 9/225, train perplexity: 1.0207904327152477\tvalidation perplexity: 1.442480134637588\tstep time: 0.8161631345748901\n",
            "epoch 10 step 19/225, train perplexity: 1.0326519213372582\tvalidation perplexity: 1.4424328837037546\tstep time: 0.5036569833755493\n",
            "epoch 10 step 29/225, train perplexity: 1.0222472840488774\tvalidation perplexity: 1.4436424009547504\tstep time: 0.4873538017272949\n",
            "epoch 10 step 39/225, train perplexity: 1.0298193750216904\tvalidation perplexity: 1.4437732669942307\tstep time: 0.5120074510574341\n",
            "epoch 10 step 49/225, train perplexity: 1.0268268043884634\tvalidation perplexity: 1.4434787659089674\tstep time: 0.5227430582046508\n",
            "epoch 10 step 59/225, train perplexity: 1.0334137940076662\tvalidation perplexity: 1.4442990037706764\tstep time: 0.47067196369171144\n",
            "epoch 10 step 69/225, train perplexity: 1.015518025781399\tvalidation perplexity: 1.44248005646369\tstep time: 0.48784101009368896\n",
            "epoch 10 step 79/225, train perplexity: 1.0119614851205734\tvalidation perplexity: 1.4443845885540405\tstep time: 0.4989269971847534\n",
            "epoch 10 step 89/225, train perplexity: 1.01215073799067\tvalidation perplexity: 1.4436285607219714\tstep time: 0.4854905366897583\n",
            "epoch 10 step 99/225, train perplexity: 1.0171303012447\tvalidation perplexity: 1.445454211890387\tstep time: 0.4851630210876465\n",
            "epoch 10 step 109/225, train perplexity: 1.0382227004469236\tvalidation perplexity: 1.443986245548863\tstep time: 0.5256254196166992\n",
            "epoch 10 step 119/225, train perplexity: 1.0261230373358077\tvalidation perplexity: 1.4446682970051619\tstep time: 0.5098761320114136\n",
            "epoch 10 step 129/225, train perplexity: 1.014076557753072\tvalidation perplexity: 1.444986322459106\tstep time: 0.5096842288970947\n",
            "epoch 10 step 139/225, train perplexity: 1.0330609192360776\tvalidation perplexity: 1.4433245278434064\tstep time: 0.49806942939758303\n",
            "epoch 10 step 149/225, train perplexity: 1.021478338717348\tvalidation perplexity: 1.4432510652903952\tstep time: 0.5724796295166016\n",
            "epoch 10 step 159/225, train perplexity: 1.025354530350356\tvalidation perplexity: 1.4459897079425827\tstep time: 0.46859920024871826\n",
            "epoch 10 step 169/225, train perplexity: 1.0177270879888602\tvalidation perplexity: 1.4456475965345843\tstep time: 0.514549994468689\n",
            "epoch 10 step 179/225, train perplexity: 1.0362399536709617\tvalidation perplexity: 1.4434835864198292\tstep time: 0.4909718751907349\n",
            "epoch 10 step 189/225, train perplexity: 1.0189169142974697\tvalidation perplexity: 1.44393810490822\tstep time: 0.4699070453643799\n",
            "epoch 10 step 199/225, train perplexity: 1.0311558028911492\tvalidation perplexity: 1.4446556600382523\tstep time: 0.47969927787780764\n",
            "epoch 10 step 209/225, train perplexity: 1.016504834615093\tvalidation perplexity: 1.4435800853057115\tstep time: 0.47686214447021485\n",
            "epoch 10 step 219/225, train perplexity: 1.0278154415859175\tvalidation perplexity: 1.4423096808577478\tstep time: 0.49609310626983644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 55%|█████▌    | 11/20 [30:36<25:02, 166.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 11\tlr: 1.0000000000000002e-07\n",
            "epoch 11 step 9/225, train perplexity: 1.0148605525414705\tvalidation perplexity: 1.4432128068911496\tstep time: 0.8327457666397095\n",
            "epoch 11 step 19/225, train perplexity: 1.0187164798600785\tvalidation perplexity: 1.442879539351318\tstep time: 0.5119464159011841\n",
            "epoch 11 step 29/225, train perplexity: 1.039170180612082\tvalidation perplexity: 1.4420895323329943\tstep time: 0.4965059757232666\n",
            "epoch 11 step 39/225, train perplexity: 1.0342081134388361\tvalidation perplexity: 1.4437956630330295\tstep time: 0.5042219161987305\n",
            "epoch 11 step 49/225, train perplexity: 1.0163152181541673\tvalidation perplexity: 1.4445750767672385\tstep time: 0.4825079679489136\n",
            "epoch 11 step 59/225, train perplexity: 1.019621673988647\tvalidation perplexity: 1.4448923726966039\tstep time: 0.49282238483428953\n",
            "epoch 11 step 69/225, train perplexity: 1.0122593678627212\tvalidation perplexity: 1.4433262860629095\tstep time: 0.5064268112182617\n",
            "epoch 11 step 79/225, train perplexity: 1.0286587987095444\tvalidation perplexity: 1.4413906739869482\tstep time: 0.4907907724380493\n",
            "epoch 11 step 89/225, train perplexity: 1.0174754014192022\tvalidation perplexity: 1.442916867776249\tstep time: 0.49879047870635984\n",
            "epoch 11 step 99/225, train perplexity: 1.0241470551700904\tvalidation perplexity: 1.4440275815473145\tstep time: 0.5058297157287598\n",
            "epoch 11 step 109/225, train perplexity: 1.035244105885772\tvalidation perplexity: 1.4425934565684766\tstep time: 0.4936398506164551\n",
            "epoch 11 step 119/225, train perplexity: 1.0230658459820574\tvalidation perplexity: 1.442101201560153\tstep time: 0.5020319938659668\n",
            "epoch 11 step 129/225, train perplexity: 1.0232380207748892\tvalidation perplexity: 1.4441799249593352\tstep time: 0.4746479272842407\n",
            "epoch 11 step 139/225, train perplexity: 1.012688486569327\tvalidation perplexity: 1.4424273093029538\tstep time: 0.4804887056350708\n",
            "epoch 11 step 149/225, train perplexity: 1.0313609093737819\tvalidation perplexity: 1.443180489286971\tstep time: 0.5133618831634521\n",
            "epoch 11 step 159/225, train perplexity: 1.0242509018678754\tvalidation perplexity: 1.443666926455249\tstep time: 0.4903644323348999\n",
            "epoch 11 step 169/225, train perplexity: 1.021903822168506\tvalidation perplexity: 1.4451892845906578\tstep time: 0.4971081018447876\n",
            "epoch 11 step 179/225, train perplexity: 1.021932884367757\tvalidation perplexity: 1.4441043777202143\tstep time: 0.5060816526412963\n",
            "epoch 11 step 189/225, train perplexity: 1.0273809023067475\tvalidation perplexity: 1.44390981746195\tstep time: 0.5284816741943359\n",
            "epoch 11 step 199/225, train perplexity: 1.014919064515369\tvalidation perplexity: 1.4426637464566583\tstep time: 0.5071880102157593\n",
            "epoch 11 step 209/225, train perplexity: 1.0160116510369344\tvalidation perplexity: 1.44396821861715\tstep time: 0.477506422996521\n",
            "epoch 11 step 219/225, train perplexity: 1.0383781541813804\tvalidation perplexity: 1.4448636541374709\tstep time: 0.48748002052307127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 60%|██████    | 12/20 [33:23<22:14, 166.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 12\tlr: 1.0000000000000002e-07\n",
            "epoch 12 step 9/225, train perplexity: 1.017092022525246\tvalidation perplexity: 1.4431109937717386\tstep time: 0.8692104816436768\n",
            "epoch 12 step 19/225, train perplexity: 1.0132301406623416\tvalidation perplexity: 1.4434090283972476\tstep time: 0.5004313468933106\n",
            "epoch 12 step 29/225, train perplexity: 1.0193968456805347\tvalidation perplexity: 1.443803555621833\tstep time: 0.4852914333343506\n",
            "epoch 12 step 39/225, train perplexity: 1.0214473545047862\tvalidation perplexity: 1.442421057787893\tstep time: 0.49135255813598633\n",
            "epoch 12 step 49/225, train perplexity: 1.0554777502031936\tvalidation perplexity: 1.4425433940978083\tstep time: 0.48547160625457764\n",
            "epoch 12 step 59/225, train perplexity: 1.0113318842113876\tvalidation perplexity: 1.4428385583993377\tstep time: 0.5125798463821412\n",
            "epoch 12 step 69/225, train perplexity: 1.0170192656703345\tvalidation perplexity: 1.4446452491119444\tstep time: 0.4936232566833496\n",
            "epoch 12 step 79/225, train perplexity: 1.015976377916319\tvalidation perplexity: 1.4454431001815047\tstep time: 0.4918660163879395\n",
            "epoch 12 step 89/225, train perplexity: 1.018401840154976\tvalidation perplexity: 1.4446484848361114\tstep time: 0.4979182243347168\n",
            "epoch 12 step 99/225, train perplexity: 1.0367750089282926\tvalidation perplexity: 1.4439477523526403\tstep time: 0.4889737606048584\n",
            "epoch 12 step 109/225, train perplexity: 1.0213282038975895\tvalidation perplexity: 1.4436180307408952\tstep time: 0.5048081636428833\n",
            "epoch 12 step 119/225, train perplexity: 1.0133865853399988\tvalidation perplexity: 1.4423332683847034\tstep time: 0.48726577758789064\n",
            "epoch 12 step 129/225, train perplexity: 1.012370995375982\tvalidation perplexity: 1.443065239185252\tstep time: 0.49088027477264407\n",
            "epoch 12 step 139/225, train perplexity: 1.0063257185448078\tvalidation perplexity: 1.4430994430682953\tstep time: 0.5005215883255005\n",
            "epoch 12 step 149/225, train perplexity: 1.0178811034192534\tvalidation perplexity: 1.4419136574033389\tstep time: 0.5102073907852173\n",
            "epoch 12 step 159/225, train perplexity: 1.0315777102625947\tvalidation perplexity: 1.444385148465879\tstep time: 0.5000817060470581\n",
            "epoch 12 step 169/225, train perplexity: 1.0282463973373719\tvalidation perplexity: 1.4444555298436559\tstep time: 0.5211184740066528\n",
            "epoch 12 step 179/225, train perplexity: 1.0196617534539356\tvalidation perplexity: 1.4459788018537525\tstep time: 0.5144119024276733\n",
            "epoch 12 step 189/225, train perplexity: 1.0213189774349445\tvalidation perplexity: 1.4455464767002848\tstep time: 0.487148642539978\n",
            "epoch 12 step 199/225, train perplexity: 1.017396236492413\tvalidation perplexity: 1.443998935590115\tstep time: 0.49185516834259035\n",
            "epoch 12 step 209/225, train perplexity: 1.0174877695409954\tvalidation perplexity: 1.4440601232918944\tstep time: 0.5080269813537598\n",
            "epoch 12 step 219/225, train perplexity: 1.0122905428691082\tvalidation perplexity: 1.444152289806369\tstep time: 0.4984304428100586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 65%|██████▌   | 13/20 [36:10<19:27, 166.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 13\tlr: 1.0000000000000002e-07\n",
            "epoch 13 step 9/225, train perplexity: 1.0247508430078032\tvalidation perplexity: 1.4433430699158833\tstep time: 0.8298485755920411\n",
            "epoch 13 step 19/225, train perplexity: 1.014392863481127\tvalidation perplexity: 1.4431530499123568\tstep time: 0.49798898696899413\n",
            "epoch 13 step 29/225, train perplexity: 1.0221230272749386\tvalidation perplexity: 1.4447282010412659\tstep time: 0.497501540184021\n",
            "epoch 13 step 39/225, train perplexity: 1.0172308349746055\tvalidation perplexity: 1.4446937153751769\tstep time: 0.5174672603607178\n",
            "epoch 13 step 49/225, train perplexity: 1.0307914629203248\tvalidation perplexity: 1.4446179002474404\tstep time: 0.5028264999389649\n",
            "epoch 13 step 59/225, train perplexity: 1.0264202058036713\tvalidation perplexity: 1.4429835553331243\tstep time: 0.5074275493621826\n",
            "epoch 13 step 69/225, train perplexity: 1.0250963864024658\tvalidation perplexity: 1.4420577467438347\tstep time: 0.476706337928772\n",
            "epoch 13 step 79/225, train perplexity: 1.0246366299413907\tvalidation perplexity: 1.4433994544815454\tstep time: 0.481466007232666\n",
            "epoch 13 step 89/225, train perplexity: 1.0166902672069216\tvalidation perplexity: 1.4437463553604457\tstep time: 0.5304062843322754\n",
            "epoch 13 step 99/225, train perplexity: 1.0167464065836276\tvalidation perplexity: 1.4432860582489273\tstep time: 0.4646124839782715\n",
            "epoch 13 step 109/225, train perplexity: 1.0383403658364236\tvalidation perplexity: 1.4400178682013849\tstep time: 0.4785520792007446\n",
            "epoch 13 step 119/225, train perplexity: 1.0321485965484059\tvalidation perplexity: 1.441968637610416\tstep time: 0.5088804244995118\n",
            "epoch 13 step 129/225, train perplexity: 1.0317776694165708\tvalidation perplexity: 1.4444903555059587\tstep time: 0.49439058303833006\n",
            "epoch 13 step 139/225, train perplexity: 1.017784149087581\tvalidation perplexity: 1.4425002512518823\tstep time: 0.48581562042236326\n",
            "epoch 13 step 149/225, train perplexity: 1.0145843323595298\tvalidation perplexity: 1.4436921133970921\tstep time: 0.4881206274032593\n",
            "epoch 13 step 159/225, train perplexity: 1.0160783286986776\tvalidation perplexity: 1.4454663088441606\tstep time: 0.545782494544983\n",
            "epoch 13 step 169/225, train perplexity: 1.0224599885435754\tvalidation perplexity: 1.4452659198479096\tstep time: 0.4934471845626831\n",
            "epoch 13 step 179/225, train perplexity: 1.021955589548063\tvalidation perplexity: 1.4457484829137806\tstep time: 0.5075036287307739\n",
            "epoch 13 step 189/225, train perplexity: 1.0121341241176252\tvalidation perplexity: 1.4448738191126222\tstep time: 0.48529322147369386\n",
            "epoch 13 step 199/225, train perplexity: 1.0406731219150083\tvalidation perplexity: 1.4453953842420326\tstep time: 0.4806759595870972\n",
            "epoch 13 step 209/225, train perplexity: 1.0259913149589446\tvalidation perplexity: 1.4448701692402515\tstep time: 0.49750146865844724\n",
            "epoch 13 step 219/225, train perplexity: 1.0209800720918065\tvalidation perplexity: 1.4441785123614823\tstep time: 0.512335729598999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 70%|███████   | 14/20 [38:57<16:42, 167.09s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 14\tlr: 1.0000000000000004e-08\n",
            "epoch 14 step 9/225, train perplexity: 1.0210134688057715\tvalidation perplexity: 1.441690065805528\tstep time: 0.85342378616333\n",
            "epoch 14 step 19/225, train perplexity: 1.0225625080397835\tvalidation perplexity: 1.4422964619767158\tstep time: 0.5025027751922607\n",
            "epoch 14 step 29/225, train perplexity: 1.040268709719527\tvalidation perplexity: 1.44275641690194\tstep time: 0.49869799613952637\n",
            "epoch 14 step 39/225, train perplexity: 1.023221824315578\tvalidation perplexity: 1.4437254680558975\tstep time: 0.5232505321502685\n",
            "epoch 14 step 49/225, train perplexity: 1.0218666580963511\tvalidation perplexity: 1.4431266412625718\tstep time: 0.4875967979431152\n",
            "epoch 14 step 59/225, train perplexity: 1.0279010787454712\tvalidation perplexity: 1.4443270324069364\tstep time: 0.524660587310791\n",
            "epoch 14 step 69/225, train perplexity: 1.0330090395598162\tvalidation perplexity: 1.4416775340574655\tstep time: 0.4970376968383789\n",
            "epoch 14 step 79/225, train perplexity: 1.017403413065124\tvalidation perplexity: 1.4414171280701886\tstep time: 0.5069377422332764\n",
            "epoch 14 step 89/225, train perplexity: 1.0196527376685085\tvalidation perplexity: 1.4414905234961397\tstep time: 0.4896190643310547\n",
            "epoch 14 step 99/225, train perplexity: 1.0308011896797231\tvalidation perplexity: 1.4435083845458938\tstep time: 0.48774540424346924\n",
            "epoch 14 step 109/225, train perplexity: 1.0213451313441415\tvalidation perplexity: 1.4434399620823706\tstep time: 0.523170781135559\n",
            "epoch 14 step 119/225, train perplexity: 1.0238476231957698\tvalidation perplexity: 1.4437401149941307\tstep time: 0.499206018447876\n",
            "epoch 14 step 129/225, train perplexity: 1.0210697822357753\tvalidation perplexity: 1.4438899317497031\tstep time: 0.5079986095428467\n",
            "epoch 14 step 139/225, train perplexity: 1.0165494021540933\tvalidation perplexity: 1.4461188104510585\tstep time: 0.5029632806777954\n",
            "epoch 14 step 149/225, train perplexity: 1.0215429510062455\tvalidation perplexity: 1.4463958620400126\tstep time: 0.48329527378082277\n",
            "epoch 14 step 159/225, train perplexity: 1.0233450671772102\tvalidation perplexity: 1.4460855842606728\tstep time: 0.48593873977661134\n",
            "epoch 14 step 169/225, train perplexity: 1.0230911337211697\tvalidation perplexity: 1.4435853033977124\tstep time: 0.49220967292785645\n",
            "epoch 14 step 179/225, train perplexity: 1.0198698793877228\tvalidation perplexity: 1.4428785458590399\tstep time: 0.5473637580871582\n",
            "epoch 14 step 189/225, train perplexity: 1.03464531679105\tvalidation perplexity: 1.4439024565616352\tstep time: 0.49206650257110596\n",
            "epoch 14 step 199/225, train perplexity: 1.0169778314842894\tvalidation perplexity: 1.4448063995102611\tstep time: 0.47873549461364745\n",
            "epoch 14 step 209/225, train perplexity: 1.014877319863489\tvalidation perplexity: 1.444675989405098\tstep time: 0.485688304901123\n",
            "epoch 14 step 219/225, train perplexity: 1.0302375539285755\tvalidation perplexity: 1.4437004492809584\tstep time: 0.47910778522491454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 75%|███████▌  | 15/20 [41:46<13:57, 167.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 15\tlr: 1.0000000000000004e-08\n",
            "epoch 15 step 9/225, train perplexity: 1.021279428243603\tvalidation perplexity: 1.443390480489297\tstep time: 0.8170177936553955\n",
            "epoch 15 step 19/225, train perplexity: 1.023864539000363\tvalidation perplexity: 1.4427393909273731\tstep time: 0.5174781799316406\n",
            "epoch 15 step 29/225, train perplexity: 1.018907356615354\tvalidation perplexity: 1.4424813594082373\tstep time: 0.5267109870910645\n",
            "epoch 15 step 39/225, train perplexity: 1.0216747531095987\tvalidation perplexity: 1.4426080822974865\tstep time: 0.48138539791107177\n",
            "epoch 15 step 49/225, train perplexity: 1.0246594543590704\tvalidation perplexity: 1.442661998675823\tstep time: 0.49373230934143064\n",
            "epoch 15 step 59/225, train perplexity: 1.0178785931828278\tvalidation perplexity: 1.4443678658058727\tstep time: 0.49383656978607177\n",
            "epoch 15 step 69/225, train perplexity: 1.0239577582975357\tvalidation perplexity: 1.442209496789822\tstep time: 0.5208690643310547\n",
            "epoch 15 step 79/225, train perplexity: 1.0112738757190594\tvalidation perplexity: 1.4451485185051596\tstep time: 0.515469241142273\n",
            "epoch 15 step 89/225, train perplexity: 1.0226473989446963\tvalidation perplexity: 1.4432963033662745\tstep time: 0.5094520807266235\n",
            "epoch 15 step 99/225, train perplexity: 1.0232222645783804\tvalidation perplexity: 1.4424051789081371\tstep time: 0.5056846380233765\n",
            "epoch 15 step 109/225, train perplexity: 1.028054695952046\tvalidation perplexity: 1.444626498112753\tstep time: 0.5112426996231079\n",
            "epoch 15 step 119/225, train perplexity: 1.0195098024435436\tvalidation perplexity: 1.4431367050962356\tstep time: 0.48963427543640137\n",
            "epoch 15 step 129/225, train perplexity: 1.0236213539847903\tvalidation perplexity: 1.4445081479630346\tstep time: 0.5113411188125611\n",
            "epoch 15 step 139/225, train perplexity: 1.0206617731911842\tvalidation perplexity: 1.4456281200494288\tstep time: 0.4995476245880127\n",
            "epoch 15 step 149/225, train perplexity: 1.0256957283791988\tvalidation perplexity: 1.4442703916862474\tstep time: 0.5147197723388672\n",
            "epoch 15 step 159/225, train perplexity: 1.0455536793534679\tvalidation perplexity: 1.4417335752569846\tstep time: 0.4948173999786377\n",
            "epoch 15 step 169/225, train perplexity: 1.0170712421697163\tvalidation perplexity: 1.443849776211883\tstep time: 0.49842267036437987\n",
            "epoch 15 step 179/225, train perplexity: 1.0174269158897085\tvalidation perplexity: 1.4448369639401448\tstep time: 0.4923753499984741\n",
            "epoch 15 step 189/225, train perplexity: 1.025495423621052\tvalidation perplexity: 1.4460901074903443\tstep time: 0.46843554973602297\n",
            "epoch 15 step 199/225, train perplexity: 1.0158851170731042\tvalidation perplexity: 1.443487947061493\tstep time: 0.4905754804611206\n",
            "epoch 15 step 209/225, train perplexity: 1.0199306283863743\tvalidation perplexity: 1.4428172266602046\tstep time: 0.4890016794204712\n",
            "epoch 15 step 219/225, train perplexity: 1.0258768910242853\tvalidation perplexity: 1.4421450592452223\tstep time: 0.47300970554351807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 80%|████████  | 16/20 [44:34<11:10, 167.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 16\tlr: 1.0000000000000004e-08\n",
            "epoch 16 step 9/225, train perplexity: 1.022894387969353\tvalidation perplexity: 1.4453122773318778\tstep time: 0.800529932975769\n",
            "epoch 16 step 19/225, train perplexity: 1.0320573344016775\tvalidation perplexity: 1.4467621211079131\tstep time: 0.4951669454574585\n",
            "epoch 16 step 29/225, train perplexity: 1.0175772694925322\tvalidation perplexity: 1.443675800670466\tstep time: 0.5011097908020019\n",
            "epoch 16 step 39/225, train perplexity: 1.0228888836034622\tvalidation perplexity: 1.4437263846304793\tstep time: 0.5064932584762574\n",
            "epoch 16 step 49/225, train perplexity: 1.022267227618659\tvalidation perplexity: 1.4421068580046832\tstep time: 0.4955211400985718\n",
            "epoch 16 step 59/225, train perplexity: 1.018613655333619\tvalidation perplexity: 1.4421573924722957\tstep time: 0.47467689514160155\n",
            "epoch 16 step 69/225, train perplexity: 1.0247688751011275\tvalidation perplexity: 1.442640749320349\tstep time: 0.5123663663864135\n",
            "epoch 16 step 79/225, train perplexity: 1.0269895329146557\tvalidation perplexity: 1.4432754849163658\tstep time: 0.4807800531387329\n",
            "epoch 16 step 89/225, train perplexity: 1.0160303374493895\tvalidation perplexity: 1.443422344256482\tstep time: 0.4815791368484497\n",
            "epoch 16 step 99/225, train perplexity: 1.0343492946403783\tvalidation perplexity: 1.4431096170220223\tstep time: 0.4973610877990723\n",
            "epoch 16 step 109/225, train perplexity: 1.01253956256617\tvalidation perplexity: 1.4433674219797656\tstep time: 0.5125258207321167\n",
            "epoch 16 step 119/225, train perplexity: 1.013979971474752\tvalidation perplexity: 1.4434864806353012\tstep time: 0.5376648664474487\n",
            "epoch 16 step 129/225, train perplexity: 1.0191884366018904\tvalidation perplexity: 1.4449416336252807\tstep time: 0.5002628803253174\n",
            "epoch 16 step 139/225, train perplexity: 1.0118192614912023\tvalidation perplexity: 1.4443917360773988\tstep time: 0.519706392288208\n",
            "epoch 16 step 149/225, train perplexity: 1.0351015146262497\tvalidation perplexity: 1.443438823476554\tstep time: 0.4804872989654541\n",
            "epoch 16 step 159/225, train perplexity: 1.0181921969274343\tvalidation perplexity: 1.4441407084286377\tstep time: 0.49592301845550535\n",
            "epoch 16 step 169/225, train perplexity: 1.019663943312313\tvalidation perplexity: 1.4433110380232264\tstep time: 0.46714177131652834\n",
            "epoch 16 step 179/225, train perplexity: 1.0345761527174777\tvalidation perplexity: 1.4447480895828297\tstep time: 0.5047847270965576\n",
            "epoch 16 step 189/225, train perplexity: 1.0240045656792176\tvalidation perplexity: 1.4435245764948246\tstep time: 0.5145796537399292\n",
            "epoch 16 step 199/225, train perplexity: 1.018107562070562\tvalidation perplexity: 1.4433436826253396\tstep time: 0.48595666885375977\n",
            "epoch 16 step 209/225, train perplexity: 1.0214479880687712\tvalidation perplexity: 1.4434451851409913\tstep time: 0.5064647674560547\n",
            "epoch 16 step 219/225, train perplexity: 1.0227951977036533\tvalidation perplexity: 1.4442216723080117\tstep time: 0.5247247219085693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 85%|████████▌ | 17/20 [47:22<08:23, 167.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 17\tlr: 1.0000000000000005e-09\n",
            "epoch 17 step 9/225, train perplexity: 1.0209542127820697\tvalidation perplexity: 1.4432226579645175\tstep time: 0.8313809871673584\n",
            "epoch 17 step 19/225, train perplexity: 1.0281742693295357\tvalidation perplexity: 1.4434634615327342\tstep time: 0.5207849264144897\n",
            "epoch 17 step 29/225, train perplexity: 1.0210615451804783\tvalidation perplexity: 1.4435685987600646\tstep time: 0.5318201780319214\n",
            "epoch 17 step 39/225, train perplexity: 1.0133996644635002\tvalidation perplexity: 1.4427219629335537\tstep time: 0.5118592023849488\n",
            "epoch 17 step 49/225, train perplexity: 1.020832516612625\tvalidation perplexity: 1.442109838292759\tstep time: 0.5214133977890014\n",
            "epoch 17 step 59/225, train perplexity: 1.0288187150313297\tvalidation perplexity: 1.4426721848094084\tstep time: 0.5095732927322387\n",
            "epoch 17 step 69/225, train perplexity: 1.0162043094153441\tvalidation perplexity: 1.4420902147484436\tstep time: 0.4906904220581055\n",
            "epoch 17 step 79/225, train perplexity: 1.0230590582266157\tvalidation perplexity: 1.4414084162620375\tstep time: 0.5143445014953614\n",
            "epoch 17 step 89/225, train perplexity: 1.023342013563548\tvalidation perplexity: 1.4440294527967061\tstep time: 0.48727452754974365\n",
            "epoch 17 step 99/225, train perplexity: 1.0283316432659133\tvalidation perplexity: 1.447116959304802\tstep time: 0.48337032794952395\n",
            "epoch 17 step 109/225, train perplexity: 1.0515114399332666\tvalidation perplexity: 1.4439737420026308\tstep time: 0.4878957986831665\n",
            "epoch 17 step 119/225, train perplexity: 1.0173899506370687\tvalidation perplexity: 1.4456321996036987\tstep time: 0.5018022537231446\n",
            "epoch 17 step 129/225, train perplexity: 1.0199849518231474\tvalidation perplexity: 1.4451563426223415\tstep time: 0.5036962985992431\n",
            "epoch 17 step 139/225, train perplexity: 1.0226500085640533\tvalidation perplexity: 1.4433435629031883\tstep time: 0.4953247785568237\n",
            "epoch 17 step 149/225, train perplexity: 1.0267201950828277\tvalidation perplexity: 1.4412423705239303\tstep time: 0.49725496768951416\n",
            "epoch 17 step 159/225, train perplexity: 1.0255876624131852\tvalidation perplexity: 1.442504782992241\tstep time: 0.5114148855209351\n",
            "epoch 17 step 169/225, train perplexity: 1.0164248345552305\tvalidation perplexity: 1.4438380684859438\tstep time: 0.46642212867736815\n",
            "epoch 17 step 179/225, train perplexity: 1.0123543609166261\tvalidation perplexity: 1.4446128378288092\tstep time: 0.5544737339019775\n",
            "epoch 17 step 189/225, train perplexity: 1.0224539113655158\tvalidation perplexity: 1.4451357508027074\tstep time: 0.5277157545089721\n",
            "epoch 17 step 199/225, train perplexity: 1.049878717641915\tvalidation perplexity: 1.4461751537549472\tstep time: 0.5037196636199951\n",
            "epoch 17 step 209/225, train perplexity: 1.0117427594280237\tvalidation perplexity: 1.4444239758283617\tstep time: 0.49555091857910155\n",
            "epoch 17 step 219/225, train perplexity: 1.0117407241509082\tvalidation perplexity: 1.4419552332669803\tstep time: 0.46914358139038087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 90%|█████████ | 18/20 [50:10<05:35, 167.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 18\tlr: 1.0000000000000005e-09\n",
            "epoch 18 step 9/225, train perplexity: 1.031434822861358\tvalidation perplexity: 1.4443240741235432\tstep time: 0.8012819290161133\n",
            "epoch 18 step 19/225, train perplexity: 1.0381543646411837\tvalidation perplexity: 1.442935992709189\tstep time: 0.48694581985473634\n",
            "epoch 18 step 29/225, train perplexity: 1.021979890436557\tvalidation perplexity: 1.4439121408649096\tstep time: 0.4910523176193237\n",
            "epoch 18 step 39/225, train perplexity: 1.0253589077855039\tvalidation perplexity: 1.4437050578215118\tstep time: 0.5074796199798584\n",
            "epoch 18 step 49/225, train perplexity: 1.0276945546630167\tvalidation perplexity: 1.4427150313002393\tstep time: 0.4960456371307373\n",
            "epoch 18 step 59/225, train perplexity: 1.013476507015282\tvalidation perplexity: 1.4438688381345646\tstep time: 0.48762671947479247\n",
            "epoch 18 step 69/225, train perplexity: 1.0427874068380614\tvalidation perplexity: 1.4454252062267294\tstep time: 0.512453556060791\n",
            "epoch 18 step 79/225, train perplexity: 1.0126331106958082\tvalidation perplexity: 1.4440119694622837\tstep time: 0.49181270599365234\n",
            "epoch 18 step 89/225, train perplexity: 1.0246176001211205\tvalidation perplexity: 1.4423069092189256\tstep time: 0.4937691926956177\n",
            "epoch 18 step 99/225, train perplexity: 1.0167524517317355\tvalidation perplexity: 1.4418880307412185\tstep time: 0.5092648506164551\n",
            "epoch 18 step 109/225, train perplexity: 1.015181490686348\tvalidation perplexity: 1.442508180458155\tstep time: 0.4954668045043945\n",
            "epoch 18 step 119/225, train perplexity: 1.0215929228273188\tvalidation perplexity: 1.4423290962326372\tstep time: 0.497232460975647\n",
            "epoch 18 step 129/225, train perplexity: 1.0182732009780866\tvalidation perplexity: 1.4446891741650767\tstep time: 0.5135496854782104\n",
            "epoch 18 step 139/225, train perplexity: 1.0165662087399583\tvalidation perplexity: 1.4425397904533814\tstep time: 0.5121655464172363\n",
            "epoch 18 step 149/225, train perplexity: 1.0316200793966255\tvalidation perplexity: 1.4438719085386722\tstep time: 0.501088809967041\n",
            "epoch 18 step 159/225, train perplexity: 1.0175372909143885\tvalidation perplexity: 1.445371323713406\tstep time: 0.5031845808029175\n",
            "epoch 18 step 169/225, train perplexity: 1.0468327113890366\tvalidation perplexity: 1.4453252398761487\tstep time: 0.4862661361694336\n",
            "epoch 18 step 179/225, train perplexity: 1.0223158409860114\tvalidation perplexity: 1.4434857085462964\tstep time: 0.49077227115631106\n",
            "epoch 18 step 189/225, train perplexity: 1.018014799516856\tvalidation perplexity: 1.4446913758677622\tstep time: 0.49381513595581056\n",
            "epoch 18 step 199/225, train perplexity: 1.0126185532929084\tvalidation perplexity: 1.4440585613974704\tstep time: 0.5026265144348144\n",
            "epoch 18 step 209/225, train perplexity: 1.020197573236405\tvalidation perplexity: 1.4443420012068475\tstep time: 0.5185835361480713\n",
            "epoch 18 step 219/225, train perplexity: 1.0214923613564246\tvalidation perplexity: 1.443264318460633\tstep time: 0.5285087108612061\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 95%|█████████▌| 19/20 [52:58<02:47, 167.97s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 19\tlr: 1.0000000000000005e-09\n",
            "epoch 19 step 9/225, train perplexity: 1.0146133564856339\tvalidation perplexity: 1.4429655393374332\tstep time: 0.8406452417373658\n",
            "epoch 19 step 19/225, train perplexity: 1.041543406766426\tvalidation perplexity: 1.4408893867370058\tstep time: 0.49468340873718264\n",
            "epoch 19 step 29/225, train perplexity: 1.0186501451643504\tvalidation perplexity: 1.4439780530008144\tstep time: 0.5145503044128418\n",
            "epoch 19 step 39/225, train perplexity: 1.0172096785268774\tvalidation perplexity: 1.4428926286456523\tstep time: 0.5047097444534302\n",
            "epoch 19 step 49/225, train perplexity: 1.0191598245237325\tvalidation perplexity: 1.444503489269749\tstep time: 0.526555347442627\n",
            "epoch 19 step 59/225, train perplexity: 1.023949785940174\tvalidation perplexity: 1.4439625338727216\tstep time: 0.5360347509384156\n",
            "epoch 19 step 69/225, train perplexity: 1.0217623870186703\tvalidation perplexity: 1.444120985789268\tstep time: 0.5102708816528321\n",
            "epoch 19 step 79/225, train perplexity: 1.0184017794534979\tvalidation perplexity: 1.4425424247955394\tstep time: 0.483526873588562\n",
            "epoch 19 step 89/225, train perplexity: 1.024730673474774\tvalidation perplexity: 1.4439427571201853\tstep time: 0.4851257562637329\n",
            "epoch 19 step 99/225, train perplexity: 1.019496241881929\tvalidation perplexity: 1.4452532845896116\tstep time: 0.48093810081481936\n",
            "epoch 19 step 109/225, train perplexity: 1.0252886609932497\tvalidation perplexity: 1.4428629886863642\tstep time: 0.49381706714630125\n",
            "epoch 19 step 119/225, train perplexity: 1.0324084048779678\tvalidation perplexity: 1.4426604565424903\tstep time: 0.5142370223999023\n",
            "epoch 19 step 129/225, train perplexity: 1.0208483367886863\tvalidation perplexity: 1.4430662991847372\tstep time: 0.492771315574646\n",
            "epoch 19 step 139/225, train perplexity: 1.0249287727784773\tvalidation perplexity: 1.445060972480075\tstep time: 0.4988666534423828\n",
            "epoch 19 step 149/225, train perplexity: 1.0125872295572727\tvalidation perplexity: 1.4433336311539662\tstep time: 0.49754796028137205\n",
            "epoch 19 step 159/225, train perplexity: 1.0195956667663906\tvalidation perplexity: 1.4440089499620399\tstep time: 0.4728292942047119\n",
            "epoch 19 step 169/225, train perplexity: 1.0209209606273326\tvalidation perplexity: 1.4452205557429534\tstep time: 0.4663399219512939\n",
            "epoch 19 step 179/225, train perplexity: 1.0206103450991624\tvalidation perplexity: 1.446085505974151\tstep time: 0.5100742101669311\n",
            "epoch 19 step 189/225, train perplexity: 1.025645660939729\tvalidation perplexity: 1.4441077817857515\tstep time: 0.49975881576538084\n",
            "epoch 19 step 199/225, train perplexity: 1.0229880467509713\tvalidation perplexity: 1.442885632231061\tstep time: 0.4966499090194702\n",
            "epoch 19 step 209/225, train perplexity: 1.0128098183784802\tvalidation perplexity: 1.443052201021665\tstep time: 0.5288550376892089\n",
            "epoch 19 step 219/225, train perplexity: 1.0381209119419603\tvalidation perplexity: 1.4421415906604818\tstep time: 0.5206301212310791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 20/20 [55:47<00:00, 167.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best model at epoch 0 step 119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc9d915c8d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjPKcUNCQpb_",
        "colab_type": "text"
      },
      "source": [
        "#Validation of the Model <a name=\"valid\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmiSD8SGS4l0",
        "colab_type": "text"
      },
      "source": [
        "## Execution Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogFx3Frx79la",
        "colab_type": "code",
        "outputId": "37c96391-a3d8-450a-b504-e398b98e94b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "if 'valid' in option:\n",
        "  #initialize the validation data in DataClass\n",
        "  valid_set = DeepNovoTrainDataset(input_feature_file_valid,\n",
        "                                          input_spectrum_file_valid)\n",
        "  valid_data_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
        "                                                          batch_size=batch_size,\n",
        "                                                          shuffle=False,\n",
        "                                                          num_workers=num_workers,\n",
        "                                                          collate_fn=collate_func)\n",
        "  #Feed validation data to the Model\n",
        "  forward_deepnovo, backward_deepnovo, init_net = build_model(training=False)\n",
        "  forward_deepnovo.eval()\n",
        "  backward_deepnovo.eval()\n",
        "  #compute validation loss with the validation function\n",
        "  validation_loss = validation(forward_deepnovo, backward_deepnovo, init_net, valid_data_loader)\n",
        "  print(f\"validation perplexity: {perplexity(validation_loss)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input spectrum file: Test_files/spectrum_smbp.mgf\n",
            "input feature file: Test_files/features_smbp.csv.valid\n",
            "read cached spectrum locations\n",
            "load pretrained model\n",
            "validation perplexity: 1.3300293110139931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASIQeP26eAeU",
        "colab_type": "text"
      },
      "source": [
        "# Denovo Prediction <a name=\"denovo\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUZXhvJxim7o",
        "colab_type": "text"
      },
      "source": [
        "##Denovo Path <a name=\"denovo_path\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaq2C4-winfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# denovo path files\n",
        "denovo_input_spectrum_file = \"Test_files/spectrum_smbp.mgf\"\n",
        "denovo_input_feature_file = \"Test_files/features_smbp.csv.test\"\n",
        "\n",
        "#Path of the denovo output predictions\n",
        "denovo_output_file = denovo_input_feature_file + \".deepnovo_denovo\"\n",
        "\n",
        "predicted_format = \"deepnovo\"\n",
        "target_file = denovo_input_feature_file\n",
        "predicted_file = denovo_output_file\n",
        "\n",
        "#Name of the files produced after denovo prediction\n",
        "accuracy_file = predicted_file + \".accuracy\"\n",
        "denovo_only_file = predicted_file + \".denovo_only\"\n",
        "scan2fea_file = predicted_file + \".scan2fea\"\n",
        "multifea_file = predicted_file + \".multifea\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94FLPE_IPtqe",
        "colab_type": "text"
      },
      "source": [
        "## Denovo Dataset Data reader <a name=\"denovo_datareader\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5BTcFsXPskK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepNovoDenovoDataset(DeepNovoTrainDataset):\n",
        "    # override _get_feature method in the dataclass DeepNovoTrainDataset\n",
        "    def _get_feature(self, feature: DDAFeature) -> DenovoData:\n",
        "        spectrum_location = self.spectrum_location_dict[feature.scan]\n",
        "        self.input_spectrum_handle.seek(spectrum_location)\n",
        "        # parse header lines\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"BEGIN IONS\" in line, \"Error: wrong input BEGIN IONS\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"TITLE=\" in line, \"Error: wrong input TITLE=\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"PEPMASS=\" in line, \"Error: wrong input PEPMASS=\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"CHARGE=\" in line, \"Error: wrong input CHARGE=\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"SCANS=\" in line, \"Error: wrong input SCANS=\"\n",
        "        line = self.input_spectrum_handle.readline()\n",
        "        assert \"RTINSECONDS=\" in line, \"Error: wrong input RTINSECONDS=\"\n",
        "        mz_list, intensity_list = self._parse_spectrum_ion()\n",
        "        peak_location, peak_intensity, spectrum_representation = process_peaks(mz_list, intensity_list, feature.mass)\n",
        "\n",
        "        return DenovoData(peak_location=peak_location,\n",
        "                          peak_intensity=peak_intensity,\n",
        "                          spectrum_representation=spectrum_representation,\n",
        "                          original_dda_feature=feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2RgG5ZmrNgC",
        "colab_type": "text"
      },
      "source": [
        "## Writer Functions <a name=\"denovo_writer\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8QWe-h6rQ5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@dataclass\n",
        "class BeamSearchedSequence:\n",
        "    sequence: list  # list of aa id\n",
        "    position_score: list\n",
        "    score: float  # average by length score\n",
        "\n",
        "\n",
        "class DenovoWriter(object):\n",
        "    def __init__(self, denovo_output_file):\n",
        "        self.output_handle = open(denovo_output_file, 'w')\n",
        "        header_list = [\"feature_id\",\n",
        "                       \"feature_area\",\n",
        "                       \"predicted_sequence\",\n",
        "                       \"predicted_score\",\n",
        "                       \"predicted_position_score\",\n",
        "                       \"precursor_mz\",\n",
        "                       \"precursor_charge\",\n",
        "                       \"protein_access_id\",\n",
        "                       \"scan_list_middle\",\n",
        "                       \"scan_list_original\",\n",
        "                       \"predicted_score_max\"]\n",
        "        header_row = \"\\t\".join(header_list)\n",
        "        print(header_row, file=self.output_handle, end='\\n')\n",
        "\n",
        "    def close(self):\n",
        "        self.output_handle.close()\n",
        "\n",
        "    def write(self, dda_original_feature: DDAFeature, searched_sequence: BeamSearchedSequence):\n",
        "        \"\"\"\n",
        "        Write the rows of prediction, outputed from the BeamSearch\n",
        "        :param dda_original_feature:\n",
        "        :param searched_sequence:\n",
        "        :return: -> Print predicted row\n",
        "        \"\"\"\n",
        "        feature_id = dda_original_feature.feature_id\n",
        "        feature_area = dda_original_feature.feature_area\n",
        "        precursor_mz = str(dda_original_feature.mz)\n",
        "        precursor_charge = str(dda_original_feature.z)\n",
        "        scan_list_middle = dda_original_feature.scan\n",
        "        scan_list_original = dda_original_feature.scan\n",
        "        if searched_sequence.sequence:\n",
        "            predicted_sequence = ','.join([vocab_reverse[aa_id] for\n",
        "                                           aa_id in searched_sequence.sequence])\n",
        "            predicted_score = \"{:.2f}\".format(searched_sequence.score)\n",
        "            predicted_score_max = predicted_score\n",
        "            predicted_position_score = ','.join(['{0:.2f}'.format(x) for x in searched_sequence.position_score])\n",
        "            protein_access_id = 'DENOVO'\n",
        "        else:\n",
        "            predicted_sequence = \"\"\n",
        "            predicted_score = \"\"\n",
        "            predicted_score_max = \"\"\n",
        "            predicted_position_score = \"\"\n",
        "            protein_access_id = \"\"\n",
        "        predicted_row = \"\\t\".join([feature_id,\n",
        "                                   feature_area,\n",
        "                                   predicted_sequence,\n",
        "                                   predicted_score,\n",
        "                                   predicted_position_score,\n",
        "                                   precursor_mz,\n",
        "                                   precursor_charge,\n",
        "                                   protein_access_id,\n",
        "                                   scan_list_middle,\n",
        "                                   scan_list_original,\n",
        "                                   predicted_score_max])\n",
        "        print(predicted_row, file=self.output_handle, end=\"\\n\")\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF1-s2RvpdRj",
        "colab_type": "text"
      },
      "source": [
        "## KnapSack Implementation <a name=\"knapsack\"></a>\n",
        "If the file knapsack.py is not present in the root path of the program, it will build a new one. (it takes time to build, ~40min with Colab).\n",
        "But you can upload it from the data included as it is a constant. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGUAYJfbphWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Direction(Enum):\n",
        "    forward = 1\n",
        "    backward = 2\n",
        "\n",
        "@dataclass\n",
        "class BeamSearchStartPoint:\n",
        "    prefix_mass: float\n",
        "    suffix_mass: float\n",
        "    mass_tolerance: float\n",
        "    direction: Direction\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DenovoResult:\n",
        "    dda_feature: DDAFeature\n",
        "    best_beam_search_sequence: BeamSearchedSequence\n",
        "\n",
        "\n",
        "class KnapsackSearcher(object):\n",
        "    \"\"\"\n",
        "    Implementation of the Knapsack Algorithm.\n",
        "    If Knapsack not present in the source file, build a new one based on the parameters set\n",
        "    in global variables (vocabulary and masses)\n",
        "    \"\"\"\n",
        "    def __init__(self, MZ_MAX, knapsack_file):\n",
        "        self.knapsack_file = knapsack_file\n",
        "        self.MZ_MAX = MZ_MAX\n",
        "        self.knapsack_aa_resolution = KNAPSACK_AA_RESOLUTION\n",
        "        if os.path.isfile(knapsack_file):\n",
        "            print(\"KnapsackSearcher.__init__(): load knapsack matrix\")\n",
        "            self.knapsack_matrix = np.load(knapsack_file)\n",
        "        else:\n",
        "            print(\"KnapsackSearcher.__init__(): build knapsack matrix from scratch\")\n",
        "            self.knapsack_matrix = self._build_knapsack()\n",
        "\n",
        "    def _build_knapsack(self):\n",
        "        max_mass = self.MZ_MAX - mass_N_terminus - mass_C_terminus\n",
        "        max_mass_round = int(round(max_mass * self.knapsack_aa_resolution))\n",
        "        max_mass_upperbound = max_mass_round + self.knapsack_aa_resolution\n",
        "        knapsack_matrix = np.zeros(shape=(vocab_size, max_mass_upperbound), dtype=bool)\n",
        "        for aa_id in tqdm(range(3, vocab_size)):\n",
        "            mass_aa = int(round(mass_ID[aa_id] * self.knapsack_aa_resolution))\n",
        "\n",
        "            for col in range(max_mass_upperbound):\n",
        "                current_mass = col + 1\n",
        "                if current_mass < mass_aa:\n",
        "                    knapsack_matrix[aa_id, col] = False\n",
        "                elif current_mass == mass_aa:\n",
        "                    knapsack_matrix[aa_id, col] = True\n",
        "                elif current_mass > mass_aa:\n",
        "                    sub_mass = current_mass - mass_aa\n",
        "                    sub_col = sub_mass - 1\n",
        "                    if np.sum(knapsack_matrix[:, sub_col]) > 0:\n",
        "                        knapsack_matrix[aa_id, col] = True\n",
        "                        knapsack_matrix[:, col] = np.logical_or(knapsack_matrix[:, col], knapsack_matrix[:, sub_col])\n",
        "                    else:\n",
        "                        knapsack_matrix[aa_id, col] = False\n",
        "        np.save(self.knapsack_file, knapsack_matrix)\n",
        "        return knapsack_matrix\n",
        "\n",
        "    def search_knapsack(self, mass, knapsack_tolerance):\n",
        "        mass_round = int(round(mass * self.knapsack_aa_resolution))\n",
        "        mass_upperbound = mass_round + knapsack_tolerance\n",
        "        mass_lowerbound = mass_round - knapsack_tolerance\n",
        "        if mass_upperbound < mass_AA_min_round:\n",
        "            return []\n",
        "        mass_lowerbound_col = mass_lowerbound - 1\n",
        "        mass_upperbound_col = mass_upperbound - 1\n",
        "        candidate_aa_id = np.flatnonzero(np.any(self.knapsack_matrix[:, mass_lowerbound_col:(mass_upperbound_col + 1)],\n",
        "                                                axis=1))\n",
        "        return candidate_aa_id.tolist()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deDumEalsrqv",
        "colab_type": "text"
      },
      "source": [
        "## ION CNN Denovo <a name=\"ioncnn\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvLoZhBKsuwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@dataclass\n",
        "class SearchPath:\n",
        "    aa_id_list: list\n",
        "    aa_seq_mass: float\n",
        "    score_list: list\n",
        "    score_sum: float\n",
        "    lstm_state: tuple  # state tuple store in search path is of shape [num_lstm_layers, num_units]\n",
        "    direction: Direction\n",
        "\n",
        "@dataclass\n",
        "class SearchEntry:\n",
        "    feature_index: int\n",
        "    current_path_list: list  # list of search paths\n",
        "    spectrum_state: tuple  # tuple of (peak_location, peak_intensity)\n",
        "\n",
        "\n",
        "class IonCNNDenovo(object):\n",
        "    def __init__(self, MZ_MAX, knapsack_file, beam_size):\n",
        "        self.MZ_MAX = MZ_MAX\n",
        "        self.beam_size = beam_size\n",
        "        self.knapsack_searcher = KnapsackSearcher(MZ_MAX, knapsack_file)\n",
        "\n",
        "    def _beam_search(self, model_wrapper: InferenceModelWrapper,\n",
        "                     feature_dp_batch: list, start_point_batch: list) -> list:\n",
        "        \"\"\"\n",
        "\n",
        "        :param model_wrapper:\n",
        "        :param feature_dp_batch: list of DenovoData\n",
        "        :param start_point_batch:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        num_features = len(feature_dp_batch)\n",
        "        top_path_batch = [[] for _ in range(num_features)]\n",
        "\n",
        "        direction_cint_map = {Direction.forward: 0, Direction.backward: 1}\n",
        "\n",
        "        direction = start_point_batch[0].direction\n",
        "        if direction == Direction.forward:\n",
        "            get_start_mass = lambda x: x.prefix_mass\n",
        "            first_label = GO_ID\n",
        "            last_label = EOS_ID\n",
        "        elif direction == Direction.backward:\n",
        "            get_start_mass = lambda x: x.suffix_mass\n",
        "            first_label = EOS_ID\n",
        "            last_label = GO_ID\n",
        "        else:\n",
        "            raise ValueError('direction neither forward nor backward')\n",
        "\n",
        "        # step 1: extract original spectrum\n",
        "        batch_peak_location = np.array([x.peak_location for x in feature_dp_batch])\n",
        "        batch_peak_intensity = np.array([x.peak_intensity for x in feature_dp_batch])\n",
        "        batch_spectrum_representation = np.array([x.spectrum_representation for x in feature_dp_batch])\n",
        "\n",
        "        batch_peak_location = torch.from_numpy(batch_peak_location).to(device)\n",
        "        batch_peak_intensity = torch.from_numpy(batch_peak_intensity).to(device)\n",
        "        batch_spectrum_representation = torch.from_numpy(batch_spectrum_representation).to(device)\n",
        "\n",
        "        initial_hidden_state_tuple = model_wrapper.initial_hidden_state(batch_spectrum_representation) if \\\n",
        "            use_lstm else None\n",
        "\n",
        "        # initialize activate search list\n",
        "        active_search_list = []\n",
        "        for feature_index in range(num_features):\n",
        "            # all feature in the same batch should be from same direction\n",
        "            assert direction == start_point_batch[feature_index].direction\n",
        "\n",
        "            spectrum_state = (batch_peak_location[feature_index], batch_peak_intensity[feature_index])\n",
        "\n",
        "            if use_lstm:\n",
        "                lstm_state_temp = (initial_hidden_state_tuple[0][:, feature_index, :],\n",
        "                                   initial_hidden_state_tuple[1][:, feature_index, :]\n",
        "                                   )\n",
        "            else:\n",
        "                lstm_state_temp = None\n",
        "\n",
        "            path = SearchPath(\n",
        "                aa_id_list=[first_label],\n",
        "                aa_seq_mass=get_start_mass(start_point_batch[feature_index]),\n",
        "                score_list=[0.0],\n",
        "                score_sum=0.0,\n",
        "                lstm_state=lstm_state_temp,\n",
        "                direction=direction,\n",
        "            )\n",
        "            search_entry = SearchEntry(\n",
        "                feature_index=feature_index,\n",
        "                current_path_list=[path],\n",
        "                spectrum_state=spectrum_state,\n",
        "            )\n",
        "            active_search_list.append(search_entry)\n",
        "\n",
        "        # repeat STEP 2, 3, 4 until the active_search_list is empty.\n",
        "        while True:\n",
        "            # STEP 2: gather data from active search entries and group into blocks.\n",
        "\n",
        "            # model input\n",
        "            block_aa_id_input = []\n",
        "            block_ion_location = []\n",
        "            block_peak_location = []\n",
        "            block_peak_intensity = []\n",
        "            block_lstm_h = []\n",
        "            block_lstm_c = []\n",
        "            # data stored in path\n",
        "            block_aa_id_list = []\n",
        "            block_aa_seq_mass = []\n",
        "            block_score_list = []\n",
        "            block_score_sum = []\n",
        "            block_knapsack_candidates = []\n",
        "\n",
        "            # store the number of paths of each search entry in the big blocks\n",
        "            #     to retrieve the info of each search entry later in STEP 4.\n",
        "            search_entry_size = [0] * len(active_search_list)\n",
        "\n",
        "            for entry_index, search_entry in enumerate(active_search_list):\n",
        "                feature_index = search_entry.feature_index\n",
        "                current_path_list = search_entry.current_path_list\n",
        "                precursor_mass = feature_dp_batch[feature_index].original_dda_feature.mass\n",
        "                peak_mass_tolerance = start_point_batch[feature_index].mass_tolerance\n",
        "\n",
        "                for path in current_path_list:\n",
        "                    aa_id_list = path.aa_id_list\n",
        "                    aa_id = aa_id_list[-1]\n",
        "                    score_sum = path.score_sum\n",
        "                    aa_seq_mass = path.aa_seq_mass\n",
        "                    score_list = path.score_list\n",
        "                    original_spectrum_tuple = search_entry.spectrum_state\n",
        "                    lstm_state_tuple = path.lstm_state\n",
        "\n",
        "                    if aa_id == last_label:\n",
        "                        if abs(aa_seq_mass - precursor_mass) <= peak_mass_tolerance:\n",
        "                            seq = aa_id_list[1:-1]\n",
        "                            trunc_score_list = score_list[1:-1]\n",
        "                            if direction == Direction.backward:\n",
        "                                seq = seq[::-1]\n",
        "                                trunc_score_list = trunc_score_list[::-1]\n",
        "\n",
        "                            top_path_batch[feature_index].append(\n",
        "                                BeamSearchedSequence(sequence=seq,\n",
        "                                                     position_score=trunc_score_list,\n",
        "                                                     score=path.score_sum / len(seq))\n",
        "                            )\n",
        "                        continue\n",
        "\n",
        "                    ion_location = get_ion_index(precursor_mass, aa_seq_mass, direction_cint_map[direction])  # [26,8]\n",
        "\n",
        "                    residual_mass = precursor_mass - aa_seq_mass - mass_ID[last_label]\n",
        "                    knapsack_tolerance = int(round(peak_mass_tolerance * KNAPSACK_AA_RESOLUTION))\n",
        "                    knapsack_candidates = self.knapsack_searcher.search_knapsack(residual_mass, knapsack_tolerance)\n",
        "\n",
        "                    if not knapsack_candidates:\n",
        "                        # if not possible aa, force it to stop.\n",
        "                        knapsack_candidates.append(last_label)\n",
        "\n",
        "                    block_ion_location.append(ion_location)\n",
        "                    block_aa_id_input.append(aa_id)\n",
        "                    # get hidden state block\n",
        "                    block_peak_location.append(original_spectrum_tuple[0])\n",
        "                    block_peak_intensity.append(original_spectrum_tuple[1])\n",
        "                    if use_lstm:\n",
        "                        block_lstm_h.append(lstm_state_tuple[0])\n",
        "                        block_lstm_c.append(lstm_state_tuple[1])\n",
        "\n",
        "                    block_aa_id_list.append(aa_id_list)\n",
        "                    block_aa_seq_mass.append(aa_seq_mass)\n",
        "                    block_score_list.append(score_list)\n",
        "                    block_score_sum.append(score_sum)\n",
        "                    block_knapsack_candidates.append(knapsack_candidates)\n",
        "                    # record the size of each search entry in the blocks\n",
        "                    search_entry_size[entry_index] += 1\n",
        "\n",
        "            # step 3 run model on data blocks to predict next AA.\n",
        "            #     output is stored in current_log_prob\n",
        "            # assert block_aa_id_list, 'IonCNNDenovo._beam_search(): aa_id_list is empty.'\n",
        "            if not block_ion_location:\n",
        "                # all search entry finished in the previous step\n",
        "                break\n",
        "\n",
        "            block_ion_location = torch.from_numpy(np.array(block_ion_location)).to(device)  # [batch, 26, 8, 10]\n",
        "            block_ion_location = torch.unsqueeze(block_ion_location, dim=1)  # [batch, 1, 26, 8]\n",
        "            block_peak_location = torch.stack(block_peak_location, dim=0).contiguous()\n",
        "            block_peak_intensity = torch.stack(block_peak_intensity, dim=0).contiguous()\n",
        "            if use_lstm:\n",
        "                block_lstm_h = torch.stack(block_lstm_h, dim=1).contiguous()\n",
        "                block_lstm_c = torch.stack(block_lstm_c, dim=1).contiguous()\n",
        "                block_state_tuple = (block_lstm_h, block_lstm_c)\n",
        "                block_aa_id_input = torch.from_numpy(np.array(block_aa_id_input, dtype=np.int64)).unsqueeze(1).to(\n",
        "                    device)\n",
        "            else:\n",
        "                block_state_tuple = None\n",
        "                block_aa_id_input = None\n",
        "\n",
        "            current_log_prob, new_state_tuple = model_wrapper.step(block_ion_location,\n",
        "                                                                   block_peak_location,\n",
        "                                                                   block_peak_intensity,\n",
        "                                                                   block_aa_id_input,\n",
        "                                                                   block_state_tuple,\n",
        "                                                                   direction)\n",
        "            # transfer log_prob back to cpu\n",
        "            current_log_prob = current_log_prob.cpu().numpy()\n",
        "\n",
        "            # STEP 4: retrieve data from blocks to update the active_search_list\n",
        "            #     with knapsack dynamic programming and beam search.\n",
        "            block_index = 0\n",
        "            for entry_index, search_entry in enumerate(active_search_list):\n",
        "                new_path_list = []\n",
        "                direction = search_entry.current_path_list[0].direction\n",
        "                for index in range(block_index, block_index + search_entry_size[entry_index]):\n",
        "                    for aa_id in block_knapsack_candidates[index]:\n",
        "                        if aa_id > 2:\n",
        "                            # do not add score of GO, EOS, PAD\n",
        "                            new_score_list = block_score_list[index] + [current_log_prob[index][aa_id]]\n",
        "                            new_score_sum = block_score_sum[index] + current_log_prob[index][aa_id]\n",
        "                        else:\n",
        "                            new_score_list = block_score_list[index] + [0.0]\n",
        "                            new_score_sum = block_score_sum[index] + 0.0\n",
        "\n",
        "                        if use_lstm:\n",
        "                            new_path_state_tuple = (new_state_tuple[0][:, index, :], new_state_tuple[1][:, index, :])\n",
        "                        else:\n",
        "                            new_path_state_tuple = None\n",
        "\n",
        "                        new_path = SearchPath(\n",
        "                            aa_id_list=block_aa_id_list[index] + [aa_id],\n",
        "                            aa_seq_mass=block_aa_seq_mass[index] + mass_ID[aa_id],\n",
        "                            score_list=new_score_list,\n",
        "                            score_sum=new_score_sum,\n",
        "                            lstm_state=new_path_state_tuple,\n",
        "                            direction=direction\n",
        "                        )\n",
        "                        new_path_list.append(new_path)\n",
        "                if len(new_path_list) > self.beam_size:\n",
        "                    new_path_score = np.array([x.score_sum for x in new_path_list])\n",
        "                    top_k_index = np.argpartition(-new_path_score, self.beam_size)[:self.beam_size]\n",
        "                    search_entry.current_path_list = [new_path_list[ii] for ii in top_k_index]\n",
        "                else:\n",
        "                    search_entry.current_path_list = new_path_list\n",
        "\n",
        "                block_index += search_entry_size[entry_index]\n",
        "\n",
        "            active_search_list = [x for x in active_search_list if x.current_path_list]\n",
        "\n",
        "            if not active_search_list:\n",
        "                break\n",
        "        return top_path_batch\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_start_point(feature_dp_batch: list) -> tuple:\n",
        "        mass_GO = mass_ID[GO_ID]\n",
        "        forward_start_point_lists = [BeamSearchStartPoint(prefix_mass=mass_GO,\n",
        "                                                          suffix_mass=feature_dp.original_dda_feature.mass - mass_GO,\n",
        "                                                          mass_tolerance=PRECURSOR_MASS_PRECISION_TOLERANCE,\n",
        "                                                          direction=Direction.forward)\n",
        "                                     for feature_dp in feature_dp_batch]\n",
        "\n",
        "        mass_EOS = mass_ID[EOS_ID]\n",
        "        backward_start_point_lists = [BeamSearchStartPoint(prefix_mass=feature_dp.original_dda_feature.mass - mass_EOS,\n",
        "                                                           suffix_mass=mass_EOS,\n",
        "                                                           mass_tolerance=PRECURSOR_MASS_PRECISION_TOLERANCE,\n",
        "                                                           direction=Direction.backward)\n",
        "                                      for feature_dp in feature_dp_batch]\n",
        "        return forward_start_point_lists, backward_start_point_lists\n",
        "\n",
        "    @staticmethod\n",
        "    def _select_path(feature_dp_batch: list, top_candidate_batch: list) -> list:\n",
        "        \"\"\"\n",
        "        for each feature, select the best denovo sequence given by DeepNovo model\n",
        "        :param feature_dp_batch: list of DenovoData\n",
        "        :param top_candidate_batch: defined in _search_denovo_batch\n",
        "        :return:\n",
        "        list of DenovoResult\n",
        "        \"\"\"\n",
        "        feature_batch_size = len(feature_dp_batch)\n",
        "\n",
        "        refine_batch = [[] for x in range(feature_batch_size)]\n",
        "        for feature_index in range(feature_batch_size):\n",
        "            precursor_mass = feature_dp_batch[feature_index].original_dda_feature.mass\n",
        "            candidate_list = top_candidate_batch[feature_index]\n",
        "            for beam_search_sequence in candidate_list:\n",
        "                sequence = beam_search_sequence.sequence\n",
        "                sequence_mass = sum(mass_ID[x] for x in sequence)\n",
        "                sequence_mass += mass_ID[GO_ID] + mass_ID[EOS_ID]\n",
        "                if abs(sequence_mass - precursor_mass) <= PRECURSOR_MASS_PRECISION_TOLERANCE:\n",
        "                    refine_batch[feature_index].append(beam_search_sequence)\n",
        "        predicted_batch = []\n",
        "        for feature_index in range(feature_batch_size):\n",
        "            candidate_list = refine_batch[feature_index]\n",
        "            if not candidate_list:\n",
        "                best_beam_search_sequence = BeamSearchedSequence(\n",
        "                    sequence=[],\n",
        "                    position_score=[],\n",
        "                    score=-float('inf')\n",
        "                )\n",
        "            else:\n",
        "                # sort candidate sequence by average position score\n",
        "                best_beam_search_sequence = max(candidate_list, key=lambda x: x.score)\n",
        "\n",
        "            denovo_result = DenovoResult(\n",
        "                dda_feature=feature_dp_batch[feature_index].original_dda_feature,\n",
        "                best_beam_search_sequence=best_beam_search_sequence\n",
        "            )\n",
        "            predicted_batch.append(denovo_result)\n",
        "        return predicted_batch\n",
        "\n",
        "    def _search_denovo_batch(self, feature_dp_batch: list, model_wrapper: InferenceModelWrapper) -> list:\n",
        "        start_time = time.time()\n",
        "        feature_batch_size = len(feature_dp_batch)\n",
        "        start_points_tuple = self._get_start_point(feature_dp_batch)\n",
        "        top_candidate_batch = [[] for x in range(feature_batch_size)]\n",
        "\n",
        "        for start_points in start_points_tuple:\n",
        "            beam_search_result_batch = self._beam_search(model_wrapper, feature_dp_batch, start_points)\n",
        "            for feature_index in range(feature_batch_size):\n",
        "                top_candidate_batch[feature_index].extend(beam_search_result_batch[feature_index])\n",
        "        predicted_batch = self._select_path(feature_dp_batch, top_candidate_batch)\n",
        "        test_time = time.time() - start_time\n",
        "        print(\"beam_search(): batch time {}s\".format(test_time))\n",
        "        return predicted_batch\n",
        "\n",
        "    def search_denovo(self, model_wrapper: InferenceModelWrapper,\n",
        "                      beam_search_reader: DeepNovoDenovoDataset, denovo_writer: DenovoWriter):\n",
        "        print(\"start beam search denovo\")\n",
        "        predicted_denovo_list = []\n",
        "\n",
        "        test_set_iter = chunks(list(range(len(beam_search_reader))), n=batch_size)\n",
        "        total_batch_num = int(len(beam_search_reader) / batch_size)\n",
        "        for index, feature_batch_index in enumerate(test_set_iter):\n",
        "            feature_dp_batch = [beam_search_reader[i] for i in feature_batch_index]\n",
        "            print(\"Read {}th/{} batches\".format(index, total_batch_num))\n",
        "            predicted_batch = self._search_denovo_batch(feature_dp_batch, model_wrapper)\n",
        "            predicted_denovo_list += predicted_batch\n",
        "            for denovo_result in predicted_batch:\n",
        "                denovo_writer.write(denovo_result.dda_feature, denovo_result.best_beam_search_sequence)\n",
        "\n",
        "        return predicted_denovo_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxJrg_IhQLP",
        "colab_type": "text"
      },
      "source": [
        "##Chunk function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87AFhMXzFqCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunks(l, n: int):\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i + n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEQp-lOkO8cv",
        "colab_type": "text"
      },
      "source": [
        "##Launch DeepNovo denovo prediction <a name=\"denovo_launch\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny9pj9cTPEWo",
        "colab_type": "code",
        "outputId": "8fc7df12-2194-4c29-d13c-b2fe3e0597cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "beam_size = beam_size_param #by default 5\n",
        "if 'denovo' in option:\n",
        "  data_reader = DeepNovoDenovoDataset(feature_filename=denovo_input_feature_file,\n",
        "                                              spectrum_filename=denovo_input_spectrum_file)\n",
        "  print('data_reader done \\n')\n",
        "  print('IonCNNDenovo starting \\n')\n",
        "  denovo_worker = IonCNNDenovo(MZ_MAX,\n",
        "                                      knapsack_file,\n",
        "                                      beam_size=beam_size)\n",
        "  print('Building Model ... \\n')\n",
        "  forward_deepnovo, backward_deepnovo, init_net = build_model(training=False)\n",
        "  print('Wrapper \\n')\n",
        "  model_wrapper = InferenceModelWrapper(forward_deepnovo, backward_deepnovo, init_net)\n",
        "  print('Writing ... \\n')\n",
        "  writer = DenovoWriter(denovo_output_file)\n",
        "\n",
        "  denovo_worker.search_denovo(model_wrapper, data_reader, writer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input spectrum file: Test_files/spectrum_smbp.mgf\n",
            "input feature file: Test_files/features_smbp.csv.test\n",
            "read cached spectrum locations\n",
            "data_reader done \n",
            "\n",
            "IonCNNDenovo starting \n",
            "\n",
            "KnapsackSearcher.__init__(): load knapsack matrix\n",
            "Building Model ... \n",
            "\n",
            "load pretrained model\n",
            "Wrapper \n",
            "\n",
            "Writing ... \n",
            "\n",
            "start beam search denovo\n",
            "Read 0th/46 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam_search(): batch time 0.43567872047424316s\n",
            "Read 1th/46 batches\n",
            "beam_search(): batch time 0.41751766204833984s\n",
            "Read 2th/46 batches\n",
            "beam_search(): batch time 0.4966866970062256s\n",
            "Read 3th/46 batches\n",
            "beam_search(): batch time 0.4173126220703125s\n",
            "Read 4th/46 batches\n",
            "beam_search(): batch time 0.42495107650756836s\n",
            "Read 5th/46 batches\n",
            "beam_search(): batch time 0.47627758979797363s\n",
            "Read 6th/46 batches\n",
            "beam_search(): batch time 0.57391357421875s\n",
            "Read 7th/46 batches\n",
            "beam_search(): batch time 0.44745898246765137s\n",
            "Read 8th/46 batches\n",
            "beam_search(): batch time 0.49332141876220703s\n",
            "Read 9th/46 batches\n",
            "beam_search(): batch time 0.36672544479370117s\n",
            "Read 10th/46 batches\n",
            "beam_search(): batch time 0.5602383613586426s\n",
            "Read 11th/46 batches\n",
            "beam_search(): batch time 0.735649824142456s\n",
            "Read 12th/46 batches\n",
            "beam_search(): batch time 0.5985617637634277s\n",
            "Read 13th/46 batches\n",
            "beam_search(): batch time 0.4346141815185547s\n",
            "Read 14th/46 batches\n",
            "beam_search(): batch time 0.36309289932250977s\n",
            "Read 15th/46 batches\n",
            "beam_search(): batch time 0.5446023941040039s\n",
            "Read 16th/46 batches\n",
            "beam_search(): batch time 0.41948437690734863s\n",
            "Read 17th/46 batches\n",
            "beam_search(): batch time 0.39890336990356445s\n",
            "Read 18th/46 batches\n",
            "beam_search(): batch time 0.34442734718322754s\n",
            "Read 19th/46 batches\n",
            "beam_search(): batch time 0.4432065486907959s\n",
            "Read 20th/46 batches\n",
            "beam_search(): batch time 0.3528101444244385s\n",
            "Read 21th/46 batches\n",
            "beam_search(): batch time 0.507185697555542s\n",
            "Read 22th/46 batches\n",
            "beam_search(): batch time 0.2895650863647461s\n",
            "Read 23th/46 batches\n",
            "beam_search(): batch time 0.5892248153686523s\n",
            "Read 24th/46 batches\n",
            "beam_search(): batch time 0.7395987510681152s\n",
            "Read 25th/46 batches\n",
            "beam_search(): batch time 0.6795997619628906s\n",
            "Read 26th/46 batches\n",
            "beam_search(): batch time 0.44657230377197266s\n",
            "Read 27th/46 batches\n",
            "beam_search(): batch time 0.44574499130249023s\n",
            "Read 28th/46 batches\n",
            "beam_search(): batch time 0.4043591022491455s\n",
            "Read 29th/46 batches\n",
            "beam_search(): batch time 0.5680508613586426s\n",
            "Read 30th/46 batches\n",
            "beam_search(): batch time 0.587317705154419s\n",
            "Read 31th/46 batches\n",
            "beam_search(): batch time 0.4008784294128418s\n",
            "Read 32th/46 batches\n",
            "beam_search(): batch time 0.4557678699493408s\n",
            "Read 33th/46 batches\n",
            "beam_search(): batch time 0.4694523811340332s\n",
            "Read 34th/46 batches\n",
            "beam_search(): batch time 0.5704464912414551s\n",
            "Read 35th/46 batches\n",
            "beam_search(): batch time 0.6023459434509277s\n",
            "Read 36th/46 batches\n",
            "beam_search(): batch time 0.7040290832519531s\n",
            "Read 37th/46 batches\n",
            "beam_search(): batch time 0.705817461013794s\n",
            "Read 38th/46 batches\n",
            "beam_search(): batch time 0.67112135887146s\n",
            "Read 39th/46 batches\n",
            "beam_search(): batch time 0.5970721244812012s\n",
            "Read 40th/46 batches\n",
            "beam_search(): batch time 0.6291818618774414s\n",
            "Read 41th/46 batches\n",
            "beam_search(): batch time 0.5599796772003174s\n",
            "Read 42th/46 batches\n",
            "beam_search(): batch time 0.4958066940307617s\n",
            "Read 43th/46 batches\n",
            "beam_search(): batch time 0.3885467052459717s\n",
            "Read 44th/46 batches\n",
            "beam_search(): batch time 0.39376068115234375s\n",
            "Read 45th/46 batches\n",
            "beam_search(): batch time 0.6628987789154053s\n",
            "Read 46th/46 batches\n",
            "beam_search(): batch time 0.35404348373413086s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGC7plCMk_p2",
        "colab_type": "text"
      },
      "source": [
        "# Testing Model <a name=\"test\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoasZnLplNqZ",
        "colab_type": "text"
      },
      "source": [
        "## Testing File Path <a name=\"test_path\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEe6yfwyZ6-N",
        "colab_type": "text"
      },
      "source": [
        "Select the path of the testing dataset (usualy part of the initial dataset randomely split in training, valid and testing parts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH0wk0jhlR_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_spectrum_file_test = \"Test_files/spectrum_smbp.mgf\"\n",
        "input_feature_file_test = \"Test_files/feature_smbp.csv.test\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OkvEvQSvV3H",
        "colab_type": "text"
      },
      "source": [
        "## Worker Test function <a name=\"test_worker\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8YznjlbvQuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WorkerTest(object):\n",
        "  \"\"\"\n",
        "     The WorkerTest is a function that will compare the sequence predicted by the\n",
        "     denovo function with the original sequence (if available) and display the accuracy\n",
        "     of the model trained, on these 'denovo' testing sequences\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self):\n",
        "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
        "    print(\"WorkerTest.__init__()\")\n",
        "\n",
        "    #get all the variables needed\n",
        "    self.MZ_MAX = MZ_MAX\n",
        "\n",
        "    self.target_file = target_file #denovo input\n",
        "    self.predicted_file = predicted_file #denovo output\n",
        "    self.predicted_format = predicted_format # tag \"deepnovo\"\n",
        "    \n",
        "    #get the path for the output of the function\n",
        "    self.accuracy_file = accuracy_file \n",
        "    self.denovo_only_file = denovo_only_file\n",
        "    self.scan2fea_file = scan2fea_file\n",
        "    self.multifea_file = multifea_file\n",
        "\n",
        "    print(\"target_file = {0:s}\".format(self.target_file))\n",
        "    print(\"predicted_file = {0:s}\".format(self.predicted_file))\n",
        "    print(\"predicted_format = {0:s}\".format(self.predicted_format))\n",
        "    print(\"accuracy_file = {0:s}\".format(self.accuracy_file))\n",
        "    print(\"denovo_only_file = {0:s}\".format(self.denovo_only_file))\n",
        "    print(\"scan2fea_file = {0:s}\".format(self.scan2fea_file))\n",
        "    print(\"multifea_file = {0:s}\".format(self.multifea_file))\n",
        "\n",
        "    self.target_dict = {}\n",
        "    self.predicted_list = []\n",
        "\n",
        "\n",
        "  def test_accuracy(self, db_peptide_list=None):\n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
        "    print(\"WorkerTest.test_accuracy()\")\n",
        "\n",
        "    # write the accuracy of predicted peptides\n",
        "    accuracy_handle = open(self.accuracy_file, 'w')\n",
        "    header_list = [\"feature_id\",\n",
        "                   \"feature_area\",\n",
        "                   \"target_sequence\",\n",
        "                   \"predicted_sequence\",\n",
        "                   \"predicted_score\",\n",
        "                   \"recall_AA\",\n",
        "                   \"predicted_len\",\n",
        "                   \"target_len\",\n",
        "                   \"scan_list_middle\",\n",
        "                   \"scan_list_original\"]\n",
        "    header_row = \"\\t\".join(header_list)\n",
        "    print(header_row, file=accuracy_handle, end=\"\\n\")\n",
        "\n",
        "    # write denovo_only peptides (sequence with a reference to coompare)\n",
        "    denovo_only_handle = open(self.denovo_only_file, 'w')\n",
        "    header_list = [\"feature_id\",\n",
        "                   \"feature_area\",\n",
        "                   \"predicted_sequence\",\n",
        "                   \"predicted_score\",\n",
        "                   \"predicted_score_max\",\n",
        "                   \"scan_list_middle\",\n",
        "                   \"scan_list_original\"]\n",
        "    header_row = \"\\t\".join(header_list)\n",
        "    print(header_row, file=denovo_only_handle, end=\"\\n\")\n",
        "\n",
        "    self._get_target()\n",
        "    target_count_total = len(self.target_dict)\n",
        "    target_len_total = sum([len(x) for x in self.target_dict.values()])\n",
        "    target_dict_db = {}\n",
        "\n",
        "    #Depricated if no use of PEAKS DB to compare the sequence predicted (in SMBP case, use of MASCOT)\n",
        "\n",
        "    # this part is tricky!\n",
        "    # some target peptides are reported by PEAKS DB but not found in\n",
        "    #   db_peptide_list due to mistakes in cleavage rules.\n",
        "    # if db_peptide_list is given, we only consider those target peptides,\n",
        "    #   otherwise, use all target peptides\n",
        "    if db_peptide_list is not None:\n",
        "      for feature_id, target in self.target_dict.items():\n",
        "        target_simplied = target\n",
        "        # remove the extension 'mod' from variable modifications\n",
        "        target_simplied = ['M' if x=='M(Oxidation)' else x for x in target_simplied]\n",
        "        target_simplied = ['N' if x=='N(Deamidation)' else x for x in target_simplied]\n",
        "        target_simplied = ['Q' if x=='Q(Deamidation)' else x for x in target_simplied]\n",
        "\n",
        "        #target_simplied = ['M' if x=='M(Oxidated)' else x for x in target_simplied]\n",
        "        #target_simplied = ['Q' if x=='Q(Deamidated)' else x for x in target_simplied]\n",
        "        #target_simplied = ['C' if x=='C(Carboxymethyl)' else x for x in target_simplied]\n",
        "\n",
        "        if target_simplied in db_peptide_list:\n",
        "          target_dict_db[feature_id] = target\n",
        "        else:\n",
        "          print(\"target not found: \", target_simplied)\n",
        "    #==========================================================================\n",
        "    else:\n",
        "      target_dict_db = self.target_dict\n",
        "    target_count_db = len(target_dict_db)\n",
        "    target_len_db = sum([len(x) for x in target_dict_db.values()])\n",
        "\n",
        "    # we also skip target peptides with precursor_mass > MZ_MAX\n",
        "    target_dict_db_mass = {}\n",
        "    for feature_id, peptide in target_dict_db.items():\n",
        "      if self._compute_peptide_mass(peptide) <= self.MZ_MAX:\n",
        "        target_dict_db_mass[feature_id] = peptide\n",
        "    target_count_db_mass = len(target_dict_db_mass)\n",
        "    target_len_db_mass = sum([len(x) for x in target_dict_db_mass.values()])\n",
        "\n",
        "    # read predicted peptides from deepnovo or peaks\n",
        "    if predicted_format == \"deepnovo\":\n",
        "      self._get_predicted()\n",
        "    else:\n",
        "      self._get_predicted_peaks()\n",
        "\n",
        "    # note that the prediction has already skipped precursor_mass > MZ_MAX\n",
        "    # we also skip predicted peptides whose feature_id's are not in target_dict_db_mass\n",
        "    predicted_count_mass = len(self.predicted_list)\n",
        "    predicted_count_mass_db = 0\n",
        "    predicted_len_mass_db = 0\n",
        "    predicted_only = 0\n",
        "    # the recall is calculated on remaining peptides\n",
        "    recall_AA_total = 0.0\n",
        "    recall_peptide_total = 0.0\n",
        "\n",
        "    # record scan with multiple features\n",
        "    scan_dict = {}\n",
        "\n",
        "    for index, predicted in enumerate(self.predicted_list):\n",
        "\n",
        "      feature_id = predicted[\"feature_id\"]\n",
        "      feature_area = str(predicted[\"feature_area\"])\n",
        "      feature_scan_list_middle = predicted[\"scan_list_middle\"]\n",
        "      feature_scan_list_original = predicted[\"scan_list_original\"]\n",
        "      if feature_scan_list_original:\n",
        "        for scan in re.split(';|\\r|\\n', feature_scan_list_original):\n",
        "          if scan in scan_dict:\n",
        "            scan_dict[scan][\"feature_count\"] += 1\n",
        "            scan_dict[scan][\"feature_list\"].append(feature_id)\n",
        "          else:\n",
        "            scan_dict[scan] = {}\n",
        "            scan_dict[scan][\"feature_count\"] = 1\n",
        "            scan_dict[scan][\"feature_list\"] = [feature_id]\n",
        "\n",
        "      if feature_id in target_dict_db_mass:\n",
        "\n",
        "        predicted_count_mass_db += 1\n",
        "\n",
        "        target = target_dict_db_mass[feature_id]\n",
        "        target_len= len(target)\n",
        "\n",
        "        # if >= 1 denovo peptides reported, calculate the best accuracy\n",
        "        best_recall_AA = 0\n",
        "        best_predicted_sequence = predicted[\"sequence\"][0]\n",
        "        best_predicted_score = predicted[\"score\"][0]\n",
        "        for predicted_sequence, predicted_score in zip(predicted[\"sequence\"], predicted[\"score\"]):\n",
        "          predicted_AA_id = [vocab[x] for x in predicted_sequence]\n",
        "          target_AA_id = [vocab[x] for x in target]\n",
        "          recall_AA = self._match_AA_novor(target_AA_id, predicted_AA_id)\n",
        "          if (recall_AA > best_recall_AA\n",
        "              or (recall_AA == best_recall_AA and predicted_score > best_predicted_score)):\n",
        "            best_recall_AA = recall_AA\n",
        "            best_predicted_sequence = predicted_sequence[:]\n",
        "            best_predicted_score = predicted_score\n",
        "        recall_AA = best_recall_AA\n",
        "        predicted_sequence = best_predicted_sequence[:]\n",
        "        predicted_score = best_predicted_score\n",
        "\n",
        "        recall_AA_total += recall_AA\n",
        "        if recall_AA == target_len:\n",
        "          recall_peptide_total += 1\n",
        "        predicted_len= len(predicted_sequence)\n",
        "        predicted_len_mass_db += predicted_len\n",
        "\n",
        "        # convert to string format to print out\n",
        "        target_sequence = \",\".join(target)\n",
        "        predicted_sequence = \",\".join(predicted_sequence)\n",
        "        predicted_score = \"{0:.2f}\".format(predicted_score)\n",
        "        recall_AA = \"{0:d}\".format(recall_AA)\n",
        "        predicted_len = \"{0:d}\".format(predicted_len)\n",
        "        target_len = \"{0:d}\".format(target_len)\n",
        "        print_list = [feature_id,\n",
        "                      feature_area,\n",
        "                      target_sequence,\n",
        "                      predicted_sequence,\n",
        "                      predicted_score,\n",
        "                      recall_AA,\n",
        "                      predicted_len,\n",
        "                      target_len,\n",
        "                      feature_scan_list_middle,\n",
        "                      feature_scan_list_original]\n",
        "        print_row = \"\\t\".join(print_list)\n",
        "        print(print_row, file=accuracy_handle, end=\"\\n\")\n",
        "      else:\n",
        "        predicted_only += 1\n",
        "        predicted_sequence = ';'.join([','.join(x) for x in predicted[\"sequence\"]])\n",
        "        predicted_score = ';'.join(['{0:.2f}'.format(x) for x in predicted[\"score\"]])\n",
        "        if predicted[\"score\"]:\n",
        "          predicted_score_max = '{0:.2f}'.format(np.max(predicted[\"score\"]))\n",
        "        else:\n",
        "          predicted_score_max = ''\n",
        "        print_list = [feature_id,\n",
        "                      feature_area,\n",
        "                      predicted_sequence,\n",
        "                      predicted_score,\n",
        "                      predicted_score_max,\n",
        "                      feature_scan_list_middle,\n",
        "                      feature_scan_list_original]\n",
        "        print_row = \"\\t\".join(print_list)\n",
        "        print(print_row, file=denovo_only_handle, end=\"\\n\")\n",
        "\n",
        "    accuracy_handle.close()\n",
        "    denovo_only_handle.close()\n",
        "\n",
        "    multifea_dict = {}\n",
        "    for scan_id, value in scan_dict.items():\n",
        "      feature_count = value[\"feature_count\"]\n",
        "      feature_list = value[\"feature_list\"]\n",
        "      if feature_count > 1:\n",
        "        for feature_id in feature_list:\n",
        "          if feature_id in multifea_dict:\n",
        "            multifea_dict[feature_id].append(scan_id + ':' + str(feature_count))\n",
        "          else:\n",
        "            multifea_dict[feature_id] = [scan_id + ':' + str(feature_count)]\n",
        "\n",
        "    #scan2fea_file. Display the nomber of identic scan in a the spectrum\n",
        "    with open(self.scan2fea_file, 'w') as handle:\n",
        "      header_list = [\"scan_id\",\n",
        "                     \"feature_count\",\n",
        "                     \"feature_list\"]\n",
        "      header_row = \"\\t\".join(header_list)\n",
        "      print(header_row, file=handle, end=\"\\n\")\n",
        "      for scan_id, value in scan_dict.items():\n",
        "        print_list = [scan_id,\n",
        "                      str(value[\"feature_count\"]),\n",
        "                      \";\".join(value[\"feature_list\"])]\n",
        "        print_row = \"\\t\".join(print_list)\n",
        "        print(print_row, file=handle, end=\"\\n\")\n",
        "    #multifea_file. Display wich scans are doubles in spectrum\n",
        "    with open(self.multifea_file, 'w') as handle:\n",
        "      header_list = [\"feature_id\",\n",
        "                     \"scan_list\"]\n",
        "      header_row = \"\\t\".join(header_list)\n",
        "      print(header_row, file=handle, end=\"\\n\")\n",
        "      for feature_id, scan_list in multifea_dict.items():\n",
        "        print_list = [feature_id,\n",
        "                      \";\".join(scan_list)]\n",
        "        print_row = \"\\t\".join(print_list)\n",
        "        print(print_row, file=handle, end=\"\\n\")\n",
        "\n",
        "    print(\"target_count_total = {0:d}\".format(target_count_total))\n",
        "    print(\"target_len_total = {0:d}\".format(target_len_total))\n",
        "    print(\"target_count_db = {0:d}\".format(target_count_db))\n",
        "    print(\"target_len_db = {0:d}\".format(target_len_db))\n",
        "    print(\"target_count_db_mass: {0:d}\".format(target_count_db_mass))\n",
        "    print(\"target_len_db_mass: {0:d}\".format(target_len_db_mass))\n",
        "\n",
        "    print(\"predicted_count_mass: {0:d}\".format(predicted_count_mass))\n",
        "    print(\"predicted_count_mass_db: {0:d}\".format(predicted_count_mass_db))\n",
        "    print(\"predicted_len_mass_db: {0:d}\".format(predicted_len_mass_db))\n",
        "    print(\"predicted_only: {0:d}\".format(predicted_only))\n",
        "\n",
        "    #if added in case of failed training to avoid an error.\n",
        "    if target_len_total != 0:\n",
        "      print(\"recall_AA_total = {0:.4f}\".format(recall_AA_total / target_len_total))\n",
        "    if target_len_db != 0:\n",
        "      print(\"recall_AA_db = {0:.4f}\".format(recall_AA_total / target_len_db))\n",
        "    if target_len_db_mass != 0:\n",
        "      print(\"recall_AA_db_mass = {0:.4f}\".format(recall_AA_total / target_len_db_mass))\n",
        "    if target_count_total != 0:\n",
        "      print(\"recall_peptide_total = {0:.4f}\".format(recall_peptide_total / target_count_total))\n",
        "    if target_count_db != 0:\n",
        "      print(\"recall_peptide_db = {0:.4f}\".format(recall_peptide_total / target_count_db))\n",
        "    if target_count_db_mass != 0:\n",
        "      print(\"recall_peptide_db_mass = {0:.4f}\".format(recall_peptide_total / target_count_db_mass))\n",
        "    if predicted_len_mass_db != 0:\n",
        "      print(\"precision_AA_mass_db  = {0:.4f}\".format(recall_AA_total / predicted_len_mass_db))\n",
        "    if predicted_count_mass_db != 0:\n",
        "      print(\"precision_peptide_mass_db  = {0:.4f}\".format(recall_peptide_total / predicted_count_mass_db))\n",
        "  \n",
        "  \n",
        "  def _compute_peptide_mass(self, peptide):\n",
        "    \"\"\"\n",
        "    Add start and en mass to the aa chains\n",
        "    \"\"\"\n",
        "    #print(\"\".join([\"=\"] * 80)) # section-separating line\n",
        "    #print(\"WorkerTest._compute_peptide_mass()\")\n",
        "    peptide_mass = (mass_N_terminus\n",
        "                    + sum(mass_AA[aa] for aa in peptide)\n",
        "                    + mass_C_terminus)\n",
        "\n",
        "    return peptide_mass\n",
        "\n",
        "\n",
        "  def _get_predicted(self):\n",
        "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
        "    print(\"WorkerTest._get_predicted()\")\n",
        "\n",
        "    predicted_list = []\n",
        "    col_feature_id = pcol_feature_id\n",
        "    col_feature_area = pcol_feature_area\n",
        "    col_sequence = pcol_sequence\n",
        "    col_score = pcol_score\n",
        "    col_scan_list_middle = pcol_scan_list_middle\n",
        "    col_scan_list_original = pcol_scan_list_original\n",
        "    with open(self.predicted_file, 'r') as handle:\n",
        "      # header\n",
        "      handle.readline()\n",
        "      for line in handle:\n",
        "        line_split = re.split('\\t|\\n', line)\n",
        "        predicted = {}\n",
        "        predicted[\"feature_id\"] = line_split[col_feature_id]\n",
        "        predicted[\"feature_area\"] = float(line_split[col_feature_area])\n",
        "        predicted[\"scan_list_middle\"] = line_split[col_scan_list_middle]\n",
        "        predicted[\"scan_list_original\"] = line_split[col_scan_list_original]\n",
        "        if line_split[col_sequence]: # not empty sequence\n",
        "          predicted[\"sequence\"] = [re.split(',', x)\n",
        "                                   for x in re.split(';', line_split[col_sequence])]\n",
        "          predicted[\"score\"] = [float(x)\n",
        "                                for x in re.split(';', line_split[col_score])]\n",
        "        else: \n",
        "          predicted[\"sequence\"] = [[]]\n",
        "          predicted[\"score\"] = [-999]\n",
        "        predicted_list.append(predicted)\n",
        "\n",
        "    self.predicted_list = predicted_list\n",
        "\n",
        "\n",
        "  def _get_predicted_peaks(self):\n",
        "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
        "    print(\"WorkerTest._get_predicted_peaks()\")\n",
        "\n",
        "    predicted_list = []\n",
        "    col_fraction_id = 0\n",
        "    fraction_id_map = {'1':'1',\n",
        "                       '2':'10',\n",
        "                       '3':'11',\n",
        "                       '4':'12',\n",
        "                       '5':'2',\n",
        "                       '6':'3',\n",
        "                       '7':'4',\n",
        "                       '8':'5',\n",
        "                       '9':'6',\n",
        "                       '10':'7',\n",
        "                       '11':'8',\n",
        "                       '12':'9',\n",
        "                      }\n",
        "    col_scan_id = 1\n",
        "    col_sequence = 3\n",
        "    with open(self.predicted_file, 'r') as handle:\n",
        "      # header\n",
        "      handle.readline()\n",
        "      for line in handle:\n",
        "        line_split = re.split(',|\\n', line)\n",
        "        predicted = {}\n",
        "        predicted[\"feature_id\"] = \"F\" + line_split[col_fraction_id] + \":\" + line_split[col_scan_id]\n",
        "        raw_sequence = line_split[col_sequence]\n",
        "        assert raw_sequence, \"Error: wrong format.\"\n",
        "        predicted[\"sequence\"] = self._parse_sequence(raw_sequence)\n",
        "        \n",
        "        # skip peptides with precursor_mass > MZ_MAX\n",
        "        if self._compute_peptide_mass(predicted[\"sequence\"]) > self.MZ_MAX:\n",
        "          continue\n",
        "        predicted[\"feature_area\"] = 0\n",
        "        predicted[\"scan_list_middle\"] = \"\"\n",
        "        predicted[\"scan_list_original\"] = \"\"\n",
        "        predicted[\"sequence\"] = [predicted[\"sequence\"]]\n",
        "        predicted[\"score\"] = [-999]\n",
        "        predicted_list.append(predicted)\n",
        "\n",
        "    self.predicted_list = predicted_list\n",
        "\n",
        "\n",
        "  def _get_target(self):\n",
        "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
        "    print(\"WorkerTest._get_target()\")\n",
        "\n",
        "    target_dict = {}\n",
        "    with open(self.target_file, 'r') as handle:\n",
        "      header_line = handle.readline()\n",
        "      header = header_line.strip().split(',')\n",
        "      raw_sequence_index = header.index(col_raw_sequence)\n",
        "      for line in handle:\n",
        "        line = re.split(',|\\r|\\n', line)\n",
        "        feature_id = line[0]\n",
        "        raw_sequence = line[raw_sequence_index]\n",
        "        assert raw_sequence, \"Error: wrong target format.\"\n",
        "        peptide = self._parse_sequence(raw_sequence)\n",
        "        target_dict[feature_id] = peptide\n",
        "    self.target_dict = target_dict\n",
        "\n",
        "\n",
        "  def _parse_sequence(self, raw_sequence):\n",
        "\n",
        "    #print(\"\".join([\"=\"] * 80)) # section-separating line\n",
        "    #print(\"WorkerTest._parse_sequence()\")\n",
        "\n",
        "    return re.findall(r'[A-Z](?:\\(.+?\\))?', raw_sequence)\n",
        "\n",
        "    \"\"\" # Depricated if use the MASCOT processing script. Uncomment if use the \n",
        "    data from the original paper of Tran et al. 2019.\n",
        "\n",
        "    raw_sequence_len = len(raw_sequence)\n",
        "    peptide = []\n",
        "    index = 0\n",
        "    while index < raw_sequence_len:\n",
        "      if raw_sequence[index] == \"(\":\n",
        "        if peptide[-1] == \"C\" and raw_sequence[index:index+8] == \"(+57.02)\":\n",
        "          peptide[-1] = \"C(Carbamidomethylation)\"\n",
        "          index += 8\n",
        "        elif peptide[-1] == 'M' and raw_sequence[index:index+8] == \"(+15.99)\":\n",
        "          peptide[-1] = 'M(Oxidation)'\n",
        "          index += 8\n",
        "        elif peptide[-1] == 'N' and raw_sequence[index:index+6] == \"(+.98)\":\n",
        "          peptide[-1] = 'N(Deamidation)'\n",
        "          index += 6\n",
        "        elif peptide[-1] == 'Q' and raw_sequence[index:index+6] == \"(+.98)\":\n",
        "          peptide[-1] = 'Q(Deamidation)'\n",
        "          index += 6\n",
        "        else: # unknown modification\n",
        "          print(\"ERROR: unknown modification!\")\n",
        "          print(\"raw_sequence = \", raw_sequence)\n",
        "          sys.exit()\n",
        "      else:\n",
        "        peptide.append(raw_sequence[index])\n",
        "        index += 1\n",
        "\n",
        "    return peptide\n",
        "    \"\"\"\n",
        "\n",
        "  def _match_AA_novor(self, target, predicted):\n",
        "    \"\"\"\"\"\"\n",
        "  \n",
        "    #print(\"\".join([\"=\"] * 80)) # section-separating line\n",
        "    #print(\"WorkerTest._test_AA_match_novor()\")\n",
        "\n",
        "    num_match = 0\n",
        "    target_len = len(target)\n",
        "    predicted_len = len(predicted)\n",
        "    target_mass = [mass_ID[x] for x in target]\n",
        "    target_mass_cum = np.cumsum(target_mass)\n",
        "    predicted_mass = [mass_ID[x] for x in predicted]\n",
        "    predicted_mass_cum = np.cumsum(predicted_mass)\n",
        "  \n",
        "    i = 0\n",
        "    j = 0\n",
        "    while i < target_len and j < predicted_len:\n",
        "      if abs(target_mass_cum[i] - predicted_mass_cum[j]) < 0.5:\n",
        "        if abs(target_mass[i] - predicted_mass[j]) < 0.1:\n",
        "        #~ if  decoder_input[index_aa] == output[index_aa]:\n",
        "          num_match += 1\n",
        "        i += 1\n",
        "        j += 1\n",
        "      elif target_mass_cum[i] < predicted_mass_cum[j]:\n",
        "        i += 1\n",
        "      else:\n",
        "        j += 1\n",
        "\n",
        "    return num_match"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xip7p39LGX9",
        "colab_type": "text"
      },
      "source": [
        "## Read feature accuracy <a name=\"test_accuracy\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrHGuoTPLGgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_feature_accuracy(input_file, split_char):\n",
        "  \"\"\"\n",
        "  Read and convert the accuracy file and return a list\n",
        "  \"\"\"\n",
        "  feature_list = []\n",
        "  with open(input_file, 'r') as handle:\n",
        "    header_line = handle.readline()\n",
        "    for line in handle:\n",
        "      line = re.split(split_char, line)\n",
        "      feature = {}\n",
        "      feature[\"feature_id\"] = line[0]\n",
        "      feature[\"feature_area\"] = math.log10(float(line[1]) + 1e-5)\n",
        "      feature[\"predicted_score\"] = float(line[4])\n",
        "      feature[\"recall_AA\"] = float(line[5])\n",
        "      feature[\"predicted_len\"] = float(line[6])\n",
        "      feature_list.append(feature)\n",
        "  return feature_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47qh4W2cLAcH",
        "colab_type": "text"
      },
      "source": [
        "## Find score cutoff <a name=\"test_cutoff\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-13dv_VALAm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_score_cutoff(accuracy_file, accuracy_cutoff):\n",
        "  \"\"\"\n",
        "  Find the sequences with an acccurcy higher than the cutoff and return the count,\n",
        "  and the average score computed of the predicted data.\n",
        "  \"\"\"\n",
        "  print(\"\".join([\"=\"] * 80)) # section-separating line\n",
        "  print(\"find_score_cutoff()\")\n",
        "\n",
        "  feature_list = read_feature_accuracy(accuracy_file, '\\t|\\r|\\n')\n",
        "  feature_list_sorted = sorted(feature_list, key=lambda k: k['predicted_score'], reverse=True)\n",
        "  recall_cumsum = np.cumsum([f['recall_AA'] for f in feature_list_sorted])\n",
        "  predicted_len_cumsum = np.cumsum([f['predicted_len'] for f in feature_list_sorted])\n",
        "  accuracy_cumsum = recall_cumsum / predicted_len_cumsum\n",
        "  cutoff_index = np.flatnonzero(accuracy_cumsum < accuracy_cutoff)[0]\n",
        "  cutoff_score = feature_list_sorted[cutoff_index]['predicted_score']\n",
        "  print('cutoff_index = ', cutoff_index)\n",
        "  print('cutoff_score = ', cutoff_score)\n",
        "  print('cutoff_score = ', 100*math.exp(cutoff_score))\n",
        "\n",
        "  return cutoff_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FVmA0MHKbOU",
        "colab_type": "text"
      },
      "source": [
        "## Testing Launch section <a name=\"test_launch\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ2SvmmZKacI",
        "colab_type": "code",
        "outputId": "56beb669-19e1-4e73-d2a7-91ac8dcee686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if 'test' in option:\n",
        "  worker_test = WorkerTest()\n",
        "  worker_test.test_accuracy()\n",
        "\n",
        "  # show 95 accuracy score threshold\n",
        "  accuracy_cutoff = 0.95\n",
        "  score_cutoff = find_score_cutoff(accuracy_file, accuracy_cutoff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "WorkerTest.__init__()\n",
            "target_file = Test_files/features_smbp.csv.test\n",
            "predicted_file = Test_files/features_smbp.csv.test.deepnovo_denovo\n",
            "predicted_format = deepnovo\n",
            "accuracy_file = Test_files/features_smbp.csv.test.deepnovo_denovo.accuracy\n",
            "denovo_only_file = Test_files/features_smbp.csv.test.deepnovo_denovo.denovo_only\n",
            "scan2fea_file = Test_files/features_smbp.csv.test.deepnovo_denovo.scan2fea\n",
            "multifea_file = Test_files/features_smbp.csv.test.deepnovo_denovo.multifea\n",
            "================================================================================\n",
            "WorkerTest.test_accuracy()\n",
            "================================================================================\n",
            "WorkerTest._get_target()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._parse_sequence()\n",
            "================================================================================\n",
            "WorkerTest._get_predicted()\n",
            "target_count_total = 467\n",
            "target_len_total = 6669\n",
            "target_count_db = 467\n",
            "target_len_db = 6669\n",
            "target_count_db_mass: 467\n",
            "target_len_db_mass: 6669\n",
            "predicted_count_mass: 323\n",
            "predicted_count_mass_db: 323\n",
            "predicted_len_mass_db: 4445\n",
            "predicted_only: 0\n",
            "recall_AA_total = 0.5112\n",
            "recall_AA_db = 0.5112\n",
            "recall_AA_db_mass = 0.5112\n",
            "recall_peptide_total = 0.3533\n",
            "recall_peptide_db = 0.3533\n",
            "recall_peptide_db_mass = 0.3533\n",
            "precision_AA_mass_db  = 0.7669\n",
            "precision_peptide_mass_db  = 0.5108\n",
            "================================================================================\n",
            "find_score_cutoff()\n",
            "cutoff_index =  170\n",
            "cutoff_score =  -0.51\n",
            "cutoff_score =  60.04955788122659\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}