{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nadJTaCa1qZm"
   },
   "source": [
    "# DeepNovoV2 - Jupyter Notebook Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUVW9WZWc9RF"
   },
   "source": [
    "As part of the _Introduction to Deep Learning_ module at ESPCI Paris-PSL, the DeepNovoV2 model by [Qiao et al.](https://arxiv.org/abs/1904.08514) has been dismantled and adapted to work in a Jupyter notebook environment by Simon Chardin and Samuel Diebolt. It was then modified to work with any MGF spectrum file and annotation results from the Mascot software. The model was then trained with data offered by the _Spectrométrie de Masse Biologique et Protéomique (SMBP)_ laboratory from ESPCI Paris-PSL.\n",
    "\n",
    "This notebook is compute-intensive and was designed to run with an available GPU or virtual GPU. It should work well on Google Colab with moderately sized datasets.\n",
    "\n",
    "The steps that will be run by the notebook are defined by a list in the following cell, with the following options:\n",
    "\n",
    "- `train` : training mode;\n",
    "- `valid` : validation of the previously trained model. This step is performed during training;\n",
    "- `denovo` : _de novo_ peptide sequencing;\n",
    "- `test` : evaluation of the _de novo_ results with the theoretical sequences.\n",
    "\n",
    "After having selected the desired steps, all cells should be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oL9vJb3fSwB4"
   },
   "outputs": [],
   "source": [
    "option = ['train', 'denovo', 'valid', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynbyzPNE1qZn"
   },
   "outputs": [],
   "source": [
    "# Python libraries.\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezdmjbK-1qZs"
   },
   "source": [
    "# Table of Contents\n",
    "1. [Model Configuration](#modelconfiguration)\n",
    "2. [Feature Class, Helper Functions](#featureclass)\n",
    "3. [Train Function](#train_func)\n",
    "4. [Neural Networks Models](#model)\n",
    "5. [Inference Model Wrapper](#inference) \n",
    "6. [Build Model](#build_func)\n",
    "7. [Data Reader Function](#reader)\n",
    "8. [Accecories Functions](#accesories)\n",
    "    1. [Collate Function](#collate)  \n",
    "    2. [Extract & Move Data](#extract)  \n",
    "    3. [Focal Loss Function](#loss)\n",
    "9. [Validation Function](#valid_func)\n",
    "10. [Cython Intregration](#cython)\n",
    "11. [Training Launch sequence](#train)\n",
    "12. [Validation of the trainning](#valid)\n",
    "13. [Denovo Part](#denovo)\n",
    "    1. [Denovo File Path](#denovo_path)  \n",
    "    2. [Denovo Data Reader](#denovo_datareader)  \n",
    "    3. [Results Writer Function](#denovo_writer)  \n",
    "    4. [Knapsack Implementation](#knapsack)   \n",
    "    5. [ION CNN Denovo](#ioncnn)  \n",
    "    6. [Denovo Launch Sequence](#denovo_launch)\n",
    "13. [Test of the prediction](#test)\n",
    "    1. [Testing File path selection](#test_path)  \n",
    "    2. [Worker Test function](#test_worker)  \n",
    "    3. [Read Feature Accuracy](#test_accuracy)  \n",
    "    4. [Score cutoff function](#test_cutoff)  \n",
    "    5. [Testing Launch Sequence](#test_launch)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "041AcISl1qZs"
   },
   "source": [
    "## Model Configuration <a name=\"model configuration\"></a>\n",
    "\n",
    "All parameters contained in the original configuration file `deepnovo_config.py` are declared in the following cells. \n",
    "\n",
    "**Caution**: the `batch_size` parameter needs to be assigned in the [`train` function](#train_func) due to an unresolved issue (line 14)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAIipyT3SGTS"
   },
   "source": [
    "### Global Variables and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7FO4G1qx1qZt",
    "outputId": "76b35069-08e4-4a16-8fc1-bebad76371d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Specify is the model will use one or more LSTM layers.\n",
    "use_lstm = True\n",
    "\n",
    "# Directory where the training parameters will be saved. Must be\n",
    "# created by the user beforehand.\n",
    "train_dir = 'train'\n",
    "\n",
    "# Name of the training parameters saved to the previous directory.\n",
    "forward_model_save_name = 'forward_deepnovo.pth'\n",
    "backward_model_save_name = 'backward_deepnovo.pth'\n",
    "init_net_save_name = 'init_net.pth'\n",
    "\n",
    "# Activation function for the model.\n",
    "activation_func = F.relu\n",
    "\n",
    "# Enable CUDA if available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n",
    "\n",
    "class Direction(Enum):\n",
    "    forward = 1\n",
    "    backward = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "HaG-BvhVL4jA",
    "outputId": "3ef70220-e17d-414d-fe65-13a9579dbb13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_reverse  ['_PAD', '_GO', '_EOS', 'A', 'R', 'N', 'N(Deamidated)', 'D', 'C', 'C(Carboxymethyl)', 'E', 'Q', 'Q(Deamidated)', 'G', 'H', 'I', 'L', 'K', 'M', 'M(Oxidation)', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
      "vocab  {'_PAD': 0, '_GO': 1, '_EOS': 2, 'A': 3, 'R': 4, 'N': 5, 'N(Deamidated)': 6, 'D': 7, 'C': 8, 'C(Carboxymethyl)': 9, 'E': 10, 'Q': 11, 'Q(Deamidated)': 12, 'G': 13, 'H': 14, 'I': 15, 'L': 16, 'K': 17, 'M': 18, 'M(Oxidation)': 19, 'F': 20, 'P': 21, 'S': 22, 'T': 23, 'W': 24, 'Y': 25, 'V': 26}\n",
      "vocab_size  27\n",
      "WINDOW_SIZE  10\n",
      "MAX_LEN  30\n",
      "num_ion  12\n",
      "weight_decay  0.0\n",
      "embedding_size  512\n",
      "num_lstm_layers  2\n",
      "num_units  64\n",
      "batch_size  24\n",
      "steps_per_validation  10\n",
      "Beam size =  5\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# GLOBAL VARIABLES for VOCABULARY\n",
    "# ==============================================================================\n",
    "\n",
    "# Special vocabulary symbols - we always put them at the start.\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "assert PAD_ID == 0\n",
    "\n",
    "# Amino acids vocabulary. This must match the vocabulary used in the\n",
    "# sequences available in the features file.\n",
    "vocab_reverse = ['A',\n",
    "                 'R',\n",
    "                 'N',\n",
    "                 'N(Deamidated)',\n",
    "                 'D',\n",
    "                 'C',\n",
    "                 'C(Carboxymethyl)',\n",
    "                 'E',\n",
    "                 'Q',\n",
    "                 'Q(Deamidated)',\n",
    "                 'G',\n",
    "                 'H',\n",
    "                 'I',\n",
    "                 'L',\n",
    "                 'K',\n",
    "                 'M',\n",
    "                 'M(Oxidation)',\n",
    "                 'F',\n",
    "                 'P',\n",
    "                 'S',\n",
    "                 'T',\n",
    "                 'W',\n",
    "                 'Y',\n",
    "                 'V']\n",
    "\n",
    "vocab_reverse = _START_VOCAB + vocab_reverse\n",
    "print(\"vocab_reverse \", vocab_reverse)\n",
    "\n",
    "vocab = dict([(x, y) for (y, x) in enumerate(vocab_reverse)])\n",
    "print(\"vocab \", vocab)\n",
    "\n",
    "vocab_size = len(vocab_reverse)\n",
    "print(\"vocab_size \", vocab_size)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# GLOBAL VARIABLES for THEORETICAL MASS\n",
    "# ==============================================================================\n",
    "mass_H = 1.0078\n",
    "mass_H2O = 18.0106\n",
    "mass_NH3 = 17.0265\n",
    "mass_N_terminus = 1.0078\n",
    "mass_C_terminus = 17.0027\n",
    "mass_CO = 27.9949\n",
    "\n",
    "# Masses associated to the vocabulary.\n",
    "mass_AA = {'_PAD': 0.0,\n",
    "           '_GO': mass_N_terminus-mass_H,\n",
    "           '_EOS': mass_C_terminus+mass_H,\n",
    "           'A': 71.03711,\n",
    "           'R': 156.10111,\n",
    "           'N': 114.04293,\n",
    "           'N(Deamidated)': 115.02695,\n",
    "           'D': 115.02694,\n",
    "           'C': 103.00919,\n",
    "           'C(Carboxymethyl)': 161.01919,\n",
    "           'E': 129.04259,\n",
    "           'Q': 128.05858,\n",
    "           'Q(Deamidated)': 129.0426,\n",
    "           'G': 57.02146,\n",
    "           'H': 137.05891,\n",
    "           'I': 113.08406,\n",
    "           'L': 113.08406, \n",
    "           'K': 128.09496,\n",
    "           'M': 131.04049,\n",
    "           'M(Oxidation)': 147.0354,\n",
    "           'F': 147.06841,\n",
    "           'P': 97.05276,\n",
    "           'S': 87.03203,\n",
    "           'T': 101.04768,\n",
    "           'W': 186.07931,\n",
    "           'Y': 163.06333,\n",
    "           'V': 99.06841,\n",
    "          }\n",
    "\n",
    "mass_ID = [mass_AA[vocab_reverse[x]] for x in range(vocab_size)]\n",
    "mass_ID_np = np.array(mass_ID, dtype=np.float32)\n",
    "\n",
    "mass_AA_min = mass_AA[\"G\"] # 57.02146\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# GLOBAL VARIABLES for PRECISION, RESOLUTION, temp-Limits of MASS & LEN\n",
    "# ==============================================================================\n",
    "\n",
    "WINDOW_SIZE = 10 # 10 bins\n",
    "print(\"WINDOW_SIZE \", WINDOW_SIZE)\n",
    "\n",
    "# Maximum peptide m/z ratio allowed.\n",
    "MZ_MAX = 3000.0\n",
    "\n",
    "# Number of top peaks selected in each spectrum.\n",
    "MAX_NUM_PEAK = 500\n",
    "\n",
    "# Knapsack dynamic programming parameters.\n",
    "knapsack_file = \"knapsack.npy\"\n",
    "KNAPSACK_AA_RESOLUTION = 10000 # 0.0001 Da\n",
    "mass_AA_min_round = int(round(mass_AA_min * KNAPSACK_AA_RESOLUTION)) # 57.02146\n",
    "KNAPSACK_MASS_PRECISION_TOLERANCE = 100 # 0.01 Da\n",
    "num_position = 0\n",
    "\n",
    "PRECURSOR_MASS_PRECISION_TOLERANCE = 0.01\n",
    "\n",
    "# Tolerance for an amino acid prediction match.\n",
    "AA_MATCH_PRECISION = 0.1\n",
    "\n",
    "# Maximum peptide length.\n",
    "MAX_LEN = 30 \n",
    "print(\"MAX_LEN \", MAX_LEN)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# HYPER-PARAMETERS of the NEURAL NETWORKS\n",
    "# ==============================================================================\n",
    "num_ion = 12\n",
    "print(\"num_ion \", num_ion)\n",
    "\n",
    "weight_decay = 0.0  # no weight decay lead to better result.\n",
    "print(\"weight_decay \", weight_decay)\n",
    "\n",
    "#~ encoding_cnn_size = 4 * (RESOLUTION//10) # 4 # proportion to RESOLUTION\n",
    "#~ encoding_cnn_filter = 4\n",
    "#~ print(\"encoding_cnn_size \", encoding_cnn_size)\n",
    "#~ print(\"encoding_cnn_filter \", encoding_cnn_filter)\n",
    "\n",
    "embedding_size = 512\n",
    "print(\"embedding_size \", embedding_size)\n",
    "\n",
    "# LSTM parameters.\n",
    "num_lstm_layers = 2\n",
    "num_units = 64\n",
    "lstm_hidden_units = 512\n",
    "print(\"num_lstm_layers \", num_lstm_layers)\n",
    "print(\"num_units \", num_units)\n",
    "\n",
    "# Dropout isn't used in the LSTM, as the authors found it hindered performance.\n",
    "dropout_rate = 0.25\n",
    "\n",
    "# Batch size. Setting this value too high will cause memory issues.\n",
    "batch_size = 24\n",
    "\n",
    "num_workers = 2\n",
    "print(\"batch_size \", batch_size)\n",
    "\n",
    "# Number of epochs to use.\n",
    "num_epoch = 5\n",
    "\n",
    "# Initial learning rate.\n",
    "init_lr = 1e-3\n",
    "\n",
    "# Number of steps per validation.\n",
    "steps_per_validation = 10\n",
    "print(\"steps_per_validation \", steps_per_validation)\n",
    "\n",
    "# De novo beam search parameter.\n",
    "beam_size_param = 5 #2 #5\n",
    "print(\"Beam size = \",beam_size_param)\n",
    "\n",
    "# Feature file column format.\n",
    "col_feature_id = \"spec_group_id\"\n",
    "col_precursor_mz = \"m/z\"\n",
    "col_precursor_charge = \"z\"\n",
    "col_rt_mean = \"rt_mean\"\n",
    "col_raw_sequence = \"seq\"\n",
    "col_scan_list = \"scans\"\n",
    "col_feature_area = \"feature area\"\n",
    "\n",
    "# Predicted file column format.\n",
    "pcol_feature_id = 0\n",
    "pcol_feature_area = 1\n",
    "pcol_sequence = 2\n",
    "pcol_score = 3\n",
    "pcol_position_score = 4\n",
    "pcol_precursor_mz = 5\n",
    "pcol_precursor_charge = 6\n",
    "pcol_protein_id = 7\n",
    "pcol_scan_list_middle = 8\n",
    "pcol_scan_list_original = 9\n",
    "pcol_score_max = 10\n",
    "\n",
    "# Parameters for the Cython module.\n",
    "distance_scale_factor = 100.\n",
    "sinusoid_base = 30000.\n",
    "spectrum_reso = 10\n",
    "n_position = int(MZ_MAX) * spectrum_reso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwbZDNSCMY2q"
   },
   "source": [
    "### Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtDSIFPgMWYp"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATASETS Path\n",
    "# ==============================================================================\n",
    "\n",
    "input_spectrum_file_train = \"../smbp_data/spectrum_smbp.mgf\"\n",
    "input_feature_file_train = \"../smbp_data/features_smbp.csv.train\"\n",
    "input_spectrum_file_valid = \"../smbp_data/spectrum_smbp.mgf\"\n",
    "input_feature_file_valid = \"../smbp_data/features_smbp.csv.valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_g2StBQ1qZx"
   },
   "source": [
    "## Feature Class, Helper Functions <a name=\"featureclass\"></a>\n",
    "\n",
    "- The `Feature` class is used to store data obtained from the features file.\n",
    "- The `perplexity` function is used to compute the perplexity (i.e. the measurement of how well the model predicts the sample) from the log loss during training and validation.\n",
    "- The `adjust_learning_rate` function is used to add a learning rate decay every three epochs during training.\n",
    "- The `save_model` function will save the training parameters of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSXd-kiI1qZy"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------- Feature Class ------------------------------\n",
    "@dataclass\n",
    "class Feature:\n",
    "    spec_id: str\n",
    "    mz: str\n",
    "    z: str\n",
    "    rt_mean: str\n",
    "    seq: str\n",
    "    scan: str\n",
    "\n",
    "    def to_list(self):\n",
    "        \"\"\"Convert the dataset to list\"\"\"\n",
    "        return [self.spec_id, self.mz, self.z, self.rt_mean, self.seq, self.scan, \"0.0:1.0\", \"1.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "mTYYFEHe1qZ1"
   },
   "outputs": [],
   "source": [
    "def perplexity(log_loss):\n",
    "    \"\"\" Compute the perplexity from the loss of the model\"\"\"\n",
    "    return math.exp(log_loss) if log_loss < 300 else float('inf')\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 3 epochs\"\"\"\n",
    "    lr = init_lr * (0.1 ** ((epoch + 1) // 3))\n",
    "    print(f\"epoch: {epoch}\\tlr: {lr}\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def save_model(forward_deepnovo, backward_deepnovo, init_net):\n",
    "    \"\"\"\n",
    "    Save the model paramters for forward, backward and initalisation net, at the\n",
    "    location sets in the Global variables\n",
    "    \"\"\"\n",
    "    torch.save(forward_deepnovo.state_dict(), os.path.join(train_dir,\n",
    "                                                           forward_model_save_name))\n",
    "    torch.save(backward_deepnovo.state_dict(), os.path.join(train_dir,\n",
    "                                                            backward_model_save_name))\n",
    "    if use_lstm:\n",
    "        torch.save(init_net.state_dict(), os.path.join(train_dir,\n",
    "                                                   init_net_save_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iff0TWen1qZ4"
   },
   "source": [
    "## Train Function <a name=\"train_func\"></a>\n",
    "\n",
    "The train function performs the model training and validation. It will print out the training and validation perplexity regularly and will save the optimal model parameters using the previously defined `save_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0KT4fku1qZ5"
   },
   "outputs": [],
   "source": [
    "training_perp_tab = []\n",
    "valid_perp_tab = []\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Function govern the training of the model:\n",
    "    Parameters = input_feature_file_train\n",
    "                  input_spectrum_file_train\n",
    "    \"\"\"\n",
    "    # Training dataset creation.\n",
    "    train_set = DeepNovoTrainDataset(input_feature_file_train,\n",
    "                                     input_spectrum_file_train)\n",
    "    num_train_features = len(train_set)\n",
    "    global batch_size\n",
    "    steps_per_epoch = int(num_train_features / batch_size)\n",
    "    print(steps_per_epoch,'steps per epoch')\n",
    "    train_data_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    num_workers=num_workers,\n",
    "                                                    collate_fn=collate_func)\n",
    "    # Validation dataset creation\n",
    "    valid_set = DeepNovoTrainDataset(input_feature_file_valid,\n",
    "                                     input_spectrum_file_valid)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_workers=num_workers,\n",
    "                                                    collate_fn=collate_func)\n",
    "    \n",
    "    forward_deepnovo, backward_deepnovo, init_net = build_model()\n",
    "    dense_params = list(forward_deepnovo.parameters()) + list(backward_deepnovo.parameters())\n",
    "    dense_optimizer = optim.Adam(dense_params,\n",
    "                                 lr=init_lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "    dense_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(dense_optimizer, 'min', factor=0.5, verbose=True,\n",
    "                                                                 threshold=1e-4, cooldown=10, min_lr=1e-5)\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    # train loop\n",
    "    best_epoch = None\n",
    "    best_step = None\n",
    "    start_time = time.time()\n",
    "    for epoch in tqdm(range(num_epoch)):\n",
    "        # Adjust learning rate every 3 epoch.\n",
    "        adjust_learning_rate(dense_optimizer, epoch)\n",
    "        for i, data in enumerate(train_data_loader):\n",
    "            dense_optimizer.zero_grad()\n",
    "            peak_location, \\\n",
    "            peak_intensity, \\\n",
    "            spectrum_representation, \\\n",
    "            batch_forward_id_target, \\\n",
    "            batch_backward_id_target, \\\n",
    "            batch_forward_ion_index, \\\n",
    "            batch_backward_ion_index, \\\n",
    "            batch_forward_id_input, \\\n",
    "            batch_backward_id_input = extract_and_move_data(data)\n",
    "            batch_size = batch_backward_id_target.size(0)\n",
    "\n",
    "            if use_lstm:\n",
    "                initial_state_tuple = init_net(spectrum_representation)\n",
    "                forward_logit, _ = forward_deepnovo(batch_forward_ion_index, peak_location, peak_intensity,\n",
    "                                                    batch_forward_id_input, initial_state_tuple)\n",
    "                backward_logit, _ = backward_deepnovo(batch_backward_ion_index, peak_location, peak_intensity,\n",
    "                                                      batch_backward_id_input, initial_state_tuple)\n",
    "            else:\n",
    "                forward_logit = forward_deepnovo(batch_forward_ion_index, peak_location, peak_intensity)\n",
    "                backward_logit = backward_deepnovo(batch_backward_ion_index, peak_location, peak_intensity)\n",
    "\n",
    "            forward_loss, _ = focal_loss(forward_logit, batch_forward_id_target, ignore_index=0, gamma=2.)\n",
    "            backward_loss, _ = focal_loss(backward_logit, batch_backward_id_target, ignore_index=0, gamma=2.)\n",
    "            total_loss = (forward_loss + backward_loss) / 2.\n",
    "            # compute gradient\n",
    "            total_loss.backward()\n",
    "            dense_optimizer.step()\n",
    "            #depending of the steps_per_validation selected, print the time it takes,\n",
    "            #for the model to train, and display the loss off training and of validation\n",
    "            if (i + 1) % steps_per_validation == 0:\n",
    "                duration = time.time() - start_time\n",
    "                step_time = duration / steps_per_validation\n",
    "                loss_cpu = total_loss.item()\n",
    "                # evaluation mode\n",
    "                forward_deepnovo.eval()\n",
    "                backward_deepnovo.eval()\n",
    "                validation_loss = validation(forward_deepnovo, backward_deepnovo, init_net, valid_data_loader)\n",
    "                dense_scheduler.step(validation_loss)\n",
    "\n",
    "                #Training and validation loss are saved to display graph of evolution.\n",
    "                training_perp_tab.append(perplexity(loss_cpu))\n",
    "                valid_perp_tab.append(perplexity(validation_loss))\n",
    "\n",
    "                print(f\"epoch {epoch} step {i}/{steps_per_epoch}, \"\n",
    "                            f\"train perplexity: {perplexity(loss_cpu)}\\t\"\n",
    "                            f\"validation perplexity: {perplexity(validation_loss)}\\tstep time: {step_time}\")\n",
    "\n",
    "                if validation_loss < best_valid_loss:\n",
    "                    best_valid_loss = validation_loss\n",
    "                    print('best valid loss achieved at epoch',epoch, 'step', i)\n",
    "                    best_epoch = epoch\n",
    "                    best_step = i\n",
    "                    # save model if achieve a new best valid loss. Careful, if the\n",
    "                    #step_per_validation is to low compared to the size of the data\n",
    "                    #the model may not be saved, because not evaluated during training\n",
    "                    save_model(forward_deepnovo, backward_deepnovo, init_net)\n",
    "\n",
    "                # back to train model\n",
    "                forward_deepnovo.train()\n",
    "                backward_deepnovo.train()\n",
    "\n",
    "                start_time = time.time()\n",
    "            # observed that most of gpu memory is unoccupied cache, so clear cache after each batch\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    print('best model at epoch',best_epoch,'step',best_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXszJ8nr1qZ8"
   },
   "source": [
    "## Neural Network Models <a name=\"model\"></a>\n",
    "\n",
    "All models used in DeepNovoV2 are defined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ll4inevn1qZ9"
   },
   "source": [
    "   ### DeepNovoPointNet with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drj8yUBK1qZ9"
   },
   "outputs": [],
   "source": [
    "class DeepNovoPointNetWithLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNovoPointNetWithLSTM, self).__init__()\n",
    "        self.t_net = TNet(with_lstm=True)\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                      embedding_dim=embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_hidden_units,\n",
    "                            num_layers=num_lstm_layers,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(num_units + lstm_hidden_units,\n",
    "                                      vocab_size)\n",
    "\n",
    "    def forward(self, location_index, peaks_location, peaks_intensity, aa_input=None, state_tuple=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param location_index: [batch, T, 26, 8] long\n",
    "        :param peaks_location: [batch, N] N stands for MAX_NUM_PEAK, long\n",
    "        :param peaks_intensity: [batch, N], float32\n",
    "        :param aa_input:[batch, T]\n",
    "        :param state_tuple: (h0, c0), where each is [num_lstm_layer, batch_size, num_units] tensor\n",
    "        :return:\n",
    "            logits: [batch, T, 26]\n",
    "        \"\"\"\n",
    "        assert aa_input is not None\n",
    "        N = peaks_location.size(1)\n",
    "        assert N == peaks_intensity.size(1)\n",
    "        batch_size, T, vocab_size, num_ion = location_index.size()\n",
    "\n",
    "        peaks_location = peaks_location.view(batch_size, 1, N, 1)\n",
    "        peaks_intensity = peaks_intensity.view(batch_size, 1, N, 1)\n",
    "        peaks_location = peaks_location.expand(-1, T, -1, -1)  # [batch, T, N, 1]\n",
    "        peaks_location_mask = (peaks_location > 1e-5).float()\n",
    "        peaks_intensity = peaks_intensity.expand(-1, T, -1, -1)  # [batch, T, N, 1]\n",
    "\n",
    "        location_index = location_index.view(batch_size, T, 1, vocab_size * num_ion)\n",
    "        location_index_mask = (location_index > 1e-5).float()\n",
    "\n",
    "        location_exp_minus_abs_diff = torch.exp(\n",
    "            -torch.abs(\n",
    "                (peaks_location - location_index) * distance_scale_factor\n",
    "            )\n",
    "        )\n",
    "        # [batch, T, N, 26*8]\n",
    "\n",
    "        location_exp_minus_abs_diff = location_exp_minus_abs_diff * peaks_location_mask * location_index_mask\n",
    "\n",
    "        input_feature = torch.cat((location_exp_minus_abs_diff, peaks_intensity), dim=3)\n",
    "        input_feature = input_feature.view(batch_size * T, N, vocab_size * num_ion + 1)\n",
    "        input_feature = input_feature.transpose(1, 2)\n",
    "\n",
    "        ion_feature = self.t_net(input_feature).view(batch_size, T, num_units)  # attention on peaks\n",
    "\n",
    "        # embedding\n",
    "        aa_embedded = self.embedding(aa_input)\n",
    "        lstm_input = aa_embedded  # [batch, T, embedding_size]\n",
    "        #dropout layer\n",
    "        #lstm_input = self.dropout(lstm_input)\n",
    "        #dropout doesn't appear to be efficient in this configuration\n",
    "        output_feature, new_state_tuple = self.lstm(lstm_input, state_tuple)\n",
    "        output_feature = torch.cat((ion_feature, activation_func(output_feature)), dim=2)\n",
    "        output_feature = self.dropout(output_feature)\n",
    "        logit = self.output_layer(output_feature)\n",
    "        return logit, new_state_tuple\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F466FSfQ1qaB"
   },
   "source": [
    "### T-Net Module\n",
    "\n",
    "Same model as presented in the [PointNet article](https://arxiv.org/abs/1612.00593)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe9imurR1qaB"
   },
   "outputs": [],
   "source": [
    "class TNet(nn.Module):\n",
    "    \"\"\"\n",
    "    the T-net structure in the Point Net paper\n",
    "    \"\"\"\n",
    "    def __init__(self, with_lstm=False):\n",
    "        super(TNet, self).__init__()\n",
    "        self.with_lstm = with_lstm\n",
    "        self.conv1 = nn.Conv1d(vocab_size * num_ion + 1, num_units, 1)\n",
    "        self.conv2 = nn.Conv1d(num_units, 2*num_units, 1)\n",
    "        self.conv3 = nn.Conv1d(2*num_units, 4*num_units, 1)\n",
    "        self.fc1 = nn.Linear(4*num_units, 2*num_units)\n",
    "        self.fc2 = nn.Linear(2*num_units, num_units)\n",
    "        if not with_lstm:\n",
    "            self.output_layer = nn.Linear(num_units, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.input_batch_norm = nn.BatchNorm1d(vocab_size * num_ion + 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_units)\n",
    "        self.bn2 = nn.BatchNorm1d(2*num_units)\n",
    "        self.bn3 = nn.BatchNorm1d(4*num_units)\n",
    "        self.bn4 = nn.BatchNorm1d(2*num_units)\n",
    "        self.bn5 = nn.BatchNorm1d(num_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: [batch * T, 26*8+1, N]\n",
    "        :return:\n",
    "            logit: [batch * T, 26]\n",
    "        \"\"\"\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = activation_func(self.bn1(self.conv1(x)))\n",
    "        x = activation_func(self.bn2(self.conv2(x)))\n",
    "        x = activation_func(self.bn3(self.conv3(x)))\n",
    "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
    "        assert x.size(1) == 4*num_units\n",
    "\n",
    "        x = activation_func(self.bn4(self.fc1(x)))\n",
    "        x = activation_func(self.bn5(self.fc2(x)))\n",
    "        if not self.with_lstm:\n",
    "            x = self.output_layer(x)  # [batch * T, 26]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AloXQe0k1qaE"
   },
   "source": [
    "### Initialization Layer of the LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NewPxsP11qaF"
   },
   "outputs": [],
   "source": [
    "class InitNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InitNet, self).__init__()\n",
    "        self.init_state_layer = nn.Linear(embedding_size, 2 * lstm_hidden_units)\n",
    "\n",
    "    def forward(self, spectrum_representation):\n",
    "        \"\"\"\n",
    "\n",
    "        :param spectrum_representation: [N, embedding_size]\n",
    "        :return:\n",
    "            [num_lstm_layers, batch_size, lstm_units], [num_lstm_layers, batch_size, lstm_units],\n",
    "        \"\"\"\n",
    "        x = torch.tanh(self.init_state_layer(spectrum_representation))\n",
    "        h_0, c_0 = torch.split(x, lstm_hidden_units, dim=1)\n",
    "        h_0 = torch.unsqueeze(h_0, dim=0)\n",
    "        h_0 = h_0.repeat(num_lstm_layers, 1, 1)\n",
    "        c_0 = torch.unsqueeze(c_0, dim=0)\n",
    "        c_0 = c_0.repeat(num_lstm_layers, 1, 1)\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoXZ-4Tk1qaH"
   },
   "source": [
    "### DeepNovoPointNet Model, without LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDDlKn-u1qaI"
   },
   "outputs": [],
   "source": [
    "class DeepNovoPointNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNovoPointNet, self).__init__()\n",
    "        self.t_net = TNet(with_lstm=False)\n",
    "        self.distance_scale_factor = distance_scale_factor\n",
    "\n",
    "    def forward(self, location_index, peaks_location, peaks_intensity):\n",
    "        \"\"\"\n",
    "\n",
    "        :param location_index: [batch, T, 26, 8] long\n",
    "        :param peaks_location: [batch, N] N stands for MAX_NUM_PEAK, long\n",
    "        :param peaks_intensity: [batch, N], float32\n",
    "        :return:\n",
    "            logits: [batch, T, 26]\n",
    "        \"\"\"\n",
    "\n",
    "        N = peaks_location.size(1)\n",
    "        assert N == peaks_intensity.size(1)\n",
    "        batch_size, T, vocab_size, num_ion = location_index.size()\n",
    "\n",
    "        peaks_location = peaks_location.view(batch_size, 1, N, 1)\n",
    "        peaks_intensity = peaks_intensity.view(batch_size, 1, N, 1)\n",
    "        peaks_location = peaks_location.expand(-1, T, -1, -1)  # [batch, T, N, 1]\n",
    "        peaks_location_mask = (peaks_location > 1e-5).float()\n",
    "        peaks_intensity = peaks_intensity.expand(-1, T, -1, -1)  # [batch, T, N, 1]\n",
    "\n",
    "        location_index = location_index.view(batch_size, T, 1, vocab_size*num_ion)\n",
    "        location_index_mask = (location_index > 1e-5).float()\n",
    "\n",
    "        location_exp_minus_abs_diff = torch.exp(\n",
    "            -torch.abs(\n",
    "                (peaks_location - location_index) * self.distance_scale_factor\n",
    "            )\n",
    "        )\n",
    "        # [batch, T, N, 26*8]\n",
    "\n",
    "        location_exp_minus_abs_diff = location_exp_minus_abs_diff * peaks_location_mask * location_index_mask\n",
    "\n",
    "        input_feature = torch.cat((location_exp_minus_abs_diff, peaks_intensity), dim=3)\n",
    "        input_feature = input_feature.view(batch_size*T, N, vocab_size*num_ion + 1)\n",
    "        input_feature = input_feature.transpose(1, 2)\n",
    "\n",
    "        result = self.t_net(input_feature).view(batch_size, T, vocab_size)\n",
    "        return result\n",
    "\n",
    "\n",
    "if use_lstm: #Global Variable\n",
    "    DeepNovoModel = DeepNovoPointNetWithLSTM\n",
    "else:\n",
    "    DeepNovoModel = DeepNovoPointNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZupAkjDF1qaL"
   },
   "source": [
    "## Inference Model Wrapper <a name=\"inference\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7zEsNBW1qaM"
   },
   "outputs": [],
   "source": [
    "class InferenceModelWrapper(object):\n",
    "    \"\"\"\n",
    "    a wrapper class so that the beam search part of code is the same for both with lstm and without lstm model.\n",
    "    \"\"\"\n",
    "    def __init__(self, forward_model: DeepNovoModel, backward_model: DeepNovoModel, init_net: InitNet=None):\n",
    "        self.forward_model = forward_model\n",
    "        self.backward_model = backward_model\n",
    "        # make sure all models are in eval mode\n",
    "        self.forward_model.eval()\n",
    "        self.backward_model.eval()\n",
    "        if use_lstm:\n",
    "            assert init_net is not None\n",
    "            self.init_net = init_net\n",
    "            self.init_net.eval()\n",
    "\n",
    "    def step(self, candidate_location, peaks_location, peaks_intensity, aa_input, state_tuple, direction):\n",
    "        \"\"\"\n",
    "        :param state_tuple: tuple of ([num_layer, batch_size, num_unit], [num_layer, batch_size, num_unit])\n",
    "        :param aa_input: [batch, 1]\n",
    "        :param candidate_location: [batch, 1, 26, 8]\n",
    "        :param peaks_location: [batch, N]\n",
    "        :param peaks_intensity: [batch, N]\n",
    "        :param direction: enum class, whether forward or backward\n",
    "        :return: (log_prob, new_hidden_state)\n",
    "        log_prob: the pred log prob of shape [batch, 26]\n",
    "        \"\"\"\n",
    "        if direction == Direction.forward:\n",
    "            model = self.forward_model\n",
    "        else:\n",
    "            model = self.backward_model\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if use_lstm:\n",
    "                logit, new_state_tuple = model(candidate_location, peaks_location, peaks_intensity, aa_input,\n",
    "                                               state_tuple)\n",
    "            else:\n",
    "                logit = model(candidate_location, peaks_location, peaks_intensity)\n",
    "                new_state_tuple = None\n",
    "            logit = torch.squeeze(logit, dim=1)\n",
    "            log_prob = F.log_softmax(logit)\n",
    "        return log_prob, new_state_tuple\n",
    "\n",
    "    def initial_hidden_state(self, spectrum_representation):\n",
    "        \"\"\"\n",
    "\n",
    "        :param: spectrum_representation, [batch, embedding_size]\n",
    "        :return:\n",
    "            [num_lstm_layers, batch_size, lstm_units], [num_lstm_layers, batch_size, lstm_units],\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            h_0, c_0 = self.init_net(spectrum_representation)\n",
    "            return h_0.to(device), c_0.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "co7njpUO6tZW"
   },
   "source": [
    "## Build Model  <a name=\"build_func\"></a>\n",
    "\n",
    "The `build_model` function uses the previously defined classes and the model parameters to create the final model (with or without LSTM, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "axr-ZTZY1qaV"
   },
   "outputs": [],
   "source": [
    "def build_model(training=True):\n",
    "    \"\"\"\n",
    "    :Parameters:\n",
    "        training to put the model in training of prediction mode\n",
    "    :return:\n",
    "        forward_deepnovo, backward_deepnovo, init_net : in device, and with the \n",
    "        pretrained parameters if present.\n",
    "    \"\"\"\n",
    "    forward_deepnovo = DeepNovoModel()\n",
    "    backward_deepnovo = DeepNovoModel()\n",
    "    if use_lstm:\n",
    "        init_net = InitNet() #initialisation of the LSTM\n",
    "    else:\n",
    "        init_net = None\n",
    "\n",
    "    # load pretrained params if exist\n",
    "    if os.path.exists(os.path.join(train_dir, forward_model_save_name)):\n",
    "        assert os.path.exists(os.path.join(train_dir, backward_model_save_name))\n",
    "        print(\"load pretrained model\")\n",
    "        forward_deepnovo.load_state_dict(torch.load(os.path.join(train_dir, forward_model_save_name),\n",
    "                                                    map_location=device))\n",
    "        backward_deepnovo.load_state_dict(torch.load(os.path.join(train_dir, backward_model_save_name),\n",
    "                                                     map_location=device))\n",
    "        if use_lstm:\n",
    "            init_net.load_state_dict(torch.load(os.path.join(train_dir, init_net_save_name),\n",
    "                                                map_location=device))\n",
    "    else:\n",
    "        assert training, f\"building model for testing, but could not found weight under directory \" \\\n",
    "                         f\"{train_dir}\"\n",
    "        print(\"initialize a set of new parameters\")\n",
    "\n",
    "    if use_lstm:\n",
    "        # share embedding matrix\n",
    "        backward_deepnovo.embedding.weight = forward_deepnovo.embedding.weight\n",
    "\n",
    "    backward_deepnovo = backward_deepnovo.to(device)\n",
    "    forward_deepnovo = forward_deepnovo.to(device)\n",
    "    if use_lstm:\n",
    "        init_net = init_net.to(device)\n",
    "    return forward_deepnovo, backward_deepnovo, init_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43SlkAYR1qaY"
   },
   "source": [
    "## Data Reader <a name=\"reader\"></a>\n",
    "\n",
    "The data reader module is used to parse the input files and generate the Pytorch dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW6_c6VdvTOB"
   },
   "source": [
    "### Raw Sequence Parser\n",
    "\n",
    "The original raw sequence parser of DeepNovoV2 contained hardcoded PTMs and was usable only with the sequence format from PEAKS. It has been bypassed, since our Python module ensures that the sequences in the features file will contain the same vocabulary as defined in the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xq-DeA_O1qaZ"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------Data_reader------------------------------------------\n",
    "def parse_raw_sequence(raw_sequence: str):\n",
    "    return True, re.findall(r'[A-Z](?:\\(.+?\\))?', raw_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wUhLyNTvpaY"
   },
   "source": [
    "### Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7EgNwEqvQyc"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------Class Definition------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class DDAFeature:\n",
    "    feature_id: str\n",
    "    mz: float\n",
    "    z: float\n",
    "    rt_mean: float\n",
    "    peptide: list\n",
    "    scan: str\n",
    "    mass: float\n",
    "    feature_area: str\n",
    "\n",
    "@dataclass\n",
    "class DenovoData:\n",
    "    peak_location: np.ndarray\n",
    "    peak_intensity: np.ndarray\n",
    "    spectrum_representation: np.ndarray\n",
    "    original_dda_feature: DDAFeature\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainData:\n",
    "    peak_location: np.ndarray\n",
    "    peak_intensity: np.ndarray\n",
    "    spectrum_representation: np.ndarray\n",
    "    forward_id_target: list\n",
    "    backward_id_target: list\n",
    "    forward_ion_location_index_list: list\n",
    "    backward_ion_location_index_list: list\n",
    "    forward_id_input: list\n",
    "    backward_id_input: list\n",
    "\n",
    "\n",
    "class DeepNovoTrainDataset(Dataset):\n",
    "    def __init__(self, feature_filename, spectrum_filename, transform=None):\n",
    "        \"\"\"\n",
    "        read all feature information and store in memory,\n",
    "        :param feature_filename:\n",
    "        :param spectrum_filename:\n",
    "        \"\"\"\n",
    "        print(f\"input spectrum file: {spectrum_filename}\")\n",
    "        print(f\"input feature file: {feature_filename}\")\n",
    "        self.spectrum_filename = spectrum_filename\n",
    "        self.input_spectrum_handle = None\n",
    "        self.feature_list = []\n",
    "        self.spectrum_location_dict = {}\n",
    "        self.transform = transform\n",
    "        # read spectrum location file\n",
    "        spectrum_location_file = spectrum_filename + '.location.pytorch.pkl'\n",
    "        if os.path.exists(spectrum_location_file):\n",
    "            print(\"read cached spectrum locations\")\n",
    "            with open(spectrum_location_file, 'rb') as fr:\n",
    "                self.spectrum_location_dict = pickle.load(fr)\n",
    "        else:\n",
    "            print(\"build spectrum location from scratch\")\n",
    "            spectrum_location_dict = {}\n",
    "            line = True\n",
    "            with open(spectrum_filename, 'r') as f:\n",
    "                while line:\n",
    "                    current_location = f.tell()\n",
    "                    line = f.readline()\n",
    "                    if \"BEGIN IONS\" in line:\n",
    "                        spectrum_location = current_location\n",
    "                    elif \"SCANS=\" in line:\n",
    "                        scan = re.split('[=\\r\\n]', line)[1]\n",
    "                        spectrum_location_dict[scan] = spectrum_location\n",
    "            self.spectrum_location_dict = spectrum_location_dict\n",
    "            with open(spectrum_location_file, 'wb') as fw:\n",
    "                pickle.dump(self.spectrum_location_dict, fw)\n",
    "\n",
    "        # read feature file\n",
    "        skipped_by_mass = 0\n",
    "        skipped_by_ptm = 0\n",
    "        skipped_by_length = 0\n",
    "        with open(feature_filename, 'r') as fr:\n",
    "            reader = csv.reader(fr, delimiter=',')\n",
    "            header = next(reader)\n",
    "            feature_id_index = header.index(col_feature_id)\n",
    "            mz_index = header.index(col_precursor_mz)\n",
    "            z_index = header.index(col_precursor_charge)\n",
    "            rt_mean_index = header.index(col_rt_mean)\n",
    "            seq_index = header.index(col_raw_sequence)\n",
    "            scan_index = header.index(col_scan_list)\n",
    "            feature_area_index = header.index(col_feature_area)\n",
    "            for line in reader:\n",
    "                mass = (float(line[mz_index]) - mass_H) * float(line[z_index])\n",
    "                ok, peptide = parse_raw_sequence(line[seq_index])\n",
    "                if not ok:\n",
    "                    skipped_by_ptm += 1\n",
    "                    print(f\"{line[seq_index]} skipped by ptm\")\n",
    "                    continue\n",
    "                if mass > MZ_MAX:\n",
    "                    skipped_by_mass += 1\n",
    "                    continue\n",
    "                if len(peptide) >= MAX_LEN:\n",
    "                    skipped_by_length += 1\n",
    "                    continue\n",
    "                new_feature = DDAFeature(feature_id=line[feature_id_index],\n",
    "                                         mz=float(line[mz_index]),\n",
    "                                         z=float(line[z_index]),\n",
    "                                         rt_mean=float(line[rt_mean_index]),\n",
    "                                         peptide=peptide,\n",
    "                                         scan=line[scan_index],\n",
    "                                         mass=mass,\n",
    "                                         feature_area=line[feature_area_index])\n",
    "                self.feature_list.append(new_feature)\n",
    "        print(f\"read {len(self.feature_list)} features, {skipped_by_mass} skipped by mass, \"\n",
    "                    f\"{skipped_by_ptm} skipped by unknown modification, {skipped_by_length} skipped by length\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_list)\n",
    "\n",
    "    def close(self):\n",
    "        self.input_spectrum_handle.close()\n",
    "\n",
    "    def _parse_spectrum_ion(self):\n",
    "        mz_list = []\n",
    "        intensity_list = []\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        while not \"END IONS\" in line:\n",
    "            mz, intensity = re.split(' |\\r|\\n', line)[:2]\n",
    "            mz_float = float(mz)\n",
    "            intensity_float = float(intensity)\n",
    "            # skip an ion if its mass > MZ_MAX\n",
    "            if mz_float > MZ_MAX:\n",
    "                line = self.input_spectrum_handle.readline()\n",
    "                continue\n",
    "            mz_list.append(mz_float)\n",
    "            intensity_list.append(intensity_float)\n",
    "            line = self.input_spectrum_handle.readline()\n",
    "        return mz_list, intensity_list\n",
    "\n",
    "    def _get_feature(self, feature: DDAFeature) -> TrainData:\n",
    "        spectrum_location = self.spectrum_location_dict[feature.scan]\n",
    "        self.input_spectrum_handle.seek(spectrum_location)\n",
    "        # parse header lines\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"BEGIN IONS\" in line, \"Error: wrong input BEGIN IONS\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"TITLE=\" in line, \"Error: wrong input TITLE=\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"PEPMASS=\" in line, \"Error: wrong input PEPMASS=\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"CHARGE=\" in line, \"Error: wrong input CHARGE=\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"SCANS=\" in line, \"Error: wrong input SCANS=\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"RTINSECONDS=\" in line, \"Error: wrong input RTINSECONDS=\"\n",
    "        mz_list, intensity_list = self._parse_spectrum_ion()\n",
    "        peak_location, peak_intensity, spectrum_representation = process_peaks(mz_list, intensity_list, feature.mass)\n",
    "\n",
    "        assert np.max(peak_intensity) < 1.0 + 1e-5\n",
    "        # no id needed for the denovo (Denovo part)\n",
    "        peptide_id_list = [vocab[x] for x in feature.peptide]\n",
    "        forward_id_input = [GO_ID] + peptide_id_list\n",
    "        forward_id_target = peptide_id_list + [EOS_ID]\n",
    "        forward_ion_location_index_list = []\n",
    "        prefix_mass = 0.\n",
    "        for i, id in enumerate(forward_id_input):\n",
    "            prefix_mass += mass_ID[id]\n",
    "            ion_location = get_ion_index(feature.mass, prefix_mass, 0)\n",
    "            forward_ion_location_index_list.append(ion_location)\n",
    "\n",
    "        backward_id_input = [EOS_ID] + peptide_id_list[::-1]\n",
    "        backward_id_target = peptide_id_list[::-1] + [GO_ID]\n",
    "        backward_ion_location_index_list = []\n",
    "        suffix_mass = 0\n",
    "        for i, id in enumerate(backward_id_input):\n",
    "            suffix_mass += mass_ID[id]\n",
    "            ion_location = get_ion_index(feature.mass, suffix_mass, 1)\n",
    "            backward_ion_location_index_list.append(ion_location)\n",
    "\n",
    "        return TrainData(peak_location=peak_location,\n",
    "                         peak_intensity=peak_intensity,\n",
    "                         spectrum_representation=spectrum_representation,\n",
    "                         forward_id_target=forward_id_target,\n",
    "                         backward_id_target=backward_id_target,\n",
    "                         forward_ion_location_index_list=forward_ion_location_index_list,\n",
    "                         backward_ion_location_index_list=backward_ion_location_index_list,\n",
    "                         forward_id_input=forward_id_input,\n",
    "                         backward_id_input=backward_id_input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.input_spectrum_handle is None:\n",
    "            self.input_spectrum_handle = open(self.spectrum_filename, 'r')\n",
    "        feature = self.feature_list[idx]\n",
    "        return self._get_feature(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZM4nJeK1qab"
   },
   "source": [
    "## Collate Function <a name=\"collate\"></a>\n",
    "Allows for the creation of a map-style for the dataset `TrainData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9CVExmx1qac"
   },
   "outputs": [],
   "source": [
    "def collate_func(train_data_list):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_data_list: list of TrainData\n",
    "    :return:\n",
    "        peak_location: [batch, N]\n",
    "        peak_intensity: [batch, N]\n",
    "        forward_target_id: [batch, T]\n",
    "        backward_target_id: [batch, T]\n",
    "        forward_ion_index_list: [batch, T, 26, 8]\n",
    "        backward_ion_index_list: [batch, T, 26, 8]\n",
    "    \"\"\"\n",
    "    # sort data by seq length (decreasing order)\n",
    "    train_data_list.sort(key=lambda x: len(x.forward_id_target), reverse=True)\n",
    "    batch_max_seq_len = len(train_data_list[0].forward_id_target)\n",
    "    ion_index_shape = train_data_list[0].forward_ion_location_index_list[0].shape\n",
    "    assert ion_index_shape == (vocab_size, num_ion)\n",
    "\n",
    "    peak_location = [x.peak_location for x in train_data_list]\n",
    "    peak_location = np.stack(peak_location) # [batch_size, N]\n",
    "    peak_location = torch.from_numpy(peak_location)\n",
    "\n",
    "    peak_intensity = [x.peak_intensity for x in train_data_list]\n",
    "    peak_intensity = np.stack(peak_intensity) # [batch_size, N]\n",
    "    peak_intensity = torch.from_numpy(peak_intensity)\n",
    "\n",
    "    spectrum_representation = [x.spectrum_representation for x in train_data_list]\n",
    "    spectrum_representation = np.stack(spectrum_representation)  # [batch_size, embed_size]\n",
    "    spectrum_representation = torch.from_numpy(spectrum_representation)\n",
    "\n",
    "    batch_forward_ion_index = []\n",
    "    batch_forward_id_target = []\n",
    "    batch_forward_id_input = []\n",
    "    for data in train_data_list:\n",
    "        ion_index = np.zeros((batch_max_seq_len, ion_index_shape[0], ion_index_shape[1]),\n",
    "                               np.float32)\n",
    "        forward_ion_index = np.stack(data.forward_ion_location_index_list)\n",
    "        ion_index[:forward_ion_index.shape[0], :, :] = forward_ion_index\n",
    "        batch_forward_ion_index.append(ion_index)\n",
    "\n",
    "        f_target = np.zeros((batch_max_seq_len,), np.int64)\n",
    "        forward_target = np.array(data.forward_id_target, np.int64)\n",
    "        f_target[:forward_target.shape[0]] = forward_target\n",
    "        batch_forward_id_target.append(f_target)\n",
    "\n",
    "        f_input = np.zeros((batch_max_seq_len,), np.int64)\n",
    "        forward_input = np.array(data.forward_id_input, np.int64)\n",
    "        f_input[:forward_input.shape[0]] = forward_input\n",
    "        batch_forward_id_input.append(f_input)\n",
    "\n",
    "\n",
    "\n",
    "    batch_forward_id_target = torch.from_numpy(np.stack(batch_forward_id_target))  # [batch_size, T]\n",
    "    batch_forward_ion_index = torch.from_numpy(np.stack(batch_forward_ion_index))  # [batch, T, 26, 8]\n",
    "    batch_forward_id_input = torch.from_numpy(np.stack(batch_forward_id_input))\n",
    "\n",
    "    batch_backward_ion_index = []\n",
    "    batch_backward_id_target = []\n",
    "    batch_backward_id_input = []\n",
    "    for data in train_data_list:\n",
    "        ion_index = np.zeros((batch_max_seq_len, ion_index_shape[0], ion_index_shape[1]),\n",
    "                             np.float32)\n",
    "        backward_ion_index = np.stack(data.backward_ion_location_index_list)\n",
    "        ion_index[:backward_ion_index.shape[0], :, :] = backward_ion_index\n",
    "        batch_backward_ion_index.append(ion_index)\n",
    "\n",
    "        b_target = np.zeros((batch_max_seq_len,), np.int64)\n",
    "        backward_target = np.array(data.backward_id_target, np.int64)\n",
    "        b_target[:backward_target.shape[0]] = backward_target\n",
    "        batch_backward_id_target.append(b_target)\n",
    "\n",
    "        b_input = np.zeros((batch_max_seq_len,), np.int64)\n",
    "        backward_input = np.array(data.backward_id_input, np.int64)\n",
    "        b_input[:backward_input.shape[0]] = backward_input\n",
    "        batch_backward_id_input.append(b_input)\n",
    "\n",
    "    batch_backward_id_target = torch.from_numpy(np.stack(batch_backward_id_target))  # [batch_size, T]\n",
    "    batch_backward_ion_index = torch.from_numpy(np.stack(batch_backward_ion_index))  # [batch, T, 26, 8]\n",
    "    batch_backward_id_input = torch.from_numpy(np.stack(batch_backward_id_input))\n",
    "\n",
    "    return (peak_location,\n",
    "            peak_intensity,\n",
    "            spectrum_representation,\n",
    "            batch_forward_id_target,\n",
    "            batch_backward_id_target,\n",
    "            batch_forward_ion_index,\n",
    "            batch_backward_ion_index,\n",
    "            batch_forward_id_input,\n",
    "            batch_backward_id_input\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHe2X1lY1qbV"
   },
   "source": [
    "## Extract & Move Data <a name=\"extract\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krkvoHis1qbV"
   },
   "outputs": [],
   "source": [
    "def extract_and_move_data(data):\n",
    "    \"\"\"\n",
    "    This function extract the data from the dataclass pytorch and put it in the device\n",
    "    used (CPU/RAM or GPU). This speed-up the process.\n",
    "    :param data: result from dataloader\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    peak_location, \\\n",
    "    peak_intensity, \\\n",
    "    spectrum_representation,\\\n",
    "    batch_forward_id_target, \\\n",
    "    batch_backward_id_target, \\\n",
    "    batch_forward_ion_index, \\\n",
    "    batch_backward_ion_index, \\\n",
    "    batch_forward_id_input, \\\n",
    "    batch_backward_id_input = data\n",
    "\n",
    "    # move to device\n",
    "    peak_location = peak_location.to(device)\n",
    "    peak_intensity = peak_intensity.to(device)\n",
    "    spectrum_representation = spectrum_representation.to(device)\n",
    "    batch_forward_id_target = batch_forward_id_target.to(device)\n",
    "    batch_backward_id_target = batch_backward_id_target.to(device)\n",
    "    batch_forward_ion_index = batch_forward_ion_index.to(device)\n",
    "    batch_backward_ion_index = batch_backward_ion_index.to(device)\n",
    "    batch_forward_id_input = batch_forward_id_input.to(device)\n",
    "    batch_backward_id_input = batch_backward_id_input.to(device)\n",
    "    return (peak_location,\n",
    "            peak_intensity,\n",
    "            spectrum_representation,\n",
    "            batch_forward_id_target,\n",
    "            batch_backward_id_target,\n",
    "            batch_forward_ion_index,\n",
    "            batch_backward_ion_index,\n",
    "            batch_forward_id_input,\n",
    "            batch_backward_id_input\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oITCBrXS1qbX"
   },
   "source": [
    "## Focal loss function <a name=\"loss\"></a>\n",
    "Focal loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. See [Focal Loss for Dense Object Detection](https://arxiv.org/pdf/1708.02002.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v3HFdpJP1qbX"
   },
   "outputs": [],
   "source": [
    "def focal_loss(logits, labels, ignore_index=-100, gamma=2.):\n",
    "    \"\"\"\n",
    "    :param logits: float tensor of shape [batch, T, 26]\n",
    "    :param labels: long tensor of shape [batch, T]\n",
    "    :param ignore_index: ignore the loss of those tokens\n",
    "    :param gamma:\n",
    "    :return: average loss, num_valid_token\n",
    "    \"\"\"\n",
    "    valid_token_mask = (labels != ignore_index).float()  # [batch, T]\n",
    "    num_valid_token = torch.sum(valid_token_mask)\n",
    "    batch_size, T, num_classes = logits.size()\n",
    "    sigmoid_p = torch.sigmoid(logits) #sigmoid used instead of soft_max\n",
    "    target_tensor = to_one_hot(labels, n_dims=num_classes).float().to(device)\n",
    "    zeros = torch.zeros_like(sigmoid_p)\n",
    "    pos_p_sub = torch.where(target_tensor >= sigmoid_p, target_tensor - sigmoid_p, zeros)  # [batch, T, 26]\n",
    "    neg_p_sub = torch.where(target_tensor > zeros, zeros, sigmoid_p)  # [batch, T, 26]\n",
    "\n",
    "    per_token_loss = - (pos_p_sub ** gamma) * torch.log(torch.clamp(sigmoid_p, 1e-8, 1.0)) - \\\n",
    "                     (neg_p_sub ** gamma) * torch.log(torch.clamp(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    per_entry_loss = torch.sum(per_token_loss, dim=2)  # [batch, T]\n",
    "    per_entry_loss = per_entry_loss * valid_token_mask  # masking out loss from pad tokens\n",
    "\n",
    "    per_entry_average_loss = torch.sum(per_entry_loss) / (num_valid_token + 1e-6)\n",
    "    return per_entry_average_loss, num_valid_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSTTJNqZ1qbZ"
   },
   "source": [
    "### to_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nyit6H_n1qbZ"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take integer y with n dims and convert it to 1-hot representation with n+1 dims. \n",
    "    c.f. Report 2.5\n",
    "    \"\"\"\n",
    "    y_tensor = y.data\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iS1EXktjNPzz"
   },
   "source": [
    "## Validation Function <a name=\"valid_func\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hkpv8RC4NOxr"
   },
   "outputs": [],
   "source": [
    "def validation(forward_deepnovo, backward_deepnovo, init_net, valid_loader) -> float:\n",
    "    \"\"\" Compute and return the loss of the model trained on Validation dataset (valid_loader) \"\"\"\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        num_valid_samples = 0\n",
    "        for data in valid_loader:\n",
    "            peak_location, \\\n",
    "            peak_intensity, \\\n",
    "            spectrum_representation, \\\n",
    "            batch_forward_id_target, \\\n",
    "            batch_backward_id_target, \\\n",
    "            batch_forward_ion_index, \\\n",
    "            batch_backward_ion_index, \\\n",
    "            batch_forward_id_input, \\\n",
    "            batch_backward_id_input = extract_and_move_data(data)\n",
    "            batch_size = batch_backward_id_target.size(0)\n",
    "            #evaluate the validation data through the model\n",
    "            if use_lstm:\n",
    "                initial_state_tuple = init_net(spectrum_representation)\n",
    "                forward_logit, _ = forward_deepnovo(batch_forward_ion_index, peak_location, peak_intensity,\n",
    "                                                    batch_forward_id_input, initial_state_tuple)\n",
    "                backward_logit, _ = backward_deepnovo(batch_backward_ion_index, peak_location, peak_intensity,\n",
    "                                                      batch_backward_id_input, initial_state_tuple)\n",
    "            else:\n",
    "                forward_logit = forward_deepnovo(batch_forward_ion_index, peak_location, peak_intensity)\n",
    "                backward_logit = backward_deepnovo(batch_backward_ion_index, peak_location, peak_intensity)\n",
    "            #compute the loss\n",
    "            forward_loss, f_num = focal_loss(forward_logit, batch_forward_id_target, ignore_index=0, gamma=2.)\n",
    "            backward_loss, b_num = focal_loss(backward_logit, batch_backward_id_target, ignore_index=0, gamma=2.)\n",
    "            valid_loss += forward_loss.item() * f_num.item() + backward_loss.item() * b_num.item()\n",
    "            num_valid_samples += f_num.item() + b_num.item()\n",
    "    #average the loss on all validation samples        \n",
    "    average_valid_loss = valid_loss / (num_valid_samples + 1e-6)\n",
    "    return float(average_valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wt5_Be2GwAM6"
   },
   "source": [
    "## Cython Integration <a name=\"cython\"></a>\n",
    "\n",
    "The following functions were initially designed to be build with Cython. Since Cython isn't realiable when working with Jupyter notebooks, they have been redesigned to work in plain Python. This impacts perfomance, but shouldn't prevent the use of this notebook on moderately sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZio9RBF1qbl"
   },
   "outputs": [],
   "source": [
    "#---------------------------------cython------------------------------------\n",
    "def get_sinusoid_encoding_table(n_position, embed_size, padding_idx=0):\n",
    "    \"\"\" Sinusoid position encoding table\n",
    "    n_position: maximum integer that the embedding op could receive\n",
    "    embed_size: embed size\n",
    "    return\n",
    "      a embedding matrix of shape [n_position, embed_size]\n",
    "    \"\"\"\n",
    "\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(sinusoid_base, 2 * (hid_idx // 2) / embed_size)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(embed_size)]\n",
    "\n",
    "    sinusoid_matrix = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position + 1)], dtype=np.float32)\n",
    "\n",
    "    sinusoid_matrix[:, 0::2] = np.sin(sinusoid_matrix[:, 0::2])  # dim 2i\n",
    "    sinusoid_matrix[:, 1::2] = np.cos(sinusoid_matrix[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    sinusoid_matrix[padding_idx] = 0.\n",
    "    return sinusoid_matrix\n",
    "\n",
    "sinusoid_matrix = get_sinusoid_encoding_table(n_position, embedding_size,\n",
    "                                              padding_idx=PAD_ID)\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking\n",
    "@cython.wraparound(False) # turn off negative index wrapping\n",
    "def get_ion_index(peptide_mass, prefix_mass, direction):\n",
    "  \"\"\"\n",
    "  :param peptide_mass: neutral mass of a peptide\n",
    "  :param prefix_mass:\n",
    "  :param direction: 0 for forward, 1 for backward\n",
    "  :return: an int32 ndarray of shape [26, 8], each element represent a index of the spectrum embbeding matrix. for out\n",
    "  of bound position, the index is 0\n",
    "  \"\"\"\n",
    "  if direction == 0:\n",
    "    candidate_b_mass = prefix_mass + mass_ID_np\n",
    "    candidate_y_mass = peptide_mass - candidate_b_mass\n",
    "  elif direction == 1:\n",
    "    candidate_y_mass = prefix_mass + mass_ID_np\n",
    "    candidate_b_mass = peptide_mass - candidate_y_mass\n",
    "  candidate_a_mass = candidate_b_mass - mass_CO\n",
    "\n",
    "  # b-ions\n",
    "  candidate_b_H2O = candidate_b_mass - mass_H2O\n",
    "  candidate_b_NH3 = candidate_b_mass - mass_NH3\n",
    "  candidate_b_plus2_charge1 = ((candidate_b_mass + 2 * mass_H) / 2\n",
    "                               - mass_H)\n",
    "\n",
    "  # a-ions\n",
    "  candidate_a_H2O = candidate_a_mass - mass_H2O\n",
    "  candidate_a_NH3 = candidate_a_mass - mass_NH3\n",
    "  candidate_a_plus2_charge1 = ((candidate_a_mass + 2 * mass_H) / 2\n",
    "                               - mass_H)\n",
    "\n",
    "  # y-ions\n",
    "  candidate_y_H2O = candidate_y_mass - mass_H2O\n",
    "  candidate_y_NH3 = candidate_y_mass - mass_NH3\n",
    "  candidate_y_plus2_charge1 = ((candidate_y_mass + 2 * mass_H) / 2\n",
    "                               - mass_H)\n",
    "\n",
    "  # ion_8\n",
    "  b_ions = [candidate_b_mass,\n",
    "            candidate_b_H2O,\n",
    "            candidate_b_NH3,\n",
    "            candidate_b_plus2_charge1]\n",
    "  y_ions = [candidate_y_mass,\n",
    "            candidate_y_H2O,\n",
    "            candidate_y_NH3,\n",
    "            candidate_y_plus2_charge1]\n",
    "  a_ions = [candidate_a_mass,\n",
    "            candidate_a_H2O,\n",
    "            candidate_a_NH3,\n",
    "            candidate_a_plus2_charge1]\n",
    "  ion_mass_list = b_ions + y_ions + a_ions\n",
    "  ion_mass = np.array(ion_mass_list, dtype=np.float32)  # 8 by 26\n",
    "\n",
    "  # ion locations\n",
    "  in_bound_mask = np.logical_and(\n",
    "      ion_mass > 0,\n",
    "      ion_mass <= MZ_MAX).astype(np.float32)\n",
    "  ion_location = ion_mass * in_bound_mask  # 8 by 26, out of bound index would have value 0\n",
    "  return ion_location.transpose()  # 26 by 8\n",
    "\n",
    "\n",
    "def pad_to_length(data: list, length, pad_token=0.):\n",
    "  \"\"\"\n",
    "  pad data to length if len(data) is smaller than length\n",
    "  :param data:\n",
    "  :param length:\n",
    "  :param pad_token:\n",
    "  :return:\n",
    "  \"\"\"\n",
    "  for i in range(length - len(data)):\n",
    "    data.append(pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9WMY5gGL1qbn"
   },
   "outputs": [],
   "source": [
    "def process_peaks(spectrum_mz_list, spectrum_intensity_list, peptide_mass):\n",
    "  \"\"\"\n",
    "\n",
    "  :param spectrum_mz_list:\n",
    "  :param spectrum_intensity_list:\n",
    "  :param peptide_mass: peptide neutral mass\n",
    "  :return:\n",
    "    peak_location: int64, [N]\n",
    "    peak_intensity: float32, [N]\n",
    "    spectrum_representation: float32 [embedding_size]\n",
    "  \"\"\"\n",
    "  charge = 1.0\n",
    "  spectrum_intensity_max = np.max(spectrum_intensity_list)\n",
    "  # charge 1 peptide location\n",
    "  spectrum_mz_list.append(peptide_mass + charge*mass_H)\n",
    "  spectrum_intensity_list.append(spectrum_intensity_max)\n",
    "\n",
    "  # N-terminal, b-ion, peptide_mass_C\n",
    "  # append N-terminal\n",
    "  mass_N = mass_N_terminus - mass_H\n",
    "  spectrum_mz_list.append(mass_N + charge*mass_H)\n",
    "  spectrum_intensity_list.append(spectrum_intensity_max)\n",
    "  # append peptide_mass_C\n",
    "  mass_C = mass_C_terminus + mass_H\n",
    "  peptide_mass_C = peptide_mass - mass_C\n",
    "  spectrum_mz_list.append(peptide_mass_C + charge*mass_H)\n",
    "  spectrum_intensity_list.append(spectrum_intensity_max)\n",
    "\n",
    "  # C-terminal, y-ion, peptide_mass_N\n",
    "  # append C-terminal\n",
    "  mass_C = mass_C_terminus + mass_H\n",
    "  spectrum_mz_list.append(mass_C + charge*mass_H)\n",
    "  spectrum_intensity_list.append(spectrum_intensity_max)\n",
    "\n",
    "\n",
    "  pad_to_length(spectrum_mz_list, MAX_NUM_PEAK)\n",
    "  pad_to_length(spectrum_intensity_list, MAX_NUM_PEAK)\n",
    "\n",
    "  spectrum_mz = np.array(spectrum_mz_list, dtype=np.float32)\n",
    "  spectrum_mz_location = np.ceil(spectrum_mz * spectrum_reso).astype(np.int32)\n",
    "\n",
    "  neutral_mass = spectrum_mz - charge*mass_H\n",
    "  in_bound_mask = np.logical_and(neutral_mass > 0., neutral_mass < MZ_MAX)\n",
    "  neutral_mass[~in_bound_mask] = 0.\n",
    "  # intensity\n",
    "  spectrum_intensity = np.array(spectrum_intensity_list, dtype=np.float32)\n",
    "  norm_intensity = spectrum_intensity / spectrum_intensity_max\n",
    "\n",
    "  spectrum_representation = np.zeros(embedding_size, dtype=np.float32)\n",
    "  for i, loc in enumerate(spectrum_mz_location):\n",
    "    if loc < 0.5 or loc > n_position:\n",
    "      continue\n",
    "    else:\n",
    "      spectrum_representation += sinusoid_matrix[loc] * norm_intensity[i]\n",
    "\n",
    "  top_N_indices = np.argpartition(norm_intensity, -MAX_NUM_PEAK)[-MAX_NUM_PEAK:]\n",
    "  intensity = norm_intensity[top_N_indices]\n",
    "  mass_location = neutral_mass[top_N_indices]\n",
    "\n",
    "  return mass_location, intensity, spectrum_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0a02qmu1qbh"
   },
   "source": [
    "## Training <a name=\"train\"></a>\n",
    "This cell performs the training using the `train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "55D1rlwA1qbh",
    "outputId": "bf9b8840-9fa2-4dc3-f97e-f0146bf3531d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spectrum file: ../smbp_data/spectrum_smbp.mgf\n",
      "input feature file: ../smbp_data/features_smbp.csv.train\n",
      "read cached spectrum locations\n",
      "read 5416 features, 3 skipped by mass, 0 skipped by unknown modification, 1 skipped by length\n",
      "225 steps per epoch\n",
      "input spectrum file: ../smbp_data/spectrum_smbp.mgf\n",
      "input feature file: ../smbp_data/features_smbp.csv.valid\n",
      "read cached spectrum locations\n",
      "read 535 features, 3 skipped by mass, 0 skipped by unknown modification, 0 skipped by length\n",
      "initialize a set of new parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tlr: 0.001\n",
      "epoch 0 step 9/225, train perplexity: 2.793949438907112\tvalidation perplexity: 3.202474555941306\tstep time: 0.3664912939071655\n",
      "best valid loss achieved at epoch 0 step 9\n",
      "epoch 0 step 19/225, train perplexity: 2.3534467970607693\tvalidation perplexity: 3.0489873399027534\tstep time: 0.3393018007278442\n",
      "best valid loss achieved at epoch 0 step 19\n",
      "epoch 0 step 29/225, train perplexity: 2.032802122952982\tvalidation perplexity: 3.010252387204791\tstep time: 0.3487901210784912\n",
      "best valid loss achieved at epoch 0 step 29\n",
      "epoch 0 step 39/225, train perplexity: 1.9092910071362985\tvalidation perplexity: 2.8932988400621875\tstep time: 0.3349908351898193\n",
      "best valid loss achieved at epoch 0 step 39\n",
      "epoch 0 step 49/225, train perplexity: 1.7135304974646226\tvalidation perplexity: 2.845209593261391\tstep time: 0.34039170742034913\n",
      "best valid loss achieved at epoch 0 step 49\n",
      "epoch 0 step 59/225, train perplexity: 1.5732570229532956\tvalidation perplexity: 2.700769023000199\tstep time: 0.3228034734725952\n",
      "best valid loss achieved at epoch 0 step 59\n",
      "epoch 0 step 69/225, train perplexity: 1.5160186174827897\tvalidation perplexity: 2.3586930819382914\tstep time: 0.37254321575164795\n",
      "best valid loss achieved at epoch 0 step 69\n",
      "epoch 0 step 79/225, train perplexity: 1.4440605035414478\tvalidation perplexity: 1.9594488618617398\tstep time: 0.3513820171356201\n",
      "best valid loss achieved at epoch 0 step 79\n",
      "epoch 0 step 89/225, train perplexity: 1.3631771146751974\tvalidation perplexity: 1.6779037976791062\tstep time: 0.34686832427978515\n",
      "best valid loss achieved at epoch 0 step 89\n",
      "epoch 0 step 99/225, train perplexity: 1.4378636057385472\tvalidation perplexity: 1.5223995918686037\tstep time: 0.35007495880126954\n",
      "best valid loss achieved at epoch 0 step 99\n",
      "epoch 0 step 109/225, train perplexity: 1.3810865611863152\tvalidation perplexity: 1.4495740508875525\tstep time: 0.36835446357727053\n",
      "best valid loss achieved at epoch 0 step 109\n",
      "epoch 0 step 119/225, train perplexity: 1.3067055244888972\tvalidation perplexity: 1.4306231425060372\tstep time: 0.3712624549865723\n",
      "best valid loss achieved at epoch 0 step 119\n",
      "epoch 0 step 129/225, train perplexity: 1.2786680405080202\tvalidation perplexity: 1.4018631956695677\tstep time: 0.3668801784515381\n",
      "best valid loss achieved at epoch 0 step 129\n",
      "epoch 0 step 139/225, train perplexity: 1.3142028504645271\tvalidation perplexity: 1.3832577399123542\tstep time: 0.3525912046432495\n",
      "best valid loss achieved at epoch 0 step 139\n",
      "epoch 0 step 149/225, train perplexity: 1.2163906048326163\tvalidation perplexity: 1.3811923566797968\tstep time: 0.3483781576156616\n",
      "best valid loss achieved at epoch 0 step 149\n",
      "epoch 0 step 159/225, train perplexity: 1.278725964881282\tvalidation perplexity: 1.3621679032559673\tstep time: 0.35304057598114014\n",
      "best valid loss achieved at epoch 0 step 159\n",
      "epoch 0 step 169/225, train perplexity: 1.3001394098432963\tvalidation perplexity: 1.3442256825690921\tstep time: 0.34868583679199217\n",
      "best valid loss achieved at epoch 0 step 169\n",
      "epoch 0 step 179/225, train perplexity: 1.265837421569232\tvalidation perplexity: 1.3698456071177543\tstep time: 0.331161642074585\n",
      "epoch 0 step 189/225, train perplexity: 1.19467538666044\tvalidation perplexity: 1.3405451534343615\tstep time: 0.36189067363739014\n",
      "best valid loss achieved at epoch 0 step 189\n",
      "epoch 0 step 199/225, train perplexity: 1.2378948006928676\tvalidation perplexity: 1.3563178369704831\tstep time: 0.35018434524536135\n",
      "epoch 0 step 209/225, train perplexity: 1.2101495031349756\tvalidation perplexity: 1.350873866937774\tstep time: 0.3584691047668457\n",
      "epoch 0 step 219/225, train perplexity: 1.1482940218033317\tvalidation perplexity: 1.3383940810582313\tstep time: 0.33319594860076907\n",
      "best valid loss achieved at epoch 0 step 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [02:22<09:31, 142.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\tlr: 0.001\n",
      "epoch 1 step 9/225, train perplexity: 1.172215155559618\tvalidation perplexity: 1.3334662389133904\tstep time: 0.5906629800796509\n",
      "best valid loss achieved at epoch 1 step 9\n",
      "epoch 1 step 19/225, train perplexity: 1.1772523921050537\tvalidation perplexity: 1.3518329869320636\tstep time: 0.34897186756134035\n",
      "epoch 1 step 29/225, train perplexity: 1.1558845733010679\tvalidation perplexity: 1.3523456532605407\tstep time: 0.35559179782867434\n",
      "epoch 1 step 39/225, train perplexity: 1.1241122589462436\tvalidation perplexity: 1.3650543767004955\tstep time: 0.3615119218826294\n",
      "epoch 1 step 49/225, train perplexity: 1.111399195906675\tvalidation perplexity: 1.341771420536697\tstep time: 0.33811635971069337\n",
      "epoch 1 step 59/225, train perplexity: 1.120864433144194\tvalidation perplexity: 1.343696545088079\tstep time: 0.3493918180465698\n",
      "epoch 1 step 69/225, train perplexity: 1.1057151261201954\tvalidation perplexity: 1.3465057765158925\tstep time: 0.3385584592819214\n",
      "epoch 1 step 79/225, train perplexity: 1.1542348119852142\tvalidation perplexity: 1.349954766775059\tstep time: 0.3690549373626709\n",
      "epoch 1 step 89/225, train perplexity: 1.1380856116437321\tvalidation perplexity: 1.3306849783185521\tstep time: 0.3488416910171509\n",
      "best valid loss achieved at epoch 1 step 89\n",
      "epoch 1 step 99/225, train perplexity: 1.1106232628914159\tvalidation perplexity: 1.355701438797374\tstep time: 0.3470675706863403\n",
      "epoch 1 step 109/225, train perplexity: 1.155366057099062\tvalidation perplexity: 1.3547490851610307\tstep time: 0.38081448078155516\n",
      "epoch 1 step 119/225, train perplexity: 1.112562296646807\tvalidation perplexity: 1.3536429185691026\tstep time: 0.3647430658340454\n",
      "epoch 1 step 129/225, train perplexity: 1.1148316474354554\tvalidation perplexity: 1.3601108822127832\tstep time: 0.3459005832672119\n",
      "epoch 1 step 139/225, train perplexity: 1.0850498018498342\tvalidation perplexity: 1.3619858522765533\tstep time: 0.34844465255737306\n",
      "epoch 1 step 149/225, train perplexity: 1.0659252068661809\tvalidation perplexity: 1.349289572263138\tstep time: 0.3435567617416382\n",
      "epoch 1 step 159/225, train perplexity: 1.1059568454258704\tvalidation perplexity: 1.3488192085641364\tstep time: 0.35366790294647216\n",
      "epoch 1 step 169/225, train perplexity: 1.1052279098478688\tvalidation perplexity: 1.3511487819539951\tstep time: 0.34320945739746095\n",
      "epoch 1 step 179/225, train perplexity: 1.15170708198858\tvalidation perplexity: 1.358131618329721\tstep time: 0.40200324058532716\n",
      "epoch 1 step 189/225, train perplexity: 1.1104091568392431\tvalidation perplexity: 1.352172968885244\tstep time: 0.3535182237625122\n",
      "Epoch    42: reducing learning rate of group 0 to 5.0000e-04.\n",
      "epoch 1 step 199/225, train perplexity: 1.0770549172344472\tvalidation perplexity: 1.3520283692182609\tstep time: 0.356056809425354\n",
      "epoch 1 step 209/225, train perplexity: 1.0774356193595742\tvalidation perplexity: 1.349675175294526\tstep time: 0.3354686498641968\n",
      "epoch 1 step 219/225, train perplexity: 1.151399738490583\tvalidation perplexity: 1.3627453202149211\tstep time: 0.36849193572998046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [04:46<07:09, 143.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\tlr: 0.0001\n",
      "epoch 2 step 9/225, train perplexity: 1.0980431424647514\tvalidation perplexity: 1.3502115444916425\tstep time: 0.552337908744812\n",
      "epoch 2 step 19/225, train perplexity: 1.0558672985385762\tvalidation perplexity: 1.3519723678809081\tstep time: 0.37793865203857424\n",
      "epoch 2 step 29/225, train perplexity: 1.0924419086348152\tvalidation perplexity: 1.3595432416661584\tstep time: 0.36386895179748535\n",
      "epoch 2 step 39/225, train perplexity: 1.0600182439531747\tvalidation perplexity: 1.3609946956874723\tstep time: 0.33583109378814696\n",
      "epoch 2 step 49/225, train perplexity: 1.1212722827409791\tvalidation perplexity: 1.3585266934074272\tstep time: 0.346697473526001\n",
      "epoch 2 step 59/225, train perplexity: 1.0697108736686187\tvalidation perplexity: 1.3594798895822757\tstep time: 0.357592511177063\n",
      "epoch 2 step 69/225, train perplexity: 1.087192794726473\tvalidation perplexity: 1.361020876805671\tstep time: 0.3531265020370483\n",
      "epoch 2 step 79/225, train perplexity: 1.052450996801551\tvalidation perplexity: 1.355811190617827\tstep time: 0.37010719776153567\n",
      "epoch 2 step 89/225, train perplexity: 1.0746499761861976\tvalidation perplexity: 1.3473570694602401\tstep time: 0.34383411407470704\n",
      "epoch 2 step 99/225, train perplexity: 1.0839099960645362\tvalidation perplexity: 1.3460117578575168\tstep time: 0.36647348403930663\n",
      "epoch 2 step 109/225, train perplexity: 1.0507029383190245\tvalidation perplexity: 1.3501836554868725\tstep time: 0.3444148778915405\n",
      "epoch 2 step 119/225, train perplexity: 1.0699830667289967\tvalidation perplexity: 1.3528669036329866\tstep time: 0.35670790672302244\n",
      "epoch 2 step 129/225, train perplexity: 1.0735426908205623\tvalidation perplexity: 1.3581789354404759\tstep time: 0.3334106683731079\n",
      "epoch 2 step 139/225, train perplexity: 1.1003614149965906\tvalidation perplexity: 1.3616503522740377\tstep time: 0.3762815475463867\n",
      "epoch 2 step 149/225, train perplexity: 1.0678525862388415\tvalidation perplexity: 1.3598501898081403\tstep time: 0.34967019557952883\n",
      "epoch 2 step 159/225, train perplexity: 1.11458272351071\tvalidation perplexity: 1.3615515769969484\tstep time: 0.3427353143692017\n",
      "epoch 2 step 169/225, train perplexity: 1.0410994333987584\tvalidation perplexity: 1.3622556364331282\tstep time: 0.3242560148239136\n",
      "epoch 2 step 179/225, train perplexity: 1.048842638291901\tvalidation perplexity: 1.35780826854815\tstep time: 0.36568686962127683\n",
      "Epoch    63: reducing learning rate of group 0 to 5.0000e-05.\n",
      "epoch 2 step 189/225, train perplexity: 1.0579239561571896\tvalidation perplexity: 1.35989620182433\tstep time: 0.3550976514816284\n",
      "epoch 2 step 199/225, train perplexity: 1.0634170092160162\tvalidation perplexity: 1.3575666682798782\tstep time: 0.3534076452255249\n",
      "epoch 2 step 209/225, train perplexity: 1.0521417628863776\tvalidation perplexity: 1.3596374520023842\tstep time: 0.3567340612411499\n",
      "epoch 2 step 219/225, train perplexity: 1.0281309750369847\tvalidation perplexity: 1.3605536684056687\tstep time: 0.3438273429870605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [07:08<04:45, 142.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\tlr: 0.0001\n",
      "epoch 3 step 9/225, train perplexity: 1.0846711127040323\tvalidation perplexity: 1.3633509942369744\tstep time: 0.5644603729248047\n",
      "epoch 3 step 19/225, train perplexity: 1.0613432399536489\tvalidation perplexity: 1.3640216368589415\tstep time: 0.3451084613800049\n",
      "epoch 3 step 29/225, train perplexity: 1.0549861729861696\tvalidation perplexity: 1.3655636501363124\tstep time: 0.3754934310913086\n",
      "epoch 3 step 39/225, train perplexity: 1.0454554172437502\tvalidation perplexity: 1.3687365954872266\tstep time: 0.3483560562133789\n",
      "epoch 3 step 49/225, train perplexity: 1.0576810326132808\tvalidation perplexity: 1.3671143645738422\tstep time: 0.3267146348953247\n",
      "epoch 3 step 59/225, train perplexity: 1.039424345051049\tvalidation perplexity: 1.3623808595622635\tstep time: 0.3526721954345703\n",
      "epoch 3 step 69/225, train perplexity: 1.075568632611243\tvalidation perplexity: 1.3615443104887683\tstep time: 0.33495924472808836\n",
      "epoch 3 step 79/225, train perplexity: 1.057473051548844\tvalidation perplexity: 1.357575935078754\tstep time: 0.3557881832122803\n",
      "epoch 3 step 89/225, train perplexity: 1.0716037399087168\tvalidation perplexity: 1.363504127348906\tstep time: 0.3229431390762329\n",
      "epoch 3 step 99/225, train perplexity: 1.0673492892341174\tvalidation perplexity: 1.366955058202863\tstep time: 0.3268211126327515\n",
      "epoch 3 step 109/225, train perplexity: 1.0670323547539566\tvalidation perplexity: 1.3646273420912955\tstep time: 0.34050023555755615\n",
      "epoch 3 step 119/225, train perplexity: 1.0422893459413587\tvalidation perplexity: 1.3620619017561202\tstep time: 0.36753790378570556\n",
      "epoch 3 step 129/225, train perplexity: 1.0523868406597454\tvalidation perplexity: 1.3617687848357902\tstep time: 0.35661437511444094\n",
      "epoch 3 step 139/225, train perplexity: 1.0771360337994762\tvalidation perplexity: 1.3602682237305828\tstep time: 0.3520070791244507\n",
      "epoch 3 step 149/225, train perplexity: 1.1000084760555195\tvalidation perplexity: 1.3633064082692383\tstep time: 0.3916593313217163\n",
      "epoch 3 step 159/225, train perplexity: 1.057836168438662\tvalidation perplexity: 1.3622515356835103\tstep time: 0.3684361219406128\n",
      "epoch 3 step 169/225, train perplexity: 1.0465623840326372\tvalidation perplexity: 1.3616565854413067\tstep time: 0.36313936710357664\n",
      "Epoch    84: reducing learning rate of group 0 to 5.0000e-05.\n",
      "epoch 3 step 179/225, train perplexity: 1.0597139951624335\tvalidation perplexity: 1.3609760100480437\tstep time: 0.34980974197387693\n",
      "epoch 3 step 189/225, train perplexity: 1.0726276428249206\tvalidation perplexity: 1.3626139746005292\tstep time: 0.3491859197616577\n",
      "epoch 3 step 199/225, train perplexity: 1.0506451901907738\tvalidation perplexity: 1.36390301388067\tstep time: 0.34498517513275145\n",
      "epoch 3 step 209/225, train perplexity: 1.0794595139132213\tvalidation perplexity: 1.3662290346287527\tstep time: 0.36450369358062745\n",
      "epoch 3 step 219/225, train perplexity: 1.0541318887589897\tvalidation perplexity: 1.367330738129325\tstep time: 0.34762420654296877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [09:30<02:22, 142.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\tlr: 0.0001\n",
      "epoch 4 step 9/225, train perplexity: 1.0632316808938074\tvalidation perplexity: 1.3631925384064778\tstep time: 0.5842698097229004\n",
      "epoch 4 step 19/225, train perplexity: 1.0780149365996794\tvalidation perplexity: 1.3603414359087938\tstep time: 0.36088201999664304\n",
      "epoch 4 step 29/225, train perplexity: 1.0442115174988875\tvalidation perplexity: 1.3531789437816026\tstep time: 0.3615744113922119\n",
      "epoch 4 step 39/225, train perplexity: 1.04607077375233\tvalidation perplexity: 1.347204982760071\tstep time: 0.34669697284698486\n",
      "epoch 4 step 49/225, train perplexity: 1.0576768718031175\tvalidation perplexity: 1.3544894955700817\tstep time: 0.3287489652633667\n",
      "epoch 4 step 59/225, train perplexity: 1.0833791853195465\tvalidation perplexity: 1.358630538979704\tstep time: 0.36172804832458494\n",
      "epoch 4 step 69/225, train perplexity: 1.0466224225858682\tvalidation perplexity: 1.3605016921395763\tstep time: 0.34485957622528074\n",
      "epoch 4 step 79/225, train perplexity: 1.0784034262293736\tvalidation perplexity: 1.3589001188034593\tstep time: 0.36804726123809817\n",
      "epoch 4 step 89/225, train perplexity: 1.0314746634944199\tvalidation perplexity: 1.3621003798864317\tstep time: 0.3622854709625244\n",
      "epoch 4 step 99/225, train perplexity: 1.0755670779718742\tvalidation perplexity: 1.3610891042623439\tstep time: 0.33236606121063234\n",
      "epoch 4 step 109/225, train perplexity: 1.0510022664790921\tvalidation perplexity: 1.354534536285706\tstep time: 0.33656585216522217\n",
      "epoch 4 step 119/225, train perplexity: 1.0376999680330374\tvalidation perplexity: 1.3573574737891156\tstep time: 0.3520066499710083\n",
      "epoch 4 step 129/225, train perplexity: 1.043588554064917\tvalidation perplexity: 1.3633579365165456\tstep time: 0.3411677598953247\n",
      "epoch 4 step 139/225, train perplexity: 1.0809615323129511\tvalidation perplexity: 1.3664919221427136\tstep time: 0.34175705909729004\n",
      "epoch 4 step 149/225, train perplexity: 1.0453919952006852\tvalidation perplexity: 1.3638403665790475\tstep time: 0.3349455833435059\n",
      "epoch 4 step 159/225, train perplexity: 1.0394827543147616\tvalidation perplexity: 1.372646151556735\tstep time: 0.333589768409729\n",
      "Epoch   105: reducing learning rate of group 0 to 5.0000e-05.\n",
      "epoch 4 step 169/225, train perplexity: 1.046561397649682\tvalidation perplexity: 1.374849116067528\tstep time: 0.3450428247451782\n",
      "epoch 4 step 179/225, train perplexity: 1.0654442908246176\tvalidation perplexity: 1.3709051817058893\tstep time: 0.34620554447174073\n",
      "epoch 4 step 189/225, train perplexity: 1.0599292597748542\tvalidation perplexity: 1.3701979139132365\tstep time: 0.36154367923736574\n",
      "epoch 4 step 199/225, train perplexity: 1.0647017029159873\tvalidation perplexity: 1.3690145252978356\tstep time: 0.36349542140960694\n",
      "epoch 4 step 209/225, train perplexity: 1.040603919289911\tvalidation perplexity: 1.3681495260343166\tstep time: 0.34413349628448486\n",
      "epoch 4 step 219/225, train perplexity: 1.0347292102103969\tvalidation perplexity: 1.3648506959888245\tstep time: 0.3534348487854004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:52<00:00, 142.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model at epoch 1 step 89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbd394ce150>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'train' in option:\n",
    "  train()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(training_perp_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmiSD8SGS4l0"
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ogFx3Frx79la",
    "outputId": "37c96391-a3d8-450a-b504-e398b98e94b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spectrum file: ../smbp_data/spectrum_smbp.mgf\n",
      "input feature file: ../smbp_data/features_smbp.csv.valid\n",
      "read cached spectrum locations\n",
      "read 535 features, 3 skipped by mass, 0 skipped by unknown modification, 0 skipped by length\n",
      "load pretrained model\n",
      "validation perplexity: 1.3306849783659758\n"
     ]
    }
   ],
   "source": [
    "if 'valid' in option:\n",
    "  #initialize the validation data in DataClass\n",
    "  valid_set = DeepNovoTrainDataset(input_feature_file_valid,\n",
    "                                          input_spectrum_file_valid)\n",
    "  valid_data_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          shuffle=False,\n",
    "                                                          num_workers=num_workers,\n",
    "                                                          collate_fn=collate_func)\n",
    "  #Feed validation data to the Model\n",
    "  forward_deepnovo, backward_deepnovo, init_net = build_model(training=False)\n",
    "  forward_deepnovo.eval()\n",
    "  backward_deepnovo.eval()\n",
    "  #compute validation loss with the validation function\n",
    "  validation_loss = validation(forward_deepnovo, backward_deepnovo, init_net, valid_data_loader)\n",
    "  print(f\"validation perplexity: {perplexity(validation_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASIQeP26eAeU"
   },
   "source": [
    "# Denovo Prediction <a name=\"denovo\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUZXhvJxim7o"
   },
   "source": [
    "## Denovo Path <a name=\"denovo_path\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xaq2C4-winfF"
   },
   "outputs": [],
   "source": [
    "# denovo path files\n",
    "denovo_input_spectrum_file = \"../smbp_data/spectrum_smbp.mgf\"\n",
    "denovo_input_feature_file = \"../smbp_data/features_smbp.csv.test\"\n",
    "\n",
    "#Path of the denovo output predictions\n",
    "denovo_output_file = denovo_input_feature_file + \".deepnovo_denovo\"\n",
    "\n",
    "predicted_format = \"deepnovo\"\n",
    "target_file = denovo_input_feature_file\n",
    "predicted_file = denovo_output_file\n",
    "\n",
    "#Name of the files produced after denovo prediction\n",
    "accuracy_file = predicted_file + \".accuracy\"\n",
    "denovo_only_file = predicted_file + \".denovo_only\"\n",
    "scan2fea_file = predicted_file + \".scan2fea\"\n",
    "multifea_file = predicted_file + \".multifea\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94FLPE_IPtqe"
   },
   "source": [
    "## Denovo Dataset Data reader <a name=\"denovo_datareader\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5BTcFsXPskK"
   },
   "outputs": [],
   "source": [
    "class DeepNovoDenovoDataset(DeepNovoTrainDataset):\n",
    "    # override _get_feature method in the dataclass DeepNovoTrainDataset\n",
    "    def _get_feature(self, feature: DDAFeature) -> DenovoData:\n",
    "        spectrum_location = self.spectrum_location_dict[feature.scan]\n",
    "        self.input_spectrum_handle.seek(spectrum_location)\n",
    "        # parse header lines\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"BEGIN IONS\" in line, \"Error: wrong input BEGIN IONS\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"TITLE=\" in line, \"Error: wrong input TITLE=\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"PEPMASS=\" in line, \"Error: wrong input PEPMASS=\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"CHARGE=\" in line, \"Error: wrong input CHARGE=\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"SCANS=\" in line, \"Error: wrong input SCANS=\"\n",
    "        line = self.input_spectrum_handle.readline()\n",
    "        assert \"RTINSECONDS=\" in line, \"Error: wrong input RTINSECONDS=\"\n",
    "        mz_list, intensity_list = self._parse_spectrum_ion()\n",
    "        peak_location, peak_intensity, spectrum_representation = process_peaks(mz_list, intensity_list, feature.mass)\n",
    "\n",
    "        return DenovoData(peak_location=peak_location,\n",
    "                          peak_intensity=peak_intensity,\n",
    "                          spectrum_representation=spectrum_representation,\n",
    "                          original_dda_feature=feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2RgG5ZmrNgC"
   },
   "source": [
    "## Writer Functions <a name=\"denovo_writer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8QWe-h6rQ5g"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BeamSearchedSequence:\n",
    "    sequence: list  # list of aa id\n",
    "    position_score: list\n",
    "    score: float  # average by length score\n",
    "\n",
    "\n",
    "class DenovoWriter(object):\n",
    "    def __init__(self, denovo_output_file):\n",
    "        self.output_handle = open(denovo_output_file, 'w')\n",
    "        header_list = [\"feature_id\",\n",
    "                       \"feature_area\",\n",
    "                       \"predicted_sequence\",\n",
    "                       \"predicted_score\",\n",
    "                       \"predicted_position_score\",\n",
    "                       \"precursor_mz\",\n",
    "                       \"precursor_charge\",\n",
    "                       \"protein_access_id\",\n",
    "                       \"scan_list_middle\",\n",
    "                       \"scan_list_original\",\n",
    "                       \"predicted_score_max\"]\n",
    "        header_row = \"\\t\".join(header_list)\n",
    "        print(header_row, file=self.output_handle, end='\\n')\n",
    "\n",
    "    def close(self):\n",
    "        self.output_handle.close()\n",
    "\n",
    "    def write(self, dda_original_feature: DDAFeature, searched_sequence: BeamSearchedSequence):\n",
    "        \"\"\"\n",
    "        Write the rows of prediction, outputed from the BeamSearch\n",
    "        :param dda_original_feature:\n",
    "        :param searched_sequence:\n",
    "        :return: -> Print predicted row\n",
    "        \"\"\"\n",
    "        feature_id = dda_original_feature.feature_id\n",
    "        feature_area = dda_original_feature.feature_area\n",
    "        precursor_mz = str(dda_original_feature.mz)\n",
    "        precursor_charge = str(dda_original_feature.z)\n",
    "        scan_list_middle = dda_original_feature.scan\n",
    "        scan_list_original = dda_original_feature.scan\n",
    "        if searched_sequence.sequence:\n",
    "            predicted_sequence = ','.join([vocab_reverse[aa_id] for\n",
    "                                           aa_id in searched_sequence.sequence])\n",
    "            predicted_score = \"{:.2f}\".format(searched_sequence.score)\n",
    "            predicted_score_max = predicted_score\n",
    "            predicted_position_score = ','.join(['{0:.2f}'.format(x) for x in searched_sequence.position_score])\n",
    "            protein_access_id = 'DENOVO'\n",
    "        else:\n",
    "            predicted_sequence = \"\"\n",
    "            predicted_score = \"\"\n",
    "            predicted_score_max = \"\"\n",
    "            predicted_position_score = \"\"\n",
    "            protein_access_id = \"\"\n",
    "        predicted_row = \"\\t\".join([feature_id,\n",
    "                                   feature_area,\n",
    "                                   predicted_sequence,\n",
    "                                   predicted_score,\n",
    "                                   predicted_position_score,\n",
    "                                   precursor_mz,\n",
    "                                   precursor_charge,\n",
    "                                   protein_access_id,\n",
    "                                   scan_list_middle,\n",
    "                                   scan_list_original,\n",
    "                                   predicted_score_max])\n",
    "        print(predicted_row, file=self.output_handle, end=\"\\n\")\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gF1-s2RvpdRj"
   },
   "source": [
    "## KnapSack Implementation <a name=\"knapsack\"></a>\n",
    "If the file knapsack.py is not present in the root path of the program, it will build a new one. (it takes time to build, ~40min with Colab).\n",
    "But you can upload it from the data included as it is a constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGUAYJfbphWH"
   },
   "outputs": [],
   "source": [
    "class Direction(Enum):\n",
    "    forward = 1\n",
    "    backward = 2\n",
    "\n",
    "@dataclass\n",
    "class BeamSearchStartPoint:\n",
    "    prefix_mass: float\n",
    "    suffix_mass: float\n",
    "    mass_tolerance: float\n",
    "    direction: Direction\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DenovoResult:\n",
    "    dda_feature: DDAFeature\n",
    "    best_beam_search_sequence: BeamSearchedSequence\n",
    "\n",
    "\n",
    "class KnapsackSearcher(object):\n",
    "    \"\"\"\n",
    "    Implementation of the Knapsack Algorithm.\n",
    "    If Knapsack not present in the source file, build a new one based on the parameters set\n",
    "    in global variables (vocabulary and masses)\n",
    "    \"\"\"\n",
    "    def __init__(self, MZ_MAX, knapsack_file):\n",
    "        self.knapsack_file = knapsack_file\n",
    "        self.MZ_MAX = MZ_MAX\n",
    "        self.knapsack_aa_resolution = KNAPSACK_AA_RESOLUTION\n",
    "        if os.path.isfile(knapsack_file):\n",
    "            print(\"KnapsackSearcher.__init__(): load knapsack matrix\")\n",
    "            self.knapsack_matrix = np.load(knapsack_file)\n",
    "        else:\n",
    "            print(\"KnapsackSearcher.__init__(): build knapsack matrix from scratch\")\n",
    "            self.knapsack_matrix = self._build_knapsack()\n",
    "\n",
    "    def _build_knapsack(self):\n",
    "        max_mass = self.MZ_MAX - mass_N_terminus - mass_C_terminus\n",
    "        max_mass_round = int(round(max_mass * self.knapsack_aa_resolution))\n",
    "        max_mass_upperbound = max_mass_round + self.knapsack_aa_resolution\n",
    "        knapsack_matrix = np.zeros(shape=(vocab_size, max_mass_upperbound), dtype=bool)\n",
    "        for aa_id in tqdm(range(3, vocab_size)):\n",
    "            mass_aa = int(round(mass_ID[aa_id] * self.knapsack_aa_resolution))\n",
    "\n",
    "            for col in range(max_mass_upperbound):\n",
    "                current_mass = col + 1\n",
    "                if current_mass < mass_aa:\n",
    "                    knapsack_matrix[aa_id, col] = False\n",
    "                elif current_mass == mass_aa:\n",
    "                    knapsack_matrix[aa_id, col] = True\n",
    "                elif current_mass > mass_aa:\n",
    "                    sub_mass = current_mass - mass_aa\n",
    "                    sub_col = sub_mass - 1\n",
    "                    if np.sum(knapsack_matrix[:, sub_col]) > 0:\n",
    "                        knapsack_matrix[aa_id, col] = True\n",
    "                        knapsack_matrix[:, col] = np.logical_or(knapsack_matrix[:, col], knapsack_matrix[:, sub_col])\n",
    "                    else:\n",
    "                        knapsack_matrix[aa_id, col] = False\n",
    "        np.save(self.knapsack_file, knapsack_matrix)\n",
    "        return knapsack_matrix\n",
    "\n",
    "    def search_knapsack(self, mass, knapsack_tolerance):\n",
    "        mass_round = int(round(mass * self.knapsack_aa_resolution))\n",
    "        mass_upperbound = mass_round + knapsack_tolerance\n",
    "        mass_lowerbound = mass_round - knapsack_tolerance\n",
    "        if mass_upperbound < mass_AA_min_round:\n",
    "            return []\n",
    "        mass_lowerbound_col = mass_lowerbound - 1\n",
    "        mass_upperbound_col = mass_upperbound - 1\n",
    "        candidate_aa_id = np.flatnonzero(np.any(self.knapsack_matrix[:, mass_lowerbound_col:(mass_upperbound_col + 1)],\n",
    "                                                axis=1))\n",
    "        return candidate_aa_id.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "deDumEalsrqv"
   },
   "source": [
    "## ION CNN Denovo <a name=\"ioncnn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvLoZhBKsuwT"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchPath:\n",
    "    aa_id_list: list\n",
    "    aa_seq_mass: float\n",
    "    score_list: list\n",
    "    score_sum: float\n",
    "    lstm_state: tuple  # state tuple store in search path is of shape [num_lstm_layers, num_units]\n",
    "    direction: Direction\n",
    "\n",
    "@dataclass\n",
    "class SearchEntry:\n",
    "    feature_index: int\n",
    "    current_path_list: list  # list of search paths\n",
    "    spectrum_state: tuple  # tuple of (peak_location, peak_intensity)\n",
    "\n",
    "\n",
    "class IonCNNDenovo(object):\n",
    "    def __init__(self, MZ_MAX, knapsack_file, beam_size):\n",
    "        self.MZ_MAX = MZ_MAX\n",
    "        self.beam_size = beam_size\n",
    "        self.knapsack_searcher = KnapsackSearcher(MZ_MAX, knapsack_file)\n",
    "\n",
    "    def _beam_search(self, model_wrapper: InferenceModelWrapper,\n",
    "                     feature_dp_batch: list, start_point_batch: list) -> list:\n",
    "        \"\"\"\n",
    "\n",
    "        :param model_wrapper:\n",
    "        :param feature_dp_batch: list of DenovoData\n",
    "        :param start_point_batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        num_features = len(feature_dp_batch)\n",
    "        top_path_batch = [[] for _ in range(num_features)]\n",
    "\n",
    "        direction_cint_map = {Direction.forward: 0, Direction.backward: 1}\n",
    "\n",
    "        direction = start_point_batch[0].direction\n",
    "        if direction == Direction.forward:\n",
    "            get_start_mass = lambda x: x.prefix_mass\n",
    "            first_label = GO_ID\n",
    "            last_label = EOS_ID\n",
    "        elif direction == Direction.backward:\n",
    "            get_start_mass = lambda x: x.suffix_mass\n",
    "            first_label = EOS_ID\n",
    "            last_label = GO_ID\n",
    "        else:\n",
    "            raise ValueError('direction neither forward nor backward')\n",
    "\n",
    "        # step 1: extract original spectrum\n",
    "        batch_peak_location = np.array([x.peak_location for x in feature_dp_batch])\n",
    "        batch_peak_intensity = np.array([x.peak_intensity for x in feature_dp_batch])\n",
    "        batch_spectrum_representation = np.array([x.spectrum_representation for x in feature_dp_batch])\n",
    "\n",
    "        batch_peak_location = torch.from_numpy(batch_peak_location).to(device)\n",
    "        batch_peak_intensity = torch.from_numpy(batch_peak_intensity).to(device)\n",
    "        batch_spectrum_representation = torch.from_numpy(batch_spectrum_representation).to(device)\n",
    "\n",
    "        initial_hidden_state_tuple = model_wrapper.initial_hidden_state(batch_spectrum_representation) if \\\n",
    "            use_lstm else None\n",
    "\n",
    "        # initialize activate search list\n",
    "        active_search_list = []\n",
    "        for feature_index in range(num_features):\n",
    "            # all feature in the same batch should be from same direction\n",
    "            assert direction == start_point_batch[feature_index].direction\n",
    "\n",
    "            spectrum_state = (batch_peak_location[feature_index], batch_peak_intensity[feature_index])\n",
    "\n",
    "            if use_lstm:\n",
    "                lstm_state_temp = (initial_hidden_state_tuple[0][:, feature_index, :],\n",
    "                                   initial_hidden_state_tuple[1][:, feature_index, :]\n",
    "                                   )\n",
    "            else:\n",
    "                lstm_state_temp = None\n",
    "\n",
    "            path = SearchPath(\n",
    "                aa_id_list=[first_label],\n",
    "                aa_seq_mass=get_start_mass(start_point_batch[feature_index]),\n",
    "                score_list=[0.0],\n",
    "                score_sum=0.0,\n",
    "                lstm_state=lstm_state_temp,\n",
    "                direction=direction,\n",
    "            )\n",
    "            search_entry = SearchEntry(\n",
    "                feature_index=feature_index,\n",
    "                current_path_list=[path],\n",
    "                spectrum_state=spectrum_state,\n",
    "            )\n",
    "            active_search_list.append(search_entry)\n",
    "\n",
    "        # repeat STEP 2, 3, 4 until the active_search_list is empty.\n",
    "        while True:\n",
    "            # STEP 2: gather data from active search entries and group into blocks.\n",
    "\n",
    "            # model input\n",
    "            block_aa_id_input = []\n",
    "            block_ion_location = []\n",
    "            block_peak_location = []\n",
    "            block_peak_intensity = []\n",
    "            block_lstm_h = []\n",
    "            block_lstm_c = []\n",
    "            # data stored in path\n",
    "            block_aa_id_list = []\n",
    "            block_aa_seq_mass = []\n",
    "            block_score_list = []\n",
    "            block_score_sum = []\n",
    "            block_knapsack_candidates = []\n",
    "\n",
    "            # store the number of paths of each search entry in the big blocks\n",
    "            #     to retrieve the info of each search entry later in STEP 4.\n",
    "            search_entry_size = [0] * len(active_search_list)\n",
    "\n",
    "            for entry_index, search_entry in enumerate(active_search_list):\n",
    "                feature_index = search_entry.feature_index\n",
    "                current_path_list = search_entry.current_path_list\n",
    "                precursor_mass = feature_dp_batch[feature_index].original_dda_feature.mass\n",
    "                peak_mass_tolerance = start_point_batch[feature_index].mass_tolerance\n",
    "\n",
    "                for path in current_path_list:\n",
    "                    aa_id_list = path.aa_id_list\n",
    "                    aa_id = aa_id_list[-1]\n",
    "                    score_sum = path.score_sum\n",
    "                    aa_seq_mass = path.aa_seq_mass\n",
    "                    score_list = path.score_list\n",
    "                    original_spectrum_tuple = search_entry.spectrum_state\n",
    "                    lstm_state_tuple = path.lstm_state\n",
    "\n",
    "                    if aa_id == last_label:\n",
    "                        if abs(aa_seq_mass - precursor_mass) <= peak_mass_tolerance:\n",
    "                            seq = aa_id_list[1:-1]\n",
    "                            trunc_score_list = score_list[1:-1]\n",
    "                            if direction == Direction.backward:\n",
    "                                seq = seq[::-1]\n",
    "                                trunc_score_list = trunc_score_list[::-1]\n",
    "\n",
    "                            top_path_batch[feature_index].append(\n",
    "                                BeamSearchedSequence(sequence=seq,\n",
    "                                                     position_score=trunc_score_list,\n",
    "                                                     score=path.score_sum / len(seq))\n",
    "                            )\n",
    "                        continue\n",
    "\n",
    "                    ion_location = get_ion_index(precursor_mass, aa_seq_mass, direction_cint_map[direction])  # [26,8]\n",
    "\n",
    "                    residual_mass = precursor_mass - aa_seq_mass - mass_ID[last_label]\n",
    "                    knapsack_tolerance = int(round(peak_mass_tolerance * KNAPSACK_AA_RESOLUTION))\n",
    "                    knapsack_candidates = self.knapsack_searcher.search_knapsack(residual_mass, knapsack_tolerance)\n",
    "\n",
    "                    if not knapsack_candidates:\n",
    "                        # if not possible aa, force it to stop.\n",
    "                        knapsack_candidates.append(last_label)\n",
    "\n",
    "                    block_ion_location.append(ion_location)\n",
    "                    block_aa_id_input.append(aa_id)\n",
    "                    # get hidden state block\n",
    "                    block_peak_location.append(original_spectrum_tuple[0])\n",
    "                    block_peak_intensity.append(original_spectrum_tuple[1])\n",
    "                    if use_lstm:\n",
    "                        block_lstm_h.append(lstm_state_tuple[0])\n",
    "                        block_lstm_c.append(lstm_state_tuple[1])\n",
    "\n",
    "                    block_aa_id_list.append(aa_id_list)\n",
    "                    block_aa_seq_mass.append(aa_seq_mass)\n",
    "                    block_score_list.append(score_list)\n",
    "                    block_score_sum.append(score_sum)\n",
    "                    block_knapsack_candidates.append(knapsack_candidates)\n",
    "                    # record the size of each search entry in the blocks\n",
    "                    search_entry_size[entry_index] += 1\n",
    "\n",
    "            # step 3 run model on data blocks to predict next AA.\n",
    "            #     output is stored in current_log_prob\n",
    "            # assert block_aa_id_list, 'IonCNNDenovo._beam_search(): aa_id_list is empty.'\n",
    "            if not block_ion_location:\n",
    "                # all search entry finished in the previous step\n",
    "                break\n",
    "\n",
    "            block_ion_location = torch.from_numpy(np.array(block_ion_location)).to(device)  # [batch, 26, 8, 10]\n",
    "            block_ion_location = torch.unsqueeze(block_ion_location, dim=1)  # [batch, 1, 26, 8]\n",
    "            block_peak_location = torch.stack(block_peak_location, dim=0).contiguous()\n",
    "            block_peak_intensity = torch.stack(block_peak_intensity, dim=0).contiguous()\n",
    "            if use_lstm:\n",
    "                block_lstm_h = torch.stack(block_lstm_h, dim=1).contiguous()\n",
    "                block_lstm_c = torch.stack(block_lstm_c, dim=1).contiguous()\n",
    "                block_state_tuple = (block_lstm_h, block_lstm_c)\n",
    "                block_aa_id_input = torch.from_numpy(np.array(block_aa_id_input, dtype=np.int64)).unsqueeze(1).to(\n",
    "                    device)\n",
    "            else:\n",
    "                block_state_tuple = None\n",
    "                block_aa_id_input = None\n",
    "\n",
    "            current_log_prob, new_state_tuple = model_wrapper.step(block_ion_location,\n",
    "                                                                   block_peak_location,\n",
    "                                                                   block_peak_intensity,\n",
    "                                                                   block_aa_id_input,\n",
    "                                                                   block_state_tuple,\n",
    "                                                                   direction)\n",
    "            # transfer log_prob back to cpu\n",
    "            current_log_prob = current_log_prob.cpu().numpy()\n",
    "\n",
    "            # STEP 4: retrieve data from blocks to update the active_search_list\n",
    "            #     with knapsack dynamic programming and beam search.\n",
    "            block_index = 0\n",
    "            for entry_index, search_entry in enumerate(active_search_list):\n",
    "                new_path_list = []\n",
    "                direction = search_entry.current_path_list[0].direction\n",
    "                for index in range(block_index, block_index + search_entry_size[entry_index]):\n",
    "                    for aa_id in block_knapsack_candidates[index]:\n",
    "                        if aa_id > 2:\n",
    "                            # do not add score of GO, EOS, PAD\n",
    "                            new_score_list = block_score_list[index] + [current_log_prob[index][aa_id]]\n",
    "                            new_score_sum = block_score_sum[index] + current_log_prob[index][aa_id]\n",
    "                        else:\n",
    "                            new_score_list = block_score_list[index] + [0.0]\n",
    "                            new_score_sum = block_score_sum[index] + 0.0\n",
    "\n",
    "                        if use_lstm:\n",
    "                            new_path_state_tuple = (new_state_tuple[0][:, index, :], new_state_tuple[1][:, index, :])\n",
    "                        else:\n",
    "                            new_path_state_tuple = None\n",
    "\n",
    "                        new_path = SearchPath(\n",
    "                            aa_id_list=block_aa_id_list[index] + [aa_id],\n",
    "                            aa_seq_mass=block_aa_seq_mass[index] + mass_ID[aa_id],\n",
    "                            score_list=new_score_list,\n",
    "                            score_sum=new_score_sum,\n",
    "                            lstm_state=new_path_state_tuple,\n",
    "                            direction=direction\n",
    "                        )\n",
    "                        new_path_list.append(new_path)\n",
    "                if len(new_path_list) > self.beam_size:\n",
    "                    new_path_score = np.array([x.score_sum for x in new_path_list])\n",
    "                    top_k_index = np.argpartition(-new_path_score, self.beam_size)[:self.beam_size]\n",
    "                    search_entry.current_path_list = [new_path_list[ii] for ii in top_k_index]\n",
    "                else:\n",
    "                    search_entry.current_path_list = new_path_list\n",
    "\n",
    "                block_index += search_entry_size[entry_index]\n",
    "\n",
    "            active_search_list = [x for x in active_search_list if x.current_path_list]\n",
    "\n",
    "            if not active_search_list:\n",
    "                break\n",
    "        return top_path_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_start_point(feature_dp_batch: list) -> tuple:\n",
    "        mass_GO = mass_ID[GO_ID]\n",
    "        forward_start_point_lists = [BeamSearchStartPoint(prefix_mass=mass_GO,\n",
    "                                                          suffix_mass=feature_dp.original_dda_feature.mass - mass_GO,\n",
    "                                                          mass_tolerance=PRECURSOR_MASS_PRECISION_TOLERANCE,\n",
    "                                                          direction=Direction.forward)\n",
    "                                     for feature_dp in feature_dp_batch]\n",
    "\n",
    "        mass_EOS = mass_ID[EOS_ID]\n",
    "        backward_start_point_lists = [BeamSearchStartPoint(prefix_mass=feature_dp.original_dda_feature.mass - mass_EOS,\n",
    "                                                           suffix_mass=mass_EOS,\n",
    "                                                           mass_tolerance=PRECURSOR_MASS_PRECISION_TOLERANCE,\n",
    "                                                           direction=Direction.backward)\n",
    "                                      for feature_dp in feature_dp_batch]\n",
    "        return forward_start_point_lists, backward_start_point_lists\n",
    "\n",
    "    @staticmethod\n",
    "    def _select_path(feature_dp_batch: list, top_candidate_batch: list) -> list:\n",
    "        \"\"\"\n",
    "        for each feature, select the best denovo sequence given by DeepNovo model\n",
    "        :param feature_dp_batch: list of DenovoData\n",
    "        :param top_candidate_batch: defined in _search_denovo_batch\n",
    "        :return:\n",
    "        list of DenovoResult\n",
    "        \"\"\"\n",
    "        feature_batch_size = len(feature_dp_batch)\n",
    "\n",
    "        refine_batch = [[] for x in range(feature_batch_size)]\n",
    "        for feature_index in range(feature_batch_size):\n",
    "            precursor_mass = feature_dp_batch[feature_index].original_dda_feature.mass\n",
    "            candidate_list = top_candidate_batch[feature_index]\n",
    "            for beam_search_sequence in candidate_list:\n",
    "                sequence = beam_search_sequence.sequence\n",
    "                sequence_mass = sum(mass_ID[x] for x in sequence)\n",
    "                sequence_mass += mass_ID[GO_ID] + mass_ID[EOS_ID]\n",
    "                if abs(sequence_mass - precursor_mass) <= PRECURSOR_MASS_PRECISION_TOLERANCE:\n",
    "                    refine_batch[feature_index].append(beam_search_sequence)\n",
    "        predicted_batch = []\n",
    "        for feature_index in range(feature_batch_size):\n",
    "            candidate_list = refine_batch[feature_index]\n",
    "            if not candidate_list:\n",
    "                best_beam_search_sequence = BeamSearchedSequence(\n",
    "                    sequence=[],\n",
    "                    position_score=[],\n",
    "                    score=-float('inf')\n",
    "                )\n",
    "            else:\n",
    "                # sort candidate sequence by average position score\n",
    "                best_beam_search_sequence = max(candidate_list, key=lambda x: x.score)\n",
    "\n",
    "            denovo_result = DenovoResult(\n",
    "                dda_feature=feature_dp_batch[feature_index].original_dda_feature,\n",
    "                best_beam_search_sequence=best_beam_search_sequence\n",
    "            )\n",
    "            predicted_batch.append(denovo_result)\n",
    "        return predicted_batch\n",
    "\n",
    "    def _search_denovo_batch(self, feature_dp_batch: list, model_wrapper: InferenceModelWrapper) -> list:\n",
    "        start_time = time.time()\n",
    "        feature_batch_size = len(feature_dp_batch)\n",
    "        start_points_tuple = self._get_start_point(feature_dp_batch)\n",
    "        top_candidate_batch = [[] for x in range(feature_batch_size)]\n",
    "\n",
    "        for start_points in start_points_tuple:\n",
    "            beam_search_result_batch = self._beam_search(model_wrapper, feature_dp_batch, start_points)\n",
    "            for feature_index in range(feature_batch_size):\n",
    "                top_candidate_batch[feature_index].extend(beam_search_result_batch[feature_index])\n",
    "        predicted_batch = self._select_path(feature_dp_batch, top_candidate_batch)\n",
    "        test_time = time.time() - start_time\n",
    "        print(\"beam_search(): batch time {}s\".format(test_time))\n",
    "        return predicted_batch\n",
    "\n",
    "    def search_denovo(self, model_wrapper: InferenceModelWrapper,\n",
    "                      beam_search_reader: DeepNovoDenovoDataset, denovo_writer: DenovoWriter):\n",
    "        print(\"start beam search denovo\")\n",
    "        predicted_denovo_list = []\n",
    "\n",
    "        test_set_iter = chunks(list(range(len(beam_search_reader))), n=batch_size)\n",
    "        total_batch_num = int(len(beam_search_reader) / batch_size)\n",
    "        for index, feature_batch_index in enumerate(test_set_iter):\n",
    "            feature_dp_batch = [beam_search_reader[i] for i in feature_batch_index]\n",
    "            print(\"Read {}th/{} batches\".format(index, total_batch_num))\n",
    "            predicted_batch = self._search_denovo_batch(feature_dp_batch, model_wrapper)\n",
    "            predicted_denovo_list += predicted_batch\n",
    "            for denovo_result in predicted_batch:\n",
    "                denovo_writer.write(denovo_result.dda_feature, denovo_result.best_beam_search_sequence)\n",
    "\n",
    "        return predicted_denovo_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLxJrg_IhQLP"
   },
   "source": [
    "## Chunk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87AFhMXzFqCd"
   },
   "outputs": [],
   "source": [
    "def chunks(l, n: int):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yEQp-lOkO8cv"
   },
   "source": [
    "## Launch DeepNovo denovo prediction <a name=\"denovo_launch\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ny9pj9cTPEWo",
    "outputId": "8fc7df12-2194-4c29-d13c-b2fe3e0597cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spectrum file: ../smbp_data/spectrum_smbp.mgf\n",
      "input feature file: ../smbp_data/features_smbp.csv.test\n",
      "read cached spectrum locations\n",
      "read 469 features, 0 skipped by mass, 0 skipped by unknown modification, 0 skipped by length\n",
      "data_reader done \n",
      "\n",
      "IonCNNDenovo starting \n",
      "\n",
      "KnapsackSearcher.__init__(): load knapsack matrix\n",
      "Building Model ... \n",
      "\n",
      "load pretrained model\n",
      "Wrapper \n",
      "\n",
      "Writing ... \n",
      "\n",
      "start beam search denovo\n",
      "Read 0th/29 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stryars/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam_search(): batch time 0.6671297550201416s\n",
      "Read 1th/29 batches\n",
      "beam_search(): batch time 0.7019972801208496s\n",
      "Read 2th/29 batches\n",
      "beam_search(): batch time 0.6950774192810059s\n",
      "Read 3th/29 batches\n",
      "beam_search(): batch time 0.6168060302734375s\n",
      "Read 4th/29 batches\n",
      "beam_search(): batch time 0.8356964588165283s\n",
      "Read 5th/29 batches\n",
      "beam_search(): batch time 0.753415584564209s\n",
      "Read 6th/29 batches\n",
      "beam_search(): batch time 0.7738871574401855s\n",
      "Read 7th/29 batches\n",
      "beam_search(): batch time 0.9844949245452881s\n",
      "Read 8th/29 batches\n",
      "beam_search(): batch time 0.6814014911651611s\n",
      "Read 9th/29 batches\n",
      "beam_search(): batch time 0.6944026947021484s\n",
      "Read 10th/29 batches\n",
      "beam_search(): batch time 0.538715124130249s\n",
      "Read 11th/29 batches\n",
      "beam_search(): batch time 0.579709529876709s\n",
      "Read 12th/29 batches\n",
      "beam_search(): batch time 0.5671288967132568s\n",
      "Read 13th/29 batches\n",
      "beam_search(): batch time 0.7232954502105713s\n",
      "Read 14th/29 batches\n",
      "beam_search(): batch time 0.6646101474761963s\n",
      "Read 15th/29 batches\n",
      "beam_search(): batch time 1.0739986896514893s\n",
      "Read 16th/29 batches\n",
      "beam_search(): batch time 0.7627885341644287s\n",
      "Read 17th/29 batches\n",
      "beam_search(): batch time 0.6949341297149658s\n",
      "Read 18th/29 batches\n",
      "beam_search(): batch time 0.8521983623504639s\n",
      "Read 19th/29 batches\n",
      "beam_search(): batch time 0.651362419128418s\n",
      "Read 20th/29 batches\n",
      "beam_search(): batch time 0.7014880180358887s\n",
      "Read 21th/29 batches\n",
      "beam_search(): batch time 0.7670354843139648s\n",
      "Read 22th/29 batches\n",
      "beam_search(): batch time 0.9591481685638428s\n",
      "Read 23th/29 batches\n",
      "beam_search(): batch time 1.071636438369751s\n",
      "Read 24th/29 batches\n",
      "beam_search(): batch time 0.9192392826080322s\n",
      "Read 25th/29 batches\n",
      "beam_search(): batch time 1.01473069190979s\n",
      "Read 26th/29 batches\n",
      "beam_search(): batch time 0.6849613189697266s\n",
      "Read 27th/29 batches\n",
      "beam_search(): batch time 0.47785449028015137s\n",
      "Read 28th/29 batches\n",
      "beam_search(): batch time 0.9221580028533936s\n",
      "Read 29th/29 batches\n",
      "beam_search(): batch time 0.19628047943115234s\n"
     ]
    }
   ],
   "source": [
    "beam_size = beam_size_param #by default 5\n",
    "if 'denovo' in option:\n",
    "  data_reader = DeepNovoDenovoDataset(feature_filename=denovo_input_feature_file,\n",
    "                                              spectrum_filename=denovo_input_spectrum_file)\n",
    "  print('data_reader done \\n')\n",
    "  print('IonCNNDenovo starting \\n')\n",
    "  denovo_worker = IonCNNDenovo(MZ_MAX,\n",
    "                                      knapsack_file,\n",
    "                                      beam_size=beam_size)\n",
    "  print('Building Model ... \\n')\n",
    "  forward_deepnovo, backward_deepnovo, init_net = build_model(training=False)\n",
    "  print('Wrapper \\n')\n",
    "  model_wrapper = InferenceModelWrapper(forward_deepnovo, backward_deepnovo, init_net)\n",
    "  print('Writing ... \\n')\n",
    "  writer = DenovoWriter(denovo_output_file)\n",
    "\n",
    "  denovo_worker.search_denovo(model_wrapper, data_reader, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGC7plCMk_p2"
   },
   "source": [
    "# Testing Model <a name=\"test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoasZnLplNqZ"
   },
   "source": [
    "## Testing File Path <a name=\"test_path\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEe6yfwyZ6-N"
   },
   "source": [
    "Select the path of the testing dataset (usualy part of the initial dataset randomely split in training, valid and testing parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qH0wk0jhlR_t"
   },
   "outputs": [],
   "source": [
    "input_spectrum_file_test = \"../smbp_data/spectrum_smbp.mgf\"\n",
    "input_feature_file_test = \"../smbp_data/feature_smbp.csv.test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OkvEvQSvV3H"
   },
   "source": [
    "## Worker Test function <a name=\"test_worker\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w8YznjlbvQuu"
   },
   "outputs": [],
   "source": [
    "class WorkerTest(object):\n",
    "  \"\"\"\n",
    "     The WorkerTest is a function that will compare the sequence predicted by the\n",
    "     denovo function with the original sequence (if available) and display the accuracy\n",
    "     of the model trained, on these 'denovo' testing sequences\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self):\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest.__init__()\")\n",
    "\n",
    "    #get all the variables needed\n",
    "    self.MZ_MAX = MZ_MAX\n",
    "\n",
    "    self.target_file = target_file #denovo input\n",
    "    self.predicted_file = predicted_file #denovo output\n",
    "    self.predicted_format = predicted_format # tag \"deepnovo\"\n",
    "    \n",
    "    #get the path for the output of the function\n",
    "    self.accuracy_file = accuracy_file \n",
    "    self.denovo_only_file = denovo_only_file\n",
    "    self.scan2fea_file = scan2fea_file\n",
    "    self.multifea_file = multifea_file\n",
    "\n",
    "    print(\"target_file = {0:s}\".format(self.target_file))\n",
    "    print(\"predicted_file = {0:s}\".format(self.predicted_file))\n",
    "    print(\"predicted_format = {0:s}\".format(self.predicted_format))\n",
    "    print(\"accuracy_file = {0:s}\".format(self.accuracy_file))\n",
    "    print(\"denovo_only_file = {0:s}\".format(self.denovo_only_file))\n",
    "    print(\"scan2fea_file = {0:s}\".format(self.scan2fea_file))\n",
    "    print(\"multifea_file = {0:s}\".format(self.multifea_file))\n",
    "\n",
    "    self.target_dict = {}\n",
    "    self.predicted_list = []\n",
    "\n",
    "\n",
    "  def test_accuracy(self, db_peptide_list=None):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest.test_accuracy()\")\n",
    "\n",
    "    # write the accuracy of predicted peptides\n",
    "    accuracy_handle = open(self.accuracy_file, 'w')\n",
    "    header_list = [\"feature_id\",\n",
    "                   \"feature_area\",\n",
    "                   \"target_sequence\",\n",
    "                   \"predicted_sequence\",\n",
    "                   \"predicted_score\",\n",
    "                   \"recall_AA\",\n",
    "                   \"predicted_len\",\n",
    "                   \"target_len\",\n",
    "                   \"scan_list_middle\",\n",
    "                   \"scan_list_original\"]\n",
    "    header_row = \"\\t\".join(header_list)\n",
    "    print(header_row, file=accuracy_handle, end=\"\\n\")\n",
    "\n",
    "    # write denovo_only peptides (sequence with a reference to coompare)\n",
    "    denovo_only_handle = open(self.denovo_only_file, 'w')\n",
    "    header_list = [\"feature_id\",\n",
    "                   \"feature_area\",\n",
    "                   \"predicted_sequence\",\n",
    "                   \"predicted_score\",\n",
    "                   \"predicted_score_max\",\n",
    "                   \"scan_list_middle\",\n",
    "                   \"scan_list_original\"]\n",
    "    header_row = \"\\t\".join(header_list)\n",
    "    print(header_row, file=denovo_only_handle, end=\"\\n\")\n",
    "\n",
    "    self._get_target()\n",
    "    target_count_total = len(self.target_dict)\n",
    "    target_len_total = sum([len(x) for x in self.target_dict.values()])\n",
    "    target_dict_db = {}\n",
    "\n",
    "    #Depricated if no use of PEAKS DB to compare the sequence predicted (in SMBP case, use of MASCOT)\n",
    "\n",
    "    # this part is tricky!\n",
    "    # some target peptides are reported by PEAKS DB but not found in\n",
    "    #   db_peptide_list due to mistakes in cleavage rules.\n",
    "    # if db_peptide_list is given, we only consider those target peptides,\n",
    "    #   otherwise, use all target peptides\n",
    "    if db_peptide_list is not None:\n",
    "      for feature_id, target in self.target_dict.items():\n",
    "        target_simplied = target\n",
    "        # remove the extension 'mod' from variable modifications\n",
    "        target_simplied = ['M' if x=='M(Oxidation)' else x for x in target_simplied]\n",
    "        target_simplied = ['N' if x=='N(Deamidation)' else x for x in target_simplied]\n",
    "        target_simplied = ['Q' if x=='Q(Deamidation)' else x for x in target_simplied]\n",
    "\n",
    "        #target_simplied = ['M' if x=='M(Oxidated)' else x for x in target_simplied]\n",
    "        #target_simplied = ['Q' if x=='Q(Deamidated)' else x for x in target_simplied]\n",
    "        #target_simplied = ['C' if x=='C(Carboxymethyl)' else x for x in target_simplied]\n",
    "\n",
    "        if target_simplied in db_peptide_list:\n",
    "          target_dict_db[feature_id] = target\n",
    "        else:\n",
    "          print(\"target not found: \", target_simplied)\n",
    "    #==========================================================================\n",
    "    else:\n",
    "      target_dict_db = self.target_dict\n",
    "    target_count_db = len(target_dict_db)\n",
    "    target_len_db = sum([len(x) for x in target_dict_db.values()])\n",
    "\n",
    "    # we also skip target peptides with precursor_mass > MZ_MAX\n",
    "    target_dict_db_mass = {}\n",
    "    for feature_id, peptide in target_dict_db.items():\n",
    "      if self._compute_peptide_mass(peptide) <= self.MZ_MAX:\n",
    "        target_dict_db_mass[feature_id] = peptide\n",
    "    target_count_db_mass = len(target_dict_db_mass)\n",
    "    target_len_db_mass = sum([len(x) for x in target_dict_db_mass.values()])\n",
    "\n",
    "    # read predicted peptides from deepnovo or peaks\n",
    "    if predicted_format == \"deepnovo\":\n",
    "      self._get_predicted()\n",
    "    else:\n",
    "      self._get_predicted_peaks()\n",
    "\n",
    "    # note that the prediction has already skipped precursor_mass > MZ_MAX\n",
    "    # we also skip predicted peptides whose feature_id's are not in target_dict_db_mass\n",
    "    predicted_count_mass = len(self.predicted_list)\n",
    "    predicted_count_mass_db = 0\n",
    "    predicted_len_mass_db = 0\n",
    "    predicted_only = 0\n",
    "    # the recall is calculated on remaining peptides\n",
    "    recall_AA_total = 0.0\n",
    "    recall_peptide_total = 0.0\n",
    "\n",
    "    # record scan with multiple features\n",
    "    scan_dict = {}\n",
    "\n",
    "    for index, predicted in enumerate(self.predicted_list):\n",
    "\n",
    "      feature_id = predicted[\"feature_id\"]\n",
    "      feature_area = str(predicted[\"feature_area\"])\n",
    "      feature_scan_list_middle = predicted[\"scan_list_middle\"]\n",
    "      feature_scan_list_original = predicted[\"scan_list_original\"]\n",
    "      if feature_scan_list_original:\n",
    "        for scan in re.split(';|\\r|\\n', feature_scan_list_original):\n",
    "          if scan in scan_dict:\n",
    "            scan_dict[scan][\"feature_count\"] += 1\n",
    "            scan_dict[scan][\"feature_list\"].append(feature_id)\n",
    "          else:\n",
    "            scan_dict[scan] = {}\n",
    "            scan_dict[scan][\"feature_count\"] = 1\n",
    "            scan_dict[scan][\"feature_list\"] = [feature_id]\n",
    "\n",
    "      if feature_id in target_dict_db_mass:\n",
    "\n",
    "        predicted_count_mass_db += 1\n",
    "\n",
    "        target = target_dict_db_mass[feature_id]\n",
    "        target_len= len(target)\n",
    "\n",
    "        # if >= 1 denovo peptides reported, calculate the best accuracy\n",
    "        best_recall_AA = 0\n",
    "        best_predicted_sequence = predicted[\"sequence\"][0]\n",
    "        best_predicted_score = predicted[\"score\"][0]\n",
    "        for predicted_sequence, predicted_score in zip(predicted[\"sequence\"], predicted[\"score\"]):\n",
    "          predicted_AA_id = [vocab[x] for x in predicted_sequence]\n",
    "          target_AA_id = [vocab[x] for x in target]\n",
    "          recall_AA = self._match_AA_novor(target_AA_id, predicted_AA_id)\n",
    "          if (recall_AA > best_recall_AA\n",
    "              or (recall_AA == best_recall_AA and predicted_score > best_predicted_score)):\n",
    "            best_recall_AA = recall_AA\n",
    "            best_predicted_sequence = predicted_sequence[:]\n",
    "            best_predicted_score = predicted_score\n",
    "        recall_AA = best_recall_AA\n",
    "        predicted_sequence = best_predicted_sequence[:]\n",
    "        predicted_score = best_predicted_score\n",
    "\n",
    "        recall_AA_total += recall_AA\n",
    "        if recall_AA == target_len:\n",
    "          recall_peptide_total += 1\n",
    "        predicted_len= len(predicted_sequence)\n",
    "        predicted_len_mass_db += predicted_len\n",
    "\n",
    "        # convert to string format to print out\n",
    "        target_sequence = \",\".join(target)\n",
    "        predicted_sequence = \",\".join(predicted_sequence)\n",
    "        predicted_score = \"{0:.2f}\".format(predicted_score)\n",
    "        recall_AA = \"{0:d}\".format(recall_AA)\n",
    "        predicted_len = \"{0:d}\".format(predicted_len)\n",
    "        target_len = \"{0:d}\".format(target_len)\n",
    "        print_list = [feature_id,\n",
    "                      feature_area,\n",
    "                      target_sequence,\n",
    "                      predicted_sequence,\n",
    "                      predicted_score,\n",
    "                      recall_AA,\n",
    "                      predicted_len,\n",
    "                      target_len,\n",
    "                      feature_scan_list_middle,\n",
    "                      feature_scan_list_original]\n",
    "        print_row = \"\\t\".join(print_list)\n",
    "        print(print_row, file=accuracy_handle, end=\"\\n\")\n",
    "      else:\n",
    "        predicted_only += 1\n",
    "        predicted_sequence = ';'.join([','.join(x) for x in predicted[\"sequence\"]])\n",
    "        predicted_score = ';'.join(['{0:.2f}'.format(x) for x in predicted[\"score\"]])\n",
    "        if predicted[\"score\"]:\n",
    "          predicted_score_max = '{0:.2f}'.format(np.max(predicted[\"score\"]))\n",
    "        else:\n",
    "          predicted_score_max = ''\n",
    "        print_list = [feature_id,\n",
    "                      feature_area,\n",
    "                      predicted_sequence,\n",
    "                      predicted_score,\n",
    "                      predicted_score_max,\n",
    "                      feature_scan_list_middle,\n",
    "                      feature_scan_list_original]\n",
    "        print_row = \"\\t\".join(print_list)\n",
    "        print(print_row, file=denovo_only_handle, end=\"\\n\")\n",
    "\n",
    "    accuracy_handle.close()\n",
    "    denovo_only_handle.close()\n",
    "\n",
    "    multifea_dict = {}\n",
    "    for scan_id, value in scan_dict.items():\n",
    "      feature_count = value[\"feature_count\"]\n",
    "      feature_list = value[\"feature_list\"]\n",
    "      if feature_count > 1:\n",
    "        for feature_id in feature_list:\n",
    "          if feature_id in multifea_dict:\n",
    "            multifea_dict[feature_id].append(scan_id + ':' + str(feature_count))\n",
    "          else:\n",
    "            multifea_dict[feature_id] = [scan_id + ':' + str(feature_count)]\n",
    "\n",
    "    #scan2fea_file. Display the nomber of identic scan in a the spectrum\n",
    "    with open(self.scan2fea_file, 'w') as handle:\n",
    "      header_list = [\"scan_id\",\n",
    "                     \"feature_count\",\n",
    "                     \"feature_list\"]\n",
    "      header_row = \"\\t\".join(header_list)\n",
    "      print(header_row, file=handle, end=\"\\n\")\n",
    "      for scan_id, value in scan_dict.items():\n",
    "        print_list = [scan_id,\n",
    "                      str(value[\"feature_count\"]),\n",
    "                      \";\".join(value[\"feature_list\"])]\n",
    "        print_row = \"\\t\".join(print_list)\n",
    "        print(print_row, file=handle, end=\"\\n\")\n",
    "    #multifea_file. Display wich scans are doubles in spectrum\n",
    "    with open(self.multifea_file, 'w') as handle:\n",
    "      header_list = [\"feature_id\",\n",
    "                     \"scan_list\"]\n",
    "      header_row = \"\\t\".join(header_list)\n",
    "      print(header_row, file=handle, end=\"\\n\")\n",
    "      for feature_id, scan_list in multifea_dict.items():\n",
    "        print_list = [feature_id,\n",
    "                      \";\".join(scan_list)]\n",
    "        print_row = \"\\t\".join(print_list)\n",
    "        print(print_row, file=handle, end=\"\\n\")\n",
    "\n",
    "    print(\"target_count_total = {0:d}\".format(target_count_total))\n",
    "    print(\"target_len_total = {0:d}\".format(target_len_total))\n",
    "    print(\"target_count_db = {0:d}\".format(target_count_db))\n",
    "    print(\"target_len_db = {0:d}\".format(target_len_db))\n",
    "    print(\"target_count_db_mass: {0:d}\".format(target_count_db_mass))\n",
    "    print(\"target_len_db_mass: {0:d}\".format(target_len_db_mass))\n",
    "\n",
    "    print(\"predicted_count_mass: {0:d}\".format(predicted_count_mass))\n",
    "    print(\"predicted_count_mass_db: {0:d}\".format(predicted_count_mass_db))\n",
    "    print(\"predicted_len_mass_db: {0:d}\".format(predicted_len_mass_db))\n",
    "    print(\"predicted_only: {0:d}\".format(predicted_only))\n",
    "\n",
    "    #if added in case of failed training to avoid an error.\n",
    "    if target_len_total != 0:\n",
    "      print(\"recall_AA_total = {0:.4f}\".format(recall_AA_total / target_len_total))\n",
    "    if target_len_db != 0:\n",
    "      print(\"recall_AA_db = {0:.4f}\".format(recall_AA_total / target_len_db))\n",
    "    if target_len_db_mass != 0:\n",
    "      print(\"recall_AA_db_mass = {0:.4f}\".format(recall_AA_total / target_len_db_mass))\n",
    "    if target_count_total != 0:\n",
    "      print(\"recall_peptide_total = {0:.4f}\".format(recall_peptide_total / target_count_total))\n",
    "    if target_count_db != 0:\n",
    "      print(\"recall_peptide_db = {0:.4f}\".format(recall_peptide_total / target_count_db))\n",
    "    if target_count_db_mass != 0:\n",
    "      print(\"recall_peptide_db_mass = {0:.4f}\".format(recall_peptide_total / target_count_db_mass))\n",
    "    if predicted_len_mass_db != 0:\n",
    "      print(\"precision_AA_mass_db  = {0:.4f}\".format(recall_AA_total / predicted_len_mass_db))\n",
    "    if predicted_count_mass_db != 0:\n",
    "      print(\"precision_peptide_mass_db  = {0:.4f}\".format(recall_peptide_total / predicted_count_mass_db))\n",
    "  \n",
    "  \n",
    "  def _compute_peptide_mass(self, peptide):\n",
    "    \"\"\"\n",
    "    Add start and en mass to the aa chains\n",
    "    \"\"\"\n",
    "    #print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    #print(\"WorkerTest._compute_peptide_mass()\")\n",
    "    peptide_mass = (mass_N_terminus\n",
    "                    + sum(mass_AA[aa] for aa in peptide)\n",
    "                    + mass_C_terminus)\n",
    "\n",
    "    return peptide_mass\n",
    "\n",
    "\n",
    "  def _get_predicted(self):\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest._get_predicted()\")\n",
    "\n",
    "    predicted_list = []\n",
    "    col_feature_id = pcol_feature_id\n",
    "    col_feature_area = pcol_feature_area\n",
    "    col_sequence = pcol_sequence\n",
    "    col_score = pcol_score\n",
    "    col_scan_list_middle = pcol_scan_list_middle\n",
    "    col_scan_list_original = pcol_scan_list_original\n",
    "    with open(self.predicted_file, 'r') as handle:\n",
    "      # header\n",
    "      handle.readline()\n",
    "      for line in handle:\n",
    "        line_split = re.split('\\t|\\n', line)\n",
    "        predicted = {}\n",
    "        predicted[\"feature_id\"] = line_split[col_feature_id]\n",
    "        predicted[\"feature_area\"] = float(line_split[col_feature_area])\n",
    "        predicted[\"scan_list_middle\"] = line_split[col_scan_list_middle]\n",
    "        predicted[\"scan_list_original\"] = line_split[col_scan_list_original]\n",
    "        if line_split[col_sequence]: # not empty sequence\n",
    "          predicted[\"sequence\"] = [re.split(',', x)\n",
    "                                   for x in re.split(';', line_split[col_sequence])]\n",
    "          predicted[\"score\"] = [float(x)\n",
    "                                for x in re.split(';', line_split[col_score])]\n",
    "        else: \n",
    "          predicted[\"sequence\"] = [[]]\n",
    "          predicted[\"score\"] = [-999]\n",
    "        predicted_list.append(predicted)\n",
    "\n",
    "    self.predicted_list = predicted_list\n",
    "\n",
    "\n",
    "  def _get_predicted_peaks(self):\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest._get_predicted_peaks()\")\n",
    "\n",
    "    predicted_list = []\n",
    "    col_fraction_id = 0\n",
    "    fraction_id_map = {'1':'1',\n",
    "                       '2':'10',\n",
    "                       '3':'11',\n",
    "                       '4':'12',\n",
    "                       '5':'2',\n",
    "                       '6':'3',\n",
    "                       '7':'4',\n",
    "                       '8':'5',\n",
    "                       '9':'6',\n",
    "                       '10':'7',\n",
    "                       '11':'8',\n",
    "                       '12':'9',\n",
    "                      }\n",
    "    col_scan_id = 1\n",
    "    col_sequence = 3\n",
    "    with open(self.predicted_file, 'r') as handle:\n",
    "      # header\n",
    "      handle.readline()\n",
    "      for line in handle:\n",
    "        line_split = re.split(',|\\n', line)\n",
    "        predicted = {}\n",
    "        predicted[\"feature_id\"] = \"F\" + line_split[col_fraction_id] + \":\" + line_split[col_scan_id]\n",
    "        raw_sequence = line_split[col_sequence]\n",
    "        assert raw_sequence, \"Error: wrong format.\"\n",
    "        predicted[\"sequence\"] = self._parse_sequence(raw_sequence)\n",
    "        \n",
    "        # skip peptides with precursor_mass > MZ_MAX\n",
    "        if self._compute_peptide_mass(predicted[\"sequence\"]) > self.MZ_MAX:\n",
    "          continue\n",
    "        predicted[\"feature_area\"] = 0\n",
    "        predicted[\"scan_list_middle\"] = \"\"\n",
    "        predicted[\"scan_list_original\"] = \"\"\n",
    "        predicted[\"sequence\"] = [predicted[\"sequence\"]]\n",
    "        predicted[\"score\"] = [-999]\n",
    "        predicted_list.append(predicted)\n",
    "\n",
    "    self.predicted_list = predicted_list\n",
    "\n",
    "\n",
    "  def _get_target(self):\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest._get_target()\")\n",
    "\n",
    "    target_dict = {}\n",
    "    with open(self.target_file, 'r') as handle:\n",
    "      header_line = handle.readline()\n",
    "      header = header_line.strip().split(',')\n",
    "      raw_sequence_index = header.index(col_raw_sequence)\n",
    "      for line in handle:\n",
    "        line = re.split(',|\\r|\\n', line)\n",
    "        feature_id = line[0]\n",
    "        raw_sequence = line[raw_sequence_index]\n",
    "        assert raw_sequence, \"Error: wrong target format.\"\n",
    "        peptide = self._parse_sequence(raw_sequence)\n",
    "        target_dict[feature_id] = peptide\n",
    "    self.target_dict = target_dict\n",
    "\n",
    "\n",
    "  def _parse_sequence(self, raw_sequence):\n",
    "\n",
    "    #print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    #print(\"WorkerTest._parse_sequence()\")\n",
    "\n",
    "    return re.findall(r'[A-Z](?:\\(.+?\\))?', raw_sequence)\n",
    "\n",
    "    \"\"\" # Depricated if use the MASCOT processing script. Uncomment if use the \n",
    "    data from the original paper of Tran et al. 2019.\n",
    "\n",
    "    raw_sequence_len = len(raw_sequence)\n",
    "    peptide = []\n",
    "    index = 0\n",
    "    while index < raw_sequence_len:\n",
    "      if raw_sequence[index] == \"(\":\n",
    "        if peptide[-1] == \"C\" and raw_sequence[index:index+8] == \"(+57.02)\":\n",
    "          peptide[-1] = \"C(Carbamidomethylation)\"\n",
    "          index += 8\n",
    "        elif peptide[-1] == 'M' and raw_sequence[index:index+8] == \"(+15.99)\":\n",
    "          peptide[-1] = 'M(Oxidation)'\n",
    "          index += 8\n",
    "        elif peptide[-1] == 'N' and raw_sequence[index:index+6] == \"(+.98)\":\n",
    "          peptide[-1] = 'N(Deamidation)'\n",
    "          index += 6\n",
    "        elif peptide[-1] == 'Q' and raw_sequence[index:index+6] == \"(+.98)\":\n",
    "          peptide[-1] = 'Q(Deamidation)'\n",
    "          index += 6\n",
    "        else: # unknown modification\n",
    "          print(\"ERROR: unknown modification!\")\n",
    "          print(\"raw_sequence = \", raw_sequence)\n",
    "          sys.exit()\n",
    "      else:\n",
    "        peptide.append(raw_sequence[index])\n",
    "        index += 1\n",
    "\n",
    "    return peptide\n",
    "    \"\"\"\n",
    "\n",
    "  def _match_AA_novor(self, target, predicted):\n",
    "    \"\"\"\"\"\"\n",
    "  \n",
    "    #print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    #print(\"WorkerTest._test_AA_match_novor()\")\n",
    "\n",
    "    num_match = 0\n",
    "    target_len = len(target)\n",
    "    predicted_len = len(predicted)\n",
    "    target_mass = [mass_ID[x] for x in target]\n",
    "    target_mass_cum = np.cumsum(target_mass)\n",
    "    predicted_mass = [mass_ID[x] for x in predicted]\n",
    "    predicted_mass_cum = np.cumsum(predicted_mass)\n",
    "  \n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < target_len and j < predicted_len:\n",
    "      if abs(target_mass_cum[i] - predicted_mass_cum[j]) < 0.5:\n",
    "        if abs(target_mass[i] - predicted_mass[j]) < 0.1:\n",
    "        #~ if  decoder_input[index_aa] == output[index_aa]:\n",
    "          num_match += 1\n",
    "        i += 1\n",
    "        j += 1\n",
    "      elif target_mass_cum[i] < predicted_mass_cum[j]:\n",
    "        i += 1\n",
    "      else:\n",
    "        j += 1\n",
    "\n",
    "    return num_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Xip7p39LGX9"
   },
   "source": [
    "## Read feature accuracy <a name=\"test_accuracy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrHGuoTPLGgf"
   },
   "outputs": [],
   "source": [
    "def read_feature_accuracy(input_file, split_char):\n",
    "  \"\"\"\n",
    "  Read and convert the accuracy file and return a list\n",
    "  \"\"\"\n",
    "  feature_list = []\n",
    "  with open(input_file, 'r') as handle:\n",
    "    header_line = handle.readline()\n",
    "    for line in handle:\n",
    "      line = re.split(split_char, line)\n",
    "      feature = {}\n",
    "      feature[\"feature_id\"] = line[0]\n",
    "      feature[\"feature_area\"] = math.log10(float(line[1]) + 1e-5)\n",
    "      feature[\"predicted_score\"] = float(line[4])\n",
    "      feature[\"recall_AA\"] = float(line[5])\n",
    "      feature[\"predicted_len\"] = float(line[6])\n",
    "      feature_list.append(feature)\n",
    "  return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47qh4W2cLAcH"
   },
   "source": [
    "## Find score cutoff <a name=\"test_cutoff\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-13dv_VALAm4"
   },
   "outputs": [],
   "source": [
    "def find_score_cutoff(accuracy_file, accuracy_cutoff):\n",
    "  \"\"\"\n",
    "  Find the sequences with an acccurcy higher than the cutoff and return the count,\n",
    "  and the average score computed of the predicted data.\n",
    "  \"\"\"\n",
    "  print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "  print(\"find_score_cutoff()\")\n",
    "\n",
    "  feature_list = read_feature_accuracy(accuracy_file, '\\t|\\r|\\n')\n",
    "  feature_list_sorted = sorted(feature_list, key=lambda k: k['predicted_score'], reverse=True)\n",
    "  recall_cumsum = np.cumsum([f['recall_AA'] for f in feature_list_sorted])\n",
    "  predicted_len_cumsum = np.cumsum([f['predicted_len'] for f in feature_list_sorted])\n",
    "  accuracy_cumsum = recall_cumsum / predicted_len_cumsum\n",
    "  cutoff_index = np.flatnonzero(accuracy_cumsum < accuracy_cutoff)[0]\n",
    "  cutoff_score = feature_list_sorted[cutoff_index]['predicted_score']\n",
    "  print('cutoff_index = ', cutoff_index)\n",
    "  print('cutoff_score = ', cutoff_score)\n",
    "  print('cutoff_score = ', 100*math.exp(cutoff_score))\n",
    "\n",
    "  return cutoff_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FVmA0MHKbOU"
   },
   "source": [
    "## Testing Launch section <a name=\"test_launch\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kQ2SvmmZKacI",
    "outputId": "56beb669-19e1-4e73-d2a7-91ac8dcee686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WorkerTest.__init__()\n",
      "target_file = ../smbp_data/features_smbp.csv.test\n",
      "predicted_file = ../smbp_data/features_smbp.csv.test.deepnovo_denovo\n",
      "predicted_format = deepnovo\n",
      "accuracy_file = ../smbp_data/features_smbp.csv.test.deepnovo_denovo.accuracy\n",
      "denovo_only_file = ../smbp_data/features_smbp.csv.test.deepnovo_denovo.denovo_only\n",
      "scan2fea_file = ../smbp_data/features_smbp.csv.test.deepnovo_denovo.scan2fea\n",
      "multifea_file = ../smbp_data/features_smbp.csv.test.deepnovo_denovo.multifea\n",
      "================================================================================\n",
      "WorkerTest.test_accuracy()\n",
      "================================================================================\n",
      "WorkerTest._get_target()\n",
      "================================================================================\n",
      "WorkerTest._get_predicted()\n",
      "target_count_total = 467\n",
      "target_len_total = 6669\n",
      "target_count_db = 467\n",
      "target_len_db = 6669\n",
      "target_count_db_mass: 467\n",
      "target_len_db_mass: 6669\n",
      "predicted_count_mass: 452\n",
      "predicted_count_mass_db: 452\n",
      "predicted_len_mass_db: 6450\n",
      "predicted_only: 0\n",
      "recall_AA_total = 0.6839\n",
      "recall_AA_db = 0.6839\n",
      "recall_AA_db_mass = 0.6839\n",
      "recall_peptide_total = 0.3340\n",
      "recall_peptide_db = 0.3340\n",
      "recall_peptide_db_mass = 0.3340\n",
      "precision_AA_mass_db  = 0.7071\n",
      "precision_peptide_mass_db  = 0.3451\n",
      "================================================================================\n",
      "find_score_cutoff()\n",
      "cutoff_index =  98\n",
      "cutoff_score =  -0.54\n",
      "cutoff_score =  58.274825237398964\n"
     ]
    }
   ],
   "source": [
    "if 'test' in option:\n",
    "  worker_test = WorkerTest()\n",
    "  worker_test.test_accuracy()\n",
    "\n",
    "  # show 95 accuracy score threshold\n",
    "  accuracy_cutoff = 0.95\n",
    "  score_cutoff = find_score_cutoff(accuracy_file, accuracy_cutoff)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DeepNovoV2_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
